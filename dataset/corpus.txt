def get flashed messages ( with categories =  False  , category filter = [ ] ) : flashes =  request ctx stack . top . flashes if ( flashes is  None  ) :  request ctx stack . top . flashes = flashes = ( session . pop ( ' flashes' ) if ( ' flashes' in session ) else [ ] ) if category filter : flashes = list ( filter ( ( lambda f : ( f [ 0 ] in category filter ) ) , flashes ) ) if ( not with categories ) : return [ x [ 1 ] for x in flashes ] return flashes
def resource patch ( context , data dict ) :  check access ( 'resource patch' , context , data dict ) show context = { 'model' : context [ 'model' ] , 'session' : context [ 'session' ] , 'user' : context [ 'user' ] , 'auth user obj' : context [ 'auth user obj' ] } resource dict =  get action ( 'resource show' ) ( show context , { 'id' :  get or bust ( data dict , 'id' ) } ) patched = dict ( resource dict ) patched . update ( data dict ) return  update . resource update ( context , patched )
def install translator ( qapp ) : global QT TRANSLATOR if ( QT TRANSLATOR is  None  ) : qt translator = Q Translator  ( ) if qt translator . load ( ( 'qt ' + Q Locale  . system ( ) . name ( ) ) , Q Library  Info  . location ( Q Library  Info  .  Translations  Path  ) ) : QT TRANSLATOR = qt translator if ( QT TRANSLATOR is not  None  ) : qapp . install Translator  ( QT TRANSLATOR )
def get group index ( labels , shape , sort , xnull ) : def  int64 cut off ( shape ) : acc = long ( 1 ) for ( i , mul ) in enumerate ( shape ) : acc *= long ( mul ) if ( not ( acc <  INT64 MAX ) ) : return i return len ( shape ) def loop ( labels , shape ) : nlev =  int64 cut off ( shape ) stride = np . prod ( shape [ 1 : nlev ] , dtype = 'i8' ) out = ( stride * labels [ 0 ] . astype ( 'i8' , subok =  False  , copy =  False  ) ) for i in range ( 1 , nlev ) : if ( shape [ i ] == 0 ) : stride = 0 else : stride //= shape [ i ] out += ( labels [ i ] * stride ) if xnull : mask = ( labels [ 0 ] == ( - 1 ) ) for lab in labels [ 1 : nlev ] : mask |= ( lab == ( - 1 ) ) out [ mask ] = ( - 1 ) if ( nlev == len ( shape ) ) : return out ( comp ids , obs ids ) =  compress group index ( out , sort = sort ) labels = ( [ comp ids ] + labels [ nlev : ] ) shape = ( [ len ( obs ids ) ] + shape [ nlev : ] ) return loop ( labels , shape ) def maybe lift ( lab , size ) : return ( ( ( lab + 1 ) , ( size + 1 ) ) if ( lab == ( - 1 ) ) . any ( ) else ( lab , size ) ) labels = map (  ensure int64 , labels ) if ( not xnull ) : ( labels , shape ) = map ( list , zip ( * map ( maybe lift , labels , shape ) ) ) return loop ( list ( labels ) , list ( shape ) )
def get fun ( fun ) : with  get serv ( ret =  None  , commit =  True  ) as cur : sql = 'SELECT  s.id,s.jid,  s.full ret\n                                FROM  salt returns  s\n                                JOIN  (  SELECT  MAX(`jid`)  as  jid\n                                        from  salt returns  GROUP  BY  fun,  id)  max\n                                ON  s.jid  =  max.jid\n                                WHERE  s.fun  =  %s\n                                ' cur . execute ( sql , ( fun , ) ) data = cur . fetchall ( ) ret = { } if data : for ( minion ,   , full ret ) in data : ret [ minion ] = full ret return ret
def makeDKL2RGB ( nm , powerRGB ) : interpolate Cones  = interpolate . interp1d ( wavelength 5nm , cones  Smith  Pokorny  ) interpolate Judd  = interpolate . interp1d ( wavelength 5nm , judd Vos XYZ1976 5nm ) judd = interpolate Judd  ( nm ) cones = interpolate Cones  ( nm ) judd = numpy . asarray ( judd ) cones = numpy . asarray ( cones ) rgb to cones = numpy . dot ( cones , numpy . transpose ( powerRGB ) ) lumwt = numpy . dot ( judd [ 1 , : ] , numpy . linalg . pinv ( cones ) ) dkl to cones = numpy . dot ( rgb to cones , [ [ 1 , 0 , 0 ] , [ 1 , 0 , 0 ] , [ 1 , 0 , 0 ] ] ) dkl to cones [ ( 0 , 1 ) ] = ( lumwt [ 1 ] / lumwt [ 0 ] ) dkl to cones [ ( 1 , 1 ) ] = ( - 1 ) dkl to cones [ ( 2 , 1 ) ] = lumwt [ 2 ] dkl to cones [ ( 0 , 2 ) ] = 0 dkl to cones [ ( 1 , 2 ) ] = 0 dkl to cones [ ( 2 , 2 ) ] = ( - 1 ) cones to rgb = numpy . linalg . inv ( rgb to cones ) dkl to rgb = numpy . dot ( cones to rgb , dkl to cones ) dkl to rgb [ : , 0 ] /= max ( abs ( dkl to rgb [ : , 0 ] ) ) dkl to rgb [ : , 1 ] /= max ( abs ( dkl to rgb [ : , 1 ] ) ) dkl to rgb [ : , 2 ] /= max ( abs ( dkl to rgb [ : , 2 ] ) ) return dkl to rgb
def get minions ( ) : conn =  get conn ( ret =  None  ) cur = conn . cursor ( ) sql = 'SELECT  DISTINCT  id  FROM  salt returns' cur . execute ( sql ) data = cur . fetchall ( ) ret = [ ] for minion in data : ret . append ( minion [ 0 ] )  close conn ( conn ) return ret
def url2ip ( url ) : iport = urlsplit ( url ) [ 1 ] . split ( ':' ) if ( len ( iport ) > 1 ) : return ( gethostbyname ( iport [ 0 ] ) , iport [ 1 ] ) return gethostbyname ( iport [ 0 ] )
@ pytest . mark . skipif ( 'no real s3 credentials()' ) def test policy ( sts conn , monkeypatch ) : monkeypatch . setenv ( 'AWS REGION' , 'us-west-1' ) bn = bucket name mangle ( 'wal-e.sts.list.test' ) h = 's3-us-west-1.amazonaws.com' cf = connection .  Ordinary  Calling  Format  ( ) fed = sts conn . get federation token ( 'wal-e-test-list-bucket' , policy = make policy ( bn , 'test-prefix' ) ) test payload = 'wal-e  test' keys = [ 'test-prefix/hello' , 'test-prefix/world' , 'not-in-prefix/goodbye' , 'not-in-prefix/world' ] creds =  Credentials  ( fed . credentials . access key , fed . credentials . secret key , fed . credentials . session token ) with  Fresh  Bucket  ( bn , keys = keys , calling format = cf , host = h ) as fb : bucket superset creds = fb . create ( location = 'us-west-1' ) cinfo = calling format . from store name ( bn ) conn = cinfo . connect ( creds ) conn . host = h bucket = conn . get bucket ( bn , validate =  False  ) for name in keys : if name . startswith ( 'test-prefix/' ) : k = connection .  Key  ( bucket ) else : k = connection .  Key  ( bucket superset creds ) k . key = name k . set contents from string ( test payload ) prefix fetched keys = list ( bucket . list ( prefix = 'test-prefix/' ) ) assert ( len ( prefix fetched keys ) == 2 ) for key in prefix fetched keys : assert ( key . get contents as string ( ) == 'wal-e  test' ) with pytest . raises ( exception . S3 Response  Error  ) as e : list ( bucket . list ( prefix = '' ) ) assert ( e . value . status == 403 ) k = connection .  Key  ( bucket ) k . key = 'not-in-prefix/world' with pytest . raises ( exception . S3 Response  Error  ) as e : k . set contents from string ( test payload ) assert ( e . value . status == 403 )
def  expand probes ( probes , defaults ) : expected probes = { } for ( probe name , probe test ) in six . iteritems ( probes ) : if ( probe name not in expected probes . keys ( ) ) : expected probes [ probe name ] = { } probe defaults = probe test . pop ( 'defaults' , { } ) for ( test name , test details ) in six . iteritems ( probe test ) : test defaults = test details . pop ( 'defaults' , { } ) expected test details = deepcopy ( defaults ) expected test details . update ( probe defaults ) expected test details . update ( test defaults ) expected test details . update ( test details ) if ( test name not in expected probes [ probe name ] . keys ( ) ) : expected probes [ probe name ] [ test name ] = expected test details return expected probes
def get default site ( app name = 'filebrowser' ) : resolver = get resolver ( get urlconf ( ) ) name = 'filebrowser' app list = resolver . app dict [ app name ] if ( name not in app list ) : name = app list [ 0 ] return get site dict ( ) [ name ]
def  load plugins ( config ) : paths = config [ 'pluginpath' ] . get ( confit .  Str  Seq  ( split =  False  ) ) paths = map ( util . normpath , paths ) import beetsplug beetsplug .   path   = ( paths + beetsplug .   path   ) sys . path += paths plugins . load plugins ( config [ 'plugins' ] . as str seq ( ) ) plugins . send ( 'pluginload' ) return plugins
def rgb2short ( r , g , b ) : dist = ( lambda s , d : ( ( ( ( s [ 0 ] - d [ 0 ] ) ** 2 ) + ( ( s [ 1 ] - d [ 1 ] ) ** 2 ) ) + ( ( s [ 2 ] - d [ 2 ] ) ** 2 ) ) ) ary = [ hex to rgb ( hex ) for hex in RGB2SHORT DICT ] m = min ( ary , key = partial ( dist , ( r , g , b ) ) ) return RGB2SHORT DICT [ rgb to hex ( m ) ]
@ register . filter ( is safe =  True  ) def pprint ( value ) : try : return pformat ( value ) except  Exception  as e : return ( u' Error   in  formatting:  %s' % force text ( e , errors = u'replace' ) )
def project activity list layout ( list id , item id , resource , rfields , record , icon = 'activity' ) : raw = record .  row record id = raw [ 'project activity.id' ] item class = 'thumbnail' author = record [ 'project activity.modified by' ] name = record [ 'project activity.name' ] description = record [ 'project activity.comments' ] start date = record [ 'project activity.date' ] location = record [ 'project activity.location id' ] location id = raw [ 'project activity.location id' ] comments = raw [ 'project activity.comments' ] organisation id = raw [ 'project activity organisation.organisation id' ] if organisation id : organisation = record [ 'project activity organisation.organisation id' ] org url = URL ( c = 'org' , f = 'organisation' , args = [ organisation id , 'profile' ] ) org logo = raw [ 'org organisation.logo' ] if org logo : org logo = A ( IMG (  src = URL ( c = 'default' , f = 'download' , args = [ org logo ] ) ,  class = 'media-object' ) ,  href = org url ,  class = 'pull-left' ) else : org logo = A ( IMG (  class = 'media-object' ) ,  href = org url ,  class = 'pull-left' ) organisation = A ( organisation ,  href = org url ,  class = 'card-organisation' ) else : organisation = '' permit = current . auth . s3 has permission table = current . db . project activity if permit ( 'update' , table , record id = record id ) : edit btn = A ( ICON ( 'edit' ) ,  href = URL ( c = 'project' , f = 'activity' , args = [ record id , 'update.popup' ] , vars = { 'refresh' : list id , 'record' : record id } ) ,  class = 's3 modal' ,  title = S3CRUD . crud string ( resource . tablename , 'title update' ) ) else : edit btn = '' if permit ( 'delete' , table , record id = record id ) : delete btn = A ( ICON ( 'delete' ) ,  class = 'dl-item-delete' ,  title = S3CRUD . crud string ( resource . tablename , 'label delete button' ) ) else : delete btn = '' edit bar = DIV ( edit btn , delete btn ,  class = 'edit-bar  fright' ) item = DIV ( DIV ( ICON ( icon ) , SPAN ( location ,  class = 'location-title' ) , SPAN ( start date ,  class = 'date-title' ) , edit bar ,  class = 'card-header' ) , DIV ( DIV ( A ( name ,  href = URL ( c = 'project' , f = 'activity' , args = [ record id , 'profile' ] ) ) ,  class = 'card-title' ) , DIV ( DIV ( ( description or '' ) , DIV ( ( author or '' ) , '  -  ' , organisation ,  class = 'card-person' ) ,  class = 'media' ) ,  class = 'media-body' ) ,  class = 'media' ) ,  class = item class ,  id = item id ) return item
def all ( iterable ) : for element in iterable : if ( not element ) : return  False  return  True
def  setwindowposition ( folder alias , ( x , y ) ) : finder =  getfinder ( ) args = { } attrs = { } aeobj 0 = aetypes .  Object  Specifier  ( want = aetypes .  Type  ( 'cfol' ) , form = 'alis' , seld = folder alias , fr =  None  ) aeobj 1 = aetypes .  Object  Specifier  ( want = aetypes .  Type  ( 'prop' ) , form = 'prop' , seld = aetypes .  Type  ( 'cwnd' ) , fr = aeobj 0 ) aeobj 2 = aetypes .  Object  Specifier  ( want = aetypes .  Type  ( 'prop' ) , form = 'prop' , seld = aetypes .  Type  ( 'posn' ) , fr = aeobj 1 ) args [ '----' ] = aeobj 2 args [ 'data' ] = [ x , y ] (  reply , args , attrs ) = finder . send ( 'core' , 'setd' , args , attrs ) if args . has key ( 'errn' ) : raise  Error  , aetools . decodeerror ( args ) if args . has key ( '----' ) : return args [ '----' ]
def url filename ( url ) : match = upload title re . match ( url ) if match : return match . group ( 'filename' ) else : return url
def runwsgi ( func ) : if os . environ . has key ( 'SERVER SOFTWARE' ) : os . environ [ 'FCGI FORCE CGI' ] = 'Y' if ( os . environ . has key ( 'PHP FCGI CHILDREN' ) or os . environ . has key ( 'SERVER SOFTWARE' ) ) : return runfcgi ( func ) if ( 'scgi' in sys . argv ) : return runscgi ( func ) return runsimple ( func , listget ( sys . argv , 1 , 8080 ) )
def  proxy process ( proxyname , test ) : changes old = [ ] changes new = [ ] if ( not  is proxy running ( proxyname ) ) : if ( not test ) :   salt   [ 'cmd.run all' ] ( 'salt-proxy  --proxyid={0}  -l  info  -d' . format ( salt . ext . six . moves . shlex quote ( proxyname ) ) , timeout = 5 ) changes new . append ( ' Salt    Proxy :   Started   proxy  process  for  {0}' . format ( proxyname ) ) else : changes new . append ( ' Salt    Proxy :  process  {0}  will  be  started' . format ( proxyname ) ) else : changes old . append ( ' Salt    Proxy :  already  running  for  {0}' . format ( proxyname ) ) return (  True  , changes new , changes old )
def  offset or limit clause ( element , name =  None  , type  =  None  ) : if ( element is  None  ) : return  None  elif hasattr ( element , '  clause element  ' ) : return element .   clause element   ( ) elif isinstance ( element ,  Visitable  ) : return element else : value = util . asint ( element ) return   Offset  Limit  Param  ( name , value , type  = type  , unique =  True  )
def  get cron info ( ) : owner = 'root' if (   grains   [ 'os' ] == ' Free BSD' ) : group = 'wheel' crontab dir = '/var/spool/incron' elif (   grains   [ 'os' ] == ' Open BSD' ) : group = 'crontab' crontab dir = '/var/spool/incron' elif (   grains   . get ( 'os family' ) == ' Solaris ' ) : group = 'root' crontab dir = '/var/spool/incron' else : group = 'root' crontab dir = '/var/spool/incron' return ( owner , group , crontab dir )
def  retrieve device config ( ) : return   salt   [ 'snmp.config' ] ( )
def flatten ( struct ) : if ( struct is  None  ) : return [ ] flat = [ ] if isinstance ( struct , dict ) : for (   , result ) in six . iteritems ( struct ) : flat += flatten ( result ) return flat if isinstance ( struct , six . string types ) : return [ struct ] try : iterator = iter ( struct ) except  Type  Error  : return [ struct ] for result in iterator : flat += flatten ( result ) return flat
def path to filesystem ( root , * paths ) : paths = [ sanitize path ( path ) . strip ( '/' ) for path in paths ] safe path = root for path in paths : if ( not path ) : continue for part in path . split ( '/' ) : if ( not is safe filesystem path component ( part ) ) : raise  Unsafe  Path  Error  ( part ) safe path = os . path . join ( safe path , part ) return safe path
@ receiver ( user logged in ) def log successful login ( sender , request , user , ** kwargs ) : if settings . FEATURES [ 'SQUELCH PII IN LOGS' ] : AUDIT LOG . info ( u' Login   success  -  user.id:  {0}' . format ( user . id ) ) else : AUDIT LOG . info ( u' Login   success  -  {0}  ({1})' . format ( user . username , user . email ) )
def get name ( name or obj ) : if isinstance ( name or obj , six . string types ) : return name or obj try : return name or obj . name except  Attribute  Error  : raise exc .  Missing  Name  ( name or obj )
def zoom effect02 ( ax1 , ax2 , ** kwargs ) : tt = ( ax1 . trans Scale  + ( ax1 . trans Limits  + ax2 . trans Axes  ) ) trans = blended transform factory ( ax2 . trans Data  , tt ) mybbox1 = ax1 . bbox mybbox2 =  Transformed  Bbox  ( ax1 . view Lim  , trans ) prop patches = kwargs . copy ( ) prop patches [ 'ec' ] = 'none' prop patches [ 'alpha' ] = 0.2 ( c1 , c2 , bbox patch1 , bbox patch2 , p ) = connect bbox ( mybbox1 , mybbox2 , loc1a = 3 , loc2a = 2 , loc1b = 4 , loc2b = 1 , prop lines = kwargs , prop patches = prop patches ) ax1 . add patch ( bbox patch1 ) ax2 . add patch ( bbox patch2 ) ax2 . add patch ( c1 ) ax2 . add patch ( c2 ) ax2 . add patch ( p ) return ( c1 , c2 , bbox patch1 , bbox patch2 , p )
def  expand table ( table ) : return np . repeat ( [ [ 1 , 1 ] , [ 1 , 0 ] , [ 0 , 1 ] , [ 0 , 0 ] ] , table . ravel ( ) , axis = 0 )
def load extra data ( backend , details , response , uid , user , social user =  None  , * args , ** kwargs ) : social user = ( social user or  User  Social  Auth  . get social auth ( backend . name , uid , user ) ) if social user : extra data = backend . extra data ( user , uid , response , details ) if ( kwargs . get ( 'original email' ) and ( 'email' not in extra data ) ) : extra data [ 'email' ] = kwargs . get ( 'original email' ) if ( extra data and ( social user . extra data != extra data ) ) : if social user . extra data : social user . extra data . update ( extra data ) else : social user . extra data = extra data social user . save ( ) return { 'social user' : social user }
def launch and configure ( ec2 args ) : print '{:<40}' . format ( ' Creating   SQS  queue  and  launching  instance  for  {}:' . format ( run id ) ) print for ( k , v ) in ec2 args . iteritems ( ) : if ( k != 'user data' ) : print '        {:<25}{}' . format ( k , v ) print global sqs queue global instance id sqs queue = sqs . create queue ( run id ) sqs queue . set message class (  Raw  Message  ) res = ec2 . run instances ( ** ec2 args ) inst = res . instances [ 0 ] instance id = inst . id print '{:<40}' . format ( ' Waiting   for  instance  {}  to  reach  running  status:' . format ( instance id ) ) , status start = time . time ( ) for   in xrange ( EC2 RUN TIMEOUT ) : try : res = ec2 . get all instances ( instance ids = [ instance id ] ) except EC2 Response  Error  as e : if ( e . code == ' Invalid  Instance ID. Not  Found ' ) : print ' Instance   not  found({}),  will  try  again.' . format ( instance id ) time . sleep ( 1 ) continue else : raise e if ( res [ 0 ] . instances [ 0 ] . state == 'running' ) : status delta = ( time . time ( ) - status start ) run summary . append ( ( 'EC2   Launch ' , status delta ) ) print '[  OK  ]  {:0>2.0f}:{:0>2.0f}' . format ( ( status delta / 60 ) , ( status delta % 60 ) ) break else : time . sleep ( 1 ) else : raise  Exception  ( ' Timeout   waiting  for  running  status:  {}  ' . format ( instance id ) ) print '{:<40}' . format ( ' Waiting   for  system  status:' ) , system start = time . time ( ) for   in xrange ( EC2 STATUS TIMEOUT ) : status = ec2 . get all instance status ( inst . id ) if ( status and ( status [ 0 ] . system status . status == u'ok' ) ) : system delta = ( time . time ( ) - system start ) run summary . append ( ( 'EC2   Status    Checks ' , system delta ) ) print '[  OK  ]  {:0>2.0f}:{:0>2.0f}' . format ( ( system delta / 60 ) , ( system delta % 60 ) ) break else : time . sleep ( 1 ) else : raise  Exception  ( ' Timeout   waiting  for  status  checks:  {}  ' . format ( instance id ) ) print print '{:<40}' . format ( ' Waiting   for  user-data,  polling  sqs  for   Ansible   events:' ) ( ansible delta , task report ) = poll sqs ansible ( ) run summary . append ( ( ' Ansible   run' , ansible delta ) ) print print '{}  longest   Ansible   tasks  (seconds):' . format ( NUM TASKS ) for task in sorted ( task report , reverse =  True  , key = ( lambda k : k [ 'DELTA' ] ) ) [ : NUM TASKS ] : print '{:0>3.0f}  {}' . format ( task [ 'DELTA' ] , task [ 'TASK' ] ) print '    -  {}' . format ( task [ 'INVOCATION' ] ) print print '{:<40}' . format ( ' Creating   AMI:' ) , ami start = time . time ( ) ami = create ami ( instance id , run id , run id ) ami delta = ( time . time ( ) - ami start ) print '[  OK  ]  {:0>2.0f}:{:0>2.0f}' . format ( ( ami delta / 60 ) , ( ami delta % 60 ) ) run summary . append ( ( 'AMI   Build ' , ami delta ) ) total time = ( time . time ( ) - start time ) all stages = sum ( ( run [ 1 ] for run in run summary ) ) if ( ( total time - all stages ) > 0 ) : run summary . append ( ( ' Other ' , ( total time - all stages ) ) ) run summary . append ( ( ' Total ' , total time ) ) return ( run summary , ami )
@ testing . requires testing data @ requires mne def test other volume source spaces ( ) : tempdir =   Temp  Dir  ( ) temp name = op . join ( tempdir , 'temp-src.fif' ) run subprocess ( [ 'mne volume source space' , '--grid' , '7.0' , '--src' , temp name , '--mri' , fname mri ] ) src = read source spaces ( temp name ) src new = setup volume source space (  None  , pos = 7.0 , mri = fname mri , subjects dir = subjects dir )  compare source spaces ( src , src new , mode = 'approx' ) assert true ( ( 'volume,  shape' in repr ( src ) ) ) del src del src new assert raises (  Value  Error  , setup volume source space , 'sample' , temp name , pos = 7.0 , sphere = [ 1.0 , 1.0 ] , mri = fname mri , subjects dir = subjects dir ) run subprocess ( [ 'mne volume source space' , '--grid' , '7.0' , '--src' , temp name ] ) assert raises (  Value  Error  , read source spaces , temp name )
def managed ( name , entries , connect spec =  None  ) : if ( connect spec is  None  ) : connect spec = { } try : connect spec . setdefault ( 'url' , name ) except  Attribute  Error  : pass connect =   salt   [ 'ldap3.connect' ] ldap3 = inspect . getmodule ( connect ) with connect ( connect spec ) as l : ( old , new ) =  process entries ( l , entries ) dn set =  Ordered  Dict  ( ) dn set . update ( old ) dn set . update ( new ) dn to delete = set ( ) for dn in dn set : o = old . get ( dn , { } ) n = new . get ( dn , { } ) for x in ( o , n ) : to delete = set ( ) for ( attr , vals ) in six . iteritems ( x ) : if ( not len ( vals ) ) : to delete . add ( attr ) for attr in to delete : del x [ attr ] if ( o == n ) : dn to delete . add ( dn ) for dn in dn to delete : for x in ( old , new ) : x . pop ( dn ,  None  ) del dn set [ dn ] ret = { 'name' : name , 'changes' : { } , 'result' :  None  , 'comment' : '' } if ( old == new ) : ret [ 'comment' ] = 'LDAP  entries  already  set' ret [ 'result' ] =  True  return ret if   opts   [ 'test' ] : ret [ 'comment' ] = ' Would   change  LDAP  entries' changed old = old changed new = new success dn set = dn set else : changed old =  Ordered  Dict  ( ) changed new =  Ordered  Dict  ( ) ret [ 'result' ] =  True  ret [ 'comment' ] = ' Successfully   updated  LDAP  entries' errs = [ ] success dn set =  Ordered  Dict  ( ) for dn in dn set : o = old . get ( dn , { } ) n = new . get ( dn , { } ) try : if len ( o ) : if len ( n ) : op = 'modify' assert ( o != n )   salt   [ 'ldap3.change' ] ( l , dn , o , n ) else : op = 'delete'   salt   [ 'ldap3.delete' ] ( l , dn ) else : op = 'add' assert len ( n )   salt   [ 'ldap3.add' ] ( l , dn , n ) changed old [ dn ] = o changed new [ dn ] = n success dn set [ dn ] =  True  except ldap3 . LDAP Error  : log . exception ( 'failed  to  %s  entry  %s' , op , dn ) errs . append ( ( op , dn ) ) continue if len ( errs ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ( 'failed  to  ' + ',  ' . join ( ( ( ( op + '  entry  ' ) + dn ) for ( op , dn ) in errs ) ) ) for dn in success dn set : o = changed old . get ( dn , { } ) n = changed new . get ( dn , { } ) changes = { } ret [ 'changes' ] [ dn ] = changes for ( x , xn ) in ( ( o , 'old' ) , ( n , 'new' ) ) : if ( not len ( x ) ) : changes [ xn ] =  None  continue changes [ xn ] = dict ( ( ( attr , sorted ( vals ) ) for ( attr , vals ) in six . iteritems ( x ) if ( o . get ( attr , ( ) ) != n . get ( attr , ( ) ) ) ) ) return ret
def arbitrary ( module name , func name , args , kwargs = { } ) : if module name . startswith ( 'calibre plugins' ) : from calibre . customize . ui import find plugin find plugin module = importlib . import module ( module name ) func = getattr ( module , func name ) return func ( * args , ** kwargs )
def create api deployment ( rest Api  Id  , stage Name  , stage Description  = '' , description = '' , cache Cluster  Enabled  =  False  , cache Cluster  Size  = '0.5' , variables =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : try : variables = ( dict ( ) if ( variables is  None  ) else variables ) conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) deployment = conn . create deployment ( rest Api  Id  = rest Api  Id  , stage Name  = stage Name  , stage Description  = stage Description  , description = description , cache Cluster  Enabled  = cache Cluster  Enabled  , cache Cluster  Size  = cache Cluster  Size  , variables = variables ) return { 'created' :  True  , 'deployment' :  convert datetime str ( deployment ) } except  Client  Error  as e : return { 'created' :  False  , 'error' : salt . utils . boto3 . get error ( e ) }
def sdm spoly ( f , g , O , K , phantom =  None  ) : if ( ( not f ) or ( not g ) ) : return sdm zero ( ) LM1 = sdm LM ( f ) LM2 = sdm LM ( g ) if ( LM1 [ 0 ] != LM2 [ 0 ] ) : return sdm zero ( ) LM1 = LM1 [ 1 : ] LM2 = LM2 [ 1 : ] lcm = monomial lcm ( LM1 , LM2 ) m1 = monomial div ( lcm , LM1 ) m2 = monomial div ( lcm , LM2 ) c = K . quo ( ( - sdm LC ( f , K ) ) , sdm LC ( g , K ) ) r1 = sdm add ( sdm mul term ( f , ( m1 , K . one ) , O , K ) , sdm mul term ( g , ( m2 , c ) , O , K ) , O , K ) if ( phantom is  None  ) : return r1 r2 = sdm add ( sdm mul term ( phantom [ 0 ] , ( m1 , K . one ) , O , K ) , sdm mul term ( phantom [ 1 ] , ( m2 , c ) , O , K ) , O , K ) return ( r1 , r2 )
def attach network interface ( device index , name =  None  , network interface id =  None  , instance name =  None  , instance id =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : if ( not salt . utils . exactly one ( ( name , network interface id ) ) ) : raise  Salt  Invocation  Error  ( " Exactly   one  (but  not  both)  of  'name'  or  'network interface id'  must  be  provided." ) if ( not salt . utils . exactly one ( ( instance name , instance id ) ) ) : raise  Salt  Invocation  Error  ( " Exactly   one  (but  not  both)  of  'instance name'  or  'instance id'  must  be  provided." ) conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) r = { } result =  get network interface ( conn , name , network interface id ) if ( 'error' in result ) : return result eni = result [ 'result' ] try : info =  describe network interface ( eni ) network interface id = info [ 'id' ] except  Key  Error  : r [ 'error' ] = { 'message' : 'ID  not  found  for  this  network  interface.' } return r if instance name : try : instance id = get id ( name = instance name , region = region , key = key , keyid = keyid , profile = profile ) except boto . exception .  Boto  Server  Error  as e : log . error ( e ) return  False  try : r [ 'result' ] = conn . attach network interface ( network interface id , instance id , device index ) except boto . exception . EC2 Response  Error  as e : r [ 'error' ] =   utils   [ 'boto.get error' ] ( e ) return r
def  point along a line ( x0 , y0 , x1 , y1 , d ) : ( dx , dy ) = ( ( x0 - x1 ) , ( y0 - y1 ) ) ff = ( d / ( ( ( dx * dx ) + ( dy * dy ) ) ** 0.5 ) ) ( x2 , y2 ) = ( ( x0 - ( ff * dx ) ) , ( y0 - ( ff * dy ) ) ) return ( x2 , y2 )
def upgrade ( migrate engine ) : meta =  Meta  Data  ( ) meta . bind = migrate engine volume type projects =  Table  ( 'volume type projects' , meta , autoload =  True  ) if ( migrate engine . name == 'postgresql' ) : sql = ( 'ALTER  TABLE  volume type projects  ALTER  COLUMN  deleted  ' + 'TYPE  INTEGER  USING  deleted::integer' ) migrate engine . execute ( sql ) else : volume type projects . c . deleted . alter (  Integer  )
def cert from key info ( key info , ignore age =  False  ) : res = [ ] for x509 data in key info . x509 data : x509 certificate = x509 data . x509 certificate cert = x509 certificate . text . strip ( ) cert = '\n' . join ( split len ( '' . join ( [ s . strip ( ) for s in cert . split ( ) ] ) , 64 ) ) if ( ignore age or active cert ( cert ) ) : res . append ( cert ) else : logger . info ( ' Inactive   cert' ) return res
def instance group update ( context , group uuid , values ) : return IMPL . instance group update ( context , group uuid , values )
def  tree to bitstrs ( tree ) : clades bitstrs = { } term names = [ term . name for term in tree . find clades ( terminal =  True  ) ] for clade in tree . find clades ( terminal =  False  ) : bitstr =  clade to bitstr ( clade , term names ) clades bitstrs [ clade ] = bitstr return clades bitstrs
def timeuntil ( value , arg =  None  ) : from django . utils . timesince import timesince from datetime import datetime if ( not value ) : return '' if arg : return timesince ( arg , value ) return timesince ( datetime . now ( ) , value )
def main ( ) : module =  Ansible  Module  ( argument spec = { 'table' : { 'required' :  True  } , 'record' : { 'required' :  True  } , 'col' : { 'required' :  True  } , 'key' : { 'required' :  True  } , 'value' : { 'required' :  True  } , 'timeout' : { 'default' : 5 , 'type' : 'int' } } , supports check mode =  True  ) params set ( module )
def test hashbang ( ) : entry = tokenize ( '#!this  is  a  comment\n' ) assert ( entry == [ ] )
def analyze modules ( project , task handle = taskhandle .  Null  Task  Handle  ( ) ) : resources = project . get python files ( ) job set = task handle . create jobset ( ' Analyzing    Modules ' , len ( resources ) ) for resource in resources : job set . started job ( resource . path ) analyze module ( project , resource ) job set . finished job ( )
def   virtual   ( ) : if   opts   [ 'master tops' ] . get ( 'ext nodes' ) : return  True  return  False
def disassociate api key stagekeys ( api Key  , stagekeyslist , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : try : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) pvlist = [ ( '/stages' , stagekey ) for stagekey in stagekeyslist ] response =  api key patch remove ( conn , api Key  , pvlist ) return { 'disassociated' :  True  } except  Client  Error  as e : return { 'disassociated' :  False  , 'error' : salt . utils . boto3 . get error ( e ) }
@ then ( u'we  see  database  dropped' ) def step see db dropped ( context ) :  expect exact ( context , u'DROP  DATABASE' , timeout = 2 )
@ bdd . when ( bdd . parsers . parse ( 'I  wait  for  the  javascript  message  "{message}"' ) ) def javascript message when ( quteproc , message ) : quteproc . wait for js ( message )
def  git Present  ( ) : try : gitvers = subprocess . check output ( 'git  --version' . split ( ) , stderr = subprocess . PIPE ) except (  Called  Process  Error  , OS Error  ) : gitvers = '' return bool ( gitvers . startswith ( 'git  version' ) )
def  Is  Auto  Generated  ( xml str ) : try : xml root =  Element  Tree  . fromstring ( xml str ) return ( ( xml root . tag == 'datastore-indexes' ) and   Boolean  Attribute  ( xml root . attrib . get ( 'auto Generate ' , 'false' ) ) ) except  Element  Tree  .  Parse  Error  : return  False
def jnp zeros ( n , nt ) : return jnyn zeros ( n , nt ) [ 1 ]
def get Tooth  Profile  Rack  ( derivation ) : addendum Side  = ( derivation . quarter Wavelength  - ( derivation . addendum * derivation . tan Pressure  ) ) addendum Complex  = complex ( addendum Side  , derivation . addendum ) dedendum Side  = ( derivation . quarter Wavelength  + ( derivation . dedendum * derivation . tan Pressure  ) ) dedendum Complex  = complex ( dedendum Side  , ( - derivation . dedendum ) ) tooth Profile  = [ dedendum Complex  ] if ( derivation . root Bevel  > 0.0 ) : mirror Point  = complex ( ( derivation . wavelength - dedendum Side  ) , ( - derivation . dedendum ) ) tooth Profile  = get Bevel  Path  ( addendum Complex  , derivation . root Bevel  , dedendum Complex  , mirror Point  ) if ( derivation . tip Bevel  > 0.0 ) : mirror Point  = complex ( ( - addendum Complex  . real ) , addendum Complex  . imag ) bevel Path  = get Bevel  Path  ( dedendum Complex  , derivation . tip Bevel  , addendum Complex  , mirror Point  ) bevel Path  . reverse ( ) tooth Profile  += bevel Path  else : tooth Profile  . append ( addendum Complex  ) return euclidean . get Mirror  Path  ( get Thickness  Multiplied  Path  ( tooth Profile  , derivation . tooth Thickness  Multiplier  ) )
def delete policy ( vhost , name , runas =  None  ) : if ( ( runas is  None  ) and ( not salt . utils . is windows ( ) ) ) : runas = salt . utils . get user ( ) res =   salt   [ 'cmd.run all' ] ( [   context   [ 'rabbitmqctl' ] , 'clear policy' , '-p' , vhost , name ] , runas = runas , python shell =  False  ) log . debug ( ' Delete   policy:  {0}' . format ( res [ 'stdout' ] ) ) return  format response ( res , ' Deleted ' )
def set time ( time ) : time format =  get date time format ( time ) dt obj = datetime . strptime ( time , time format ) cmd = 'systemsetup  -settime  {0}' . format ( dt obj . strftime ( '%H:%M:%S' ) ) return salt . utils . mac utils . execute return success ( cmd )
def get Rectangular  Grid  ( diameter , loops Complex  , maximum Complex  , minimum Complex  , zigzag ) : demiradius = ( 0.25 * diameter ) x Start  = ( minimum Complex  . real - demiradius . real ) y = ( minimum Complex  . imag - demiradius . imag ) grid Path  = [ ] row Index  = 0 while ( y < maximum Complex  . imag ) : add Grid  Row  ( diameter , grid Path  , loops Complex  , maximum Complex  , row Index  , x Start  , y , zigzag ) y += diameter . imag row Index  += 1 return grid Path
def get discount modules ( ) : return load module instances ( 'SHUUP DISCOUNT MODULES' , 'discount module' )
def test sample wrong X ( ) : sm = SMOTEENN ( random state = RND SEED ) sm . fit ( X , Y ) assert raises (  Runtime  Error  , sm . sample , np . random . random ( ( 100 , 40 ) ) , np . array ( ( ( [ 0 ] * 50 ) + ( [ 1 ] * 50 ) ) ) )
def problem rheader ( r , tabs = [ ] ) : if ( r . representation == 'html' ) : if ( r . record is  None  ) : return  None  problem = r . record tabs = [ ( T ( ' Problems ' ) , 'problems' ) , ( T ( ' Solutions ' ) , 'solution' ) , ( T ( ' Discuss ' ) , 'discuss' ) , ( T ( ' Vote ' ) , 'vote' ) , ( T ( ' Scale   of   Results ' ) , 'results' ) ] duser = s3db . delphi  Delphi  User  ( problem . group id ) if duser . authorised : tabs . append ( ( T ( ' Edit ' ) ,  None  ) ) rheader tabs = s3 rheader tabs ( r , tabs ) rtable = TABLE ( TR ( TH ( ( '%s:  ' % T ( ' Problem ' ) ) ) , problem . name , TH ( ( '%s:  ' % T ( ' Active ' ) ) ) , problem . active ) , TR ( TH ( ( '%s:  ' % T ( ' Description ' ) ) ) , problem . description ) , TR ( TH ( ( '%s:  ' % T ( ' Criteria ' ) ) ) , problem . criteria ) ) if ( r . component and ( r . component name == 'solution' ) and r . component id ) : stable = s3db . delphi solution query = ( stable . id == r . component id ) solution = db ( query ) . select ( stable . name , stable . description , limitby = ( 0 , 1 ) ) . first ( ) rtable . append ( DIV ( TR ( TH ( ( '%s:  ' % T ( ' Solution ' ) ) ) , solution . name ) , TR ( TH ( ( '%s:  ' % T ( ' Description ' ) ) ) , solution . description ) ) ) rheader = DIV ( rtable , rheader tabs ) return rheader
def read ( handle , format ) : format = format . lower ( ) motifs = parse ( handle , format ) if ( len ( motifs ) == 0 ) : raise  Value  Error  ( ' No   motifs  found  in  handle' ) if ( len ( motifs ) > 1 ) : raise  Value  Error  ( ' More   than  one  motif  found  in  handle' ) motif = motifs [ 0 ] return motif
def is sequence of strings ( obj ) : if ( not iterable ( obj ) ) : return  False  if is string like ( obj ) : return  False  for o in obj : if ( not is string like ( o ) ) : return  False  return  True
def conv 3d ( incoming , nb filter , filter size , strides = 1 , padding = 'same' , activation = 'linear' , bias =  True  , weights init = 'uniform scaling' , bias init = 'zeros' , regularizer =  None  , weight decay = 0.001 , trainable =  True  , restore =  True  , reuse =  False  , scope =  None  , name = ' Conv 3D' ) : input shape = utils . get incoming shape ( incoming ) assert ( len ( input shape ) == 5 ) , ' Incoming    Tensor   shape  must  be  5-D' filter size = utils . autoformat filter conv3d ( filter size , input shape [ ( - 1 ) ] , nb filter ) strides = utils . autoformat stride 3d ( strides ) padding = utils . autoformat padding ( padding ) try : vscope = tf . variable scope ( scope , default name = name , values = [ incoming ] , reuse = reuse ) except  Exception  : vscope = tf . variable op scope ( [ incoming ] , scope , name , reuse = reuse ) with vscope as scope : name = scope . name W init = weights init if isinstance ( weights init , str ) : W init = initializations . get ( weights init ) ( ) W regul =  None  if regularizer : W regul = ( lambda x : losses . get ( regularizer ) ( x , weight decay ) ) W = vs . variable ( 'W' , shape = filter size , regularizer = W regul , initializer = W init , trainable = trainable , restore = restore ) tf . add to collection ( ( ( tf .  Graph  Keys  . LAYER VARIABLES + '/' ) + name ) , W ) b =  None  if bias : if isinstance ( bias init , str ) : bias init = initializations . get ( bias init ) ( ) b = vs . variable ( 'b' , shape = nb filter , initializer = bias init , trainable = trainable , restore = restore ) tf . add to collection ( ( ( tf .  Graph  Keys  . LAYER VARIABLES + '/' ) + name ) , b ) inference = tf . nn . conv3d ( incoming , W , strides , padding ) if b : inference = tf . nn . bias add ( inference , b ) if isinstance ( activation , str ) : inference = activations . get ( activation ) ( inference ) elif hasattr ( activation , '  call  ' ) : inference = activation ( inference ) else : raise  Value  Error  ( ' Invalid    Activation .' ) tf . add to collection ( tf .  Graph  Keys  . ACTIVATIONS , inference ) inference . scope = scope inference . W = W inference . b = b tf . add to collection ( ( ( tf .  Graph  Keys  . LAYER TENSOR + '/' ) + name ) , inference ) return inference
def capture screenshot for step ( step , when ) : if world . auto capture screenshots : scenario num = ( step . scenario . feature . scenarios . index ( step . scenario ) + 1 ) step num = ( step . scenario . steps . index ( step ) + 1 ) step func name = step . defined at . function . func name image name = '{prefix:03d}  {num:03d}  {name}  {postfix}' . format ( prefix = scenario num , num = step num , name = step func name , postfix = when ) world . capture screenshot ( image name )
def get sw login version ( ) : return '-' . join ( get sw version ( strip build num =  True  ) . split ( '-' ) [ 1 : ( - 2 ) ] )
def  in Filesystem  Namespace  ( path ) : return ( path [ : 1 ] != '\x00' )
def test sort ( ) : model =  create model ( [ [ ( 'B' , '' , '' , 1 ) , ( 'C' , '' , '' , 2 ) , ( 'A' , '' , '' , 0 ) ] ] ) filter model = sortfilter .  Completion  Filter  Model  ( model ) filter model . sort ( 0 ,  Qt  .  Ascending  Order  ) actual =  extract model data ( filter model ) assert ( actual == [ [ ( 'A' , '' , '' ) , ( 'B' , '' , '' ) , ( 'C' , '' , '' ) ] ] ) filter model . sort ( 0 ,  Qt  .  Descending  Order  ) actual =  extract model data ( filter model ) assert ( actual == [ [ ( 'C' , '' , '' ) , ( 'B' , '' , '' ) , ( 'A' , '' , '' ) ] ] )
def create dendrogram ( X , orientation = 'bottom' , labels =  None  , colorscale =  None  , distfun =  None  , linkagefun = ( lambda x : sch . linkage ( x , 'complete' ) ) ) : if ( ( not scp ) or ( not scs ) or ( not sch ) ) : raise  Import  Error  ( ' Figure  Factory .create dendrogram  requires  scipy,                                                          scipy.spatial  and  scipy.hierarchy' ) s = X . shape if ( len ( s ) != 2 ) : exceptions .  Plotly  Error  ( 'X  should  be  2-dimensional  array.' ) if ( distfun is  None  ) : distfun = scs . distance . pdist dendrogram =   Dendrogram  ( X , orientation , labels , colorscale , distfun = distfun , linkagefun = linkagefun ) return { 'layout' : dendrogram . layout , 'data' : dendrogram . data }
def  Set  Help  Menu  Other  Help  ( main Menu  ) : global helpID Map  if ( helpID Map  is  None  ) : helpID Map  = { } cmdID = win32ui . ID HELP OTHER exclude List  = [ ' Main    Python    Documentation ' , ' Pythonwin    Reference ' ] first List  =  List  All  Help  Files  ( ) exclude Fnames  = [ ] for ( desc , fname ) in first List  : if ( desc in exclude List  ) : exclude Fnames  . append ( fname ) help Descs  = [ ] for ( desc , fname ) in first List  : if ( fname not in exclude Fnames  ) : helpID Map  [ cmdID ] = ( desc , fname ) win32ui .  Get  Main  Frame  ( ) .  Hook  Command  (  Handle  Help  Other  Command  , cmdID ) cmdID = ( cmdID + 1 ) help Menu  = main Menu  .  Get  Sub  Menu  ( ( main Menu  .  Get  Menu  Item  Count  ( ) - 1 ) ) other Help  Menu  Pos  = 2 other Menu  = help Menu  .  Get  Sub  Menu  ( other Help  Menu  Pos  ) while other Menu  .  Get  Menu  Item  Count  ( ) : other Menu  .  Delete  Menu  ( 0 , win32con . MF BYPOSITION ) if helpID Map  : for ( id , ( desc , fname ) ) in helpID Map  . iteritems ( ) : other Menu  .  Append  Menu  ( ( win32con . MF ENABLED | win32con . MF STRING ) , id , desc ) else : help Menu  .  Enable  Menu  Item  ( other Help  Menu  Pos  , ( win32con . MF BYPOSITION | win32con . MF GRAYED ) )
def pretty name ( name ) : name = ( name [ 0 ] . upper ( ) + name [ 1 : ] ) return name . replace ( ' ' , '  ' )
def list snapshots ( domain =  None  ) : ret = dict ( ) for vm domain in  get domain ( iterable =  True  , * ( ( domain and [ domain ] ) or list ( ) ) ) : ret [ vm domain . name ( ) ] = ( [  parse snapshot description ( snap ) for snap in vm domain . list All  Snapshots  ( ) ] or 'N/A' ) return ret
def branch list ( remote =  False  ) : if remote : return for each ref basename ( u'refs/remotes' ) else : return for each ref basename ( u'refs/heads' )
def to ( location , code = falcon . HTTP 302 ) : raise falcon . http status . HTTP Status  ( code , { 'location' : location } )
def main ( args = sys . argv [ 1 : ] , env =  Environment  ( ) , custom log error =  None  ) : args = decode args ( args , env . stdin encoding ) plugin manager . load installed plugins ( ) def log error ( msg , * args , ** kwargs ) : msg = ( msg % args ) level = kwargs . get ( 'level' , 'error' ) assert ( level in [ 'error' , 'warning' ] ) env . stderr . write ( ( '\nhttp:  %s:  %s\n' % ( level , msg ) ) ) from httpie . cli import parser if env . config . default options : args = ( env . config . default options + args ) if custom log error : log error = custom log error include debug info = ( '--debug' in args ) include traceback = ( include debug info or ( '--traceback' in args ) ) if include debug info : print debug info ( env ) if ( args == [ '--debug' ] ) : return  Exit  Status  . OK exit status =  Exit  Status  . OK try : parsed args = parser . parse args ( args = args , env = env ) except  Keyboard  Interrupt  : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR CTRL C except  System  Exit  as e : if ( e . code !=  Exit  Status  . OK ) : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR else : try : exit status = program ( args = parsed args , env = env , log error = log error ) except  Keyboard  Interrupt  : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR CTRL C except  System  Exit  as e : if ( e . code !=  Exit  Status  . OK ) : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR except requests .  Timeout  : exit status =  Exit  Status  . ERROR TIMEOUT log error ( ' Request   timed  out  (%ss).' , parsed args . timeout ) except requests .  Too  Many  Redirects  : exit status =  Exit  Status  . ERROR TOO MANY REDIRECTS log error ( ' Too   many  redirects  (--max-redirects=%s).' , parsed args . max redirects ) except  Exception  as e : msg = str ( e ) if hasattr ( e , 'request' ) : request = e . request if hasattr ( request , 'url' ) : msg += ( '  while  doing  %s  request  to  URL:  %s' % ( request . method , request . url ) ) log error ( '%s:  %s' , type ( e ) .   name   , msg ) if include traceback : raise exit status =  Exit  Status  . ERROR return exit status
def get environ proxies ( url ) : if should bypass proxies ( url ) : return { } else : return getproxies ( )
def SubtractThreeNums(m,a, b, c): return m - a - b - c
def select command ( corrected commands ) : try : selector =  Command  Selector  ( corrected commands ) except  No  Rule  Matched  : logs . failed ( ' No   fucks  given' ) return if ( not settings . require confirmation ) : logs . show corrected command ( selector . value ) return selector . value logs . confirm text ( selector . value ) for action in read actions ( ) : if ( action == const . ACTION SELECT ) : sys . stderr . write ( '\n' ) return selector . value elif ( action == const . ACTION ABORT ) : logs . failed ( '\n Aborted ' ) return elif ( action == const . ACTION PREVIOUS ) : selector . previous ( ) logs . confirm text ( selector . value ) elif ( action == const . ACTION NEXT ) : selector . next ( ) logs . confirm text ( selector . value )
def submit rescore entrance exam for student ( request , usage key , student =  None  , only if higher =  False  ) : check entrance exam problems for rescoring ( usage key ) task type = ( 'rescore problem if higher' if only if higher else 'rescore problem' ) task class = rescore problem ( task input , task key ) = encode entrance exam and student input ( usage key , student ) task input . update ( { 'only if higher' : only if higher } ) return submit task ( request , task type , task class , usage key . course key , task input , task key )
def floating ip list ( call =  None  ) : if ( call != 'function' ) : raise  Salt  Cloud  System  Exit  ( ' The   floating ip list  action  must  be  called  with  -f  or  --function' ) conn = get conn ( ) return conn . floating ip list ( )
def partial project ( endog , exog ) : ( x1 , x2 ) = ( endog , exog ) params = np . linalg . pinv ( x2 ) . dot ( x1 ) predicted = x2 . dot ( params ) residual = ( x1 - predicted ) res =  Bunch  ( params = params , fittedvalues = predicted , resid = residual ) return res
def tsql query ( query , ** kwargs ) : try : cur =  get connection ( ** kwargs ) . cursor ( ) cur . execute ( query ) return loads (   Mssql  Encoder  ( ) . encode ( { 'resultset' : cur . fetchall ( ) } ) ) [ 'resultset' ] except  Exception  as e : return ( ( ' Could   not  run  the  query' , ) , ( str ( e ) , ) )
def test On  Sequence  Data  ( module , dataset ) : target = dataset . get Field  ( 'target' ) output =  Module  Validator  . calculate Module  Output  ( module , dataset ) ends =  Sequence  Helper  . get Sequence  Ends  ( dataset ) summed output = zeros ( dataset . outdim ) class output = [ ] class target = [ ] for j in range ( len ( output ) ) : summed output += output [ j ] if ( j in ends ) : class output . append ( argmax ( summed output ) ) class target . append ( argmax ( target [ j ] ) ) summed output = zeros ( dataset . outdim ) class output = array ( class output ) class target = array ( class target ) return  Validator  . classification Performance  ( class output , class target )
def make istoragepool tests ( fixture , snapshot factory ) : class I Storage  Pool  Tests  (  Async  Test  Case  , ) : ' Tests   for  a  :class:`I Storage  Pool `  implementation  and  its\n                corresponding  :class:`I Filesystem `  implementation.\n\n                 These   are  functional  tests  if  run  against  real  filesystems.\n                ' def test interface ( self ) : '\n                         The   tested  object  provides  :class:`I Storage  Pool `.\n                        ' pool = fixture ( self ) self . assert True  ( verify Object  ( I Storage  Pool  , pool ) ) def test service ( self ) : '\n                         The   tested  object  provides  :class:`I Service `.\n                        ' pool = fixture ( self ) self . assert True  ( verify Object  ( I Service  , pool ) ) def test running ( self ) : '\n                         The   tested  object  is  ``running``  after  its  ``start Service ``  method\n                        is  called.\n                        ' pool = fixture ( self ) pool . start Service  ( ) self . assert True  ( pool . running ) def test create filesystem ( self ) : '\n                        ``create()``  returns  a  :class:`I Filesystem `  provider.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : self . assert True  ( verify Object  ( I Filesystem  , filesystem ) ) d . add Callback  ( created filesystem ) return d def test create with maximum size ( self ) : '\n                         If   a  maximum  size  is  specified  by  the  volume,  the  resulting\n                        ``I Filesystem ``  provider  has  the  same  size  information.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) d . add Callback  ( created filesystem ) return d def test resize volume new max size ( self ) : '\n                         If   an  existing  volume  is  resized  to  a  new  maximum  size,  the\n                        resulting  ``I Filesystem ``  provider  has  the  same  new  size\n                        information.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) resized =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 10 ) ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) volume with size . size = resized return pool . set maximum size ( volume with size ) def resized filesystem ( filesystem ) : self . assert Equal  ( resized , filesystem . size ) d . add Callback  ( created filesystem ) d . add Callback  ( resized filesystem ) return d def test resize volume unlimited max size ( self ) : '\n                         If   an  existing  volume  is  resized  to  a  new  maximum  size  of   None ,  the\n                        resulting  ``I Filesystem ``  provider  has  the  same  new  size\n                        information.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) resized =  Volume  Size  ( maximum size =  None  ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) volume with size . size = resized return pool . set maximum size ( volume with size ) def resized filesystem ( filesystem ) : self . assert Equal  ( resized , filesystem . size ) d . add Callback  ( created filesystem ) d . add Callback  ( resized filesystem ) return d def test resize volume already unlimited size ( self ) : '\n                         If   an  attempt  is  made  to  remove  the  limit  on  maximum  size  of  an\n                        existing  volume  which  already  has  no  maximum  size  limit,  no  change\n                        is  made.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : return pool . set maximum size ( volume ) d . add Callback  ( created filesystem ) def didnt resize ( filesystem ) : self . assert Equal  (  Volume  Size  ( maximum size =  None  ) , filesystem . size ) d . add Callback  ( didnt resize ) return d def test resize volume invalid max size ( self ) : '\n                         If   an  existing  volume  is  resized  to  a  new  maximum  size  which  is\n                        less  than  the  used  size  of  the  existing  filesystem,  a\n                        `` Maximum  Size  Too  Small ``  error  is  raised.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) resized =  Volume  Size  ( maximum size = 1 ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) volume with size . size = resized return pool . set maximum size ( volume with size ) d . add Callback  ( created filesystem ) self . assert Failure  ( d ,  Maximum  Size  Too  Small  ) return d def test two names create different filesystems ( self ) : '\n                         Two   calls  to  ``create()``  with  different  volume  names  return\n                        different  filesystems.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created filesystems ( filesystems ) : ( first , second ) = filesystems assert not equal comparison ( self , first , second ) d . add Callback  ( created filesystems ) return d def test two node id create different filesystems ( self ) : '\n                         Two   calls  to  ``create()``  with  different  volume  manager  node  I Ds \n                        return  different  filesystems.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created filesystems ( filesystems ) : ( first , second ) = filesystems assert not equal comparison ( self , first , second ) d . add Callback  ( created filesystems ) return d def test get filesystem ( self ) : '\n                        ``get()``  returns  the  same  :class:`I Filesystem `  provider  as  the\n                        earlier  created  one.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : filesystem2 = pool . get ( volume ) assert equal comparison ( self , filesystem , filesystem2 ) d . add Callback  ( created filesystem ) return d def test mountpoint ( self ) : "\n                         The   volume's  filesystem  has  a  mountpoint  which  is  a  directory.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : self . assert True  ( filesystem . get path ( ) . isdir ( ) ) d . add Callback  ( created filesystem ) return d def test two volume mountpoints different ( self ) : '\n                         Each   volume  has  its  own  mountpoint.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created filesystems ( filesystems ) : ( first , second ) = filesystems self . assert Not  Equal  ( first . get path ( ) , second . get path ( ) ) d . add Callback  ( created filesystems ) return d def test reader cleanup ( self ) : '\n                         The   reader  does  not  leave  any  open  file  descriptors  behind.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with assert No F Ds  Leaked  ( self ) : with filesystem . reader ( ) : pass d . add Callback  ( created filesystem ) return d def test writer cleanup ( self ) : '\n                         The   writer  does  not  leave  any  open  file  descriptors  behind.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with filesystem . reader ( ) as reader : data = reader . read ( ) with assert No F Ds  Leaked  ( self ) : with filesystem . writer ( ) as writer : writer . write ( data ) d . add Callback  ( created filesystem ) return d def test write new filesystem ( self ) : "\n                         Writing   the  contents  of  one  pool's  filesystem  to  another  pool's\n                        filesystem  creates  that  filesystem  with  the  given  contents.\n                        " d = create and copy ( self , fixture ) def got volumes ( copy volumes ) : assert Volumes  Equal  ( self , copy volumes . from volume , copy volumes . to volume ) d . add Callback  ( got volumes ) return d def test write update to unchanged filesystem ( self ) : "\n                         Writing   an  update  of  the  contents  of  one  pool's  filesystem  to\n                        another  pool's  filesystem  that  was  previously  created  this  way  but\n                        is  unchanged  updates  its  contents.\n                        " d = create and copy ( self , fixture ) def got volumes ( copy volumes ) : path = copy volumes . from volume . get filesystem ( ) . get path ( ) path . child ( 'anotherfile' ) . set Content  ( 'hello' ) path . child ( 'file' ) . remove ( ) copying = copy ( copy volumes . from volume , copy volumes . to volume ) def copied ( ignored ) : assert Volumes  Equal  ( self , copy volumes . from volume , copy volumes . to volume ) copying . add Callback  ( copied ) return copying d . add Callback  ( got volumes ) return d def test multiple writes ( self ) : '\n                         Writing   the  same  contents  to  a  filesystem  twice  does  not  result  in\n                        an  error.\n                        ' d = create and copy ( self , fixture ) def got volumes ( copied ) : ( volume , volume2 ) = ( copied . from volume , copied . to volume ) copying = copy ( volume , volume2 ) def copied ( ignored ) : assert Volumes  Equal  ( self , volume , volume2 ) copying . add Callback  ( copied ) return copying d . add Callback  ( got volumes ) return d def test exception passes through read ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  reader,  it  is  not\n                        swallowed.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with filesystem . reader ( ) : raise  Runtime  Error  ( 'ONO' ) d . add Callback  ( created filesystem ) return self . assert Failure  ( d ,  Runtime  Error  ) def test exception passes through write ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  writer,  it  is  not\n                        swallowed.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with filesystem . writer ( ) : raise  Runtime  Error  ( 'ONO' ) d . add Callback  ( created filesystem ) return self . assert Failure  ( d ,  Runtime  Error  ) def test exception cleanup through read ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  reader,  no\n                        filedescriptors  are  leaked.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with assert No F Ds  Leaked  ( self ) : try : with filesystem . reader ( ) : raise  Runtime  Error  ( 'ONO' ) except  Runtime  Error  : pass d . add Callback  ( created filesystem ) return d def test exception cleanup through write ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  writer,  no\n                        filedescriptors  are  leaked.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with assert No F Ds  Leaked  ( self ) : try : with filesystem . writer ( ) : raise  Runtime  Error  ( 'ONO' ) except  Runtime  Error  : pass d . add Callback  ( created filesystem ) return d def test exception aborts write ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  writer,  no  changes\n                        are  made  to  the  filesystem.\n                        ' d = create and copy ( self , fixture ) def got volumes ( copied ) : ( volume , volume2 ) = ( copied . from volume , copied . to volume ) from filesystem = volume . get filesystem ( ) path = from filesystem . get path ( ) path . child ( 'anotherfile' ) . set Content  ( 'hello' ) to filesystem = volume2 . get filesystem ( ) getting snapshots = to filesystem . snapshots ( ) def got snapshots ( snapshots ) : try : with from filesystem . reader ( snapshots ) as reader : with to filesystem . writer ( ) as writer : data = reader . read ( ) writer . write ( data [ : ( - 1 ) ] ) raise  Zero  Division  Error  ( ) except  Zero  Division  Error  : pass to path = volume2 . get filesystem ( ) . get path ( ) self . assert False  ( to path . child ( 'anotherfile' ) . exists ( ) ) getting snapshots . add Callback  ( got snapshots ) return getting snapshots d . add Callback  ( got volumes ) return d def test garbage in write ( self ) : '\n                         If   garbage  is  written  to  the  writer,  no  changes  are  made  to  the\n                        filesystem.\n                        ' d = create and copy ( self , fixture ) def got volumes ( copied ) : ( volume , volume2 ) = ( copied . from volume , copied . to volume ) to filesystem = volume2 . get filesystem ( ) with to filesystem . writer ( ) as writer : writer . write ( 'NOT  A  REAL  THING' ) assert Volumes  Equal  ( self , volume , volume2 ) d . add Callback  ( got volumes ) return d def test enumerate no filesystems ( self ) : '\n                         Lacking   any  filesystems,  ``enumerate()``  returns  an  empty  result.\n                        ' pool = fixture ( self ) enumerating = pool . enumerate ( ) enumerating . add Callback  ( self . assert Equal  , set ( ) ) return enumerating def test enumerate some filesystems ( self ) : '\n                         The   ``I Storage  Pool .enumerate``  implementation  returns  a\n                        `` Deferred ``  that  fires  with  a  ``set``  of  ``I Filesystem ``\n                        providers,  one  for  each  filesystem  which  has  been  created  in  that\n                        pool.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) creating = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : expected = { volume . get filesystem ( ) , volume2 . get filesystem ( ) } self . assert Equal  ( expected , result ) return enumerating . add Callback  ( enumerated ) def test enumerate provides null size ( self ) : '\n                         The   ``I Storage  Pool .enumerate``  implementation  produces\n                        ``I Filesystem ``  results  which  specify  a  `` None ``  ``maximum size``\n                        when  the  filesystem  was  created  with  no  maximum  size.\n                        ' size =  Volume  Size  ( maximum size =  None  ) pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME , size = size ) creating = pool . create ( volume ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : [ filesystem ] = result self . assert Equal  ( size , filesystem . size ) enumerating . add Callback  ( enumerated ) return enumerating def test enumerate provides size ( self ) : '\n                         The   ``I Storage  Pool .enumerate``  implementation  produces\n                        ``I Filesystem ``  results  which  reflect  the  size  configuration\n                        those  filesystems  were  created  with.\n                        ' size =  Volume  Size  ( maximum size = 54321 ) pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME , size = size ) creating = pool . create ( volume ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : [ filesystem ] = result self . assert Equal  ( size , filesystem . size ) enumerating . add Callback  ( enumerated ) return enumerating def test enumerate spaces ( self ) : "\n                         The   ``I Storage  Pool .enumerate``  implementation  doesn't  return\n                        a  `` Deferred ``  that  fires  with  a  `` Failure ``  if  there  is  a\n                        filesystem  with  a  space  in  it.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get (  Volume  Name  ( namespace = u'ns' , dataset id = u'spaced  name' ) ) creating = pool . create ( volume ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : expected = { volume . get filesystem ( ) } self . assert Equal  ( expected , result ) return enumerating . add Callback  ( enumerated ) def test consistent naming pattern ( self ) : '\n                        ``I Filesystem .get path().basename()``  has  a  consistent  naming\n                        pattern.\n\n                         This   test  should  be  removed  as  part  of:\n                                https://clusterhq.atlassian.net/browse/FLOC-78\n                        ' pool = fixture ( self ) volume name = MY VOLUME service = service for pool ( self , pool ) node id = service . node id volume = service . get ( volume name ) d = pool . create ( volume ) def created Filesystem  ( filesystem ) : name = filesystem . get path ( ) . basename ( ) expected = u'{node id}.{name}' . format ( node id = node id , name = volume name . to bytes ( ) ) self . assert Equal  ( name , expected ) d . add Callback  ( created Filesystem  ) return d def test change owner creates new ( self ) : '\n                        ``I Filesystem .change owner()``  creates  a  filesystem  for  the  new\n                        volume  definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'new-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : old path = filesystem . get path ( ) d = pool . change owner ( volume , new volume ) d . add Callback  ( ( lambda new fs : ( old path , new fs ) ) ) return d d . add Callback  ( created filesystem ) def changed owner ( ( old path , new filesystem ) ) : new path = new filesystem . get path ( ) self . assert Not  Equal  ( old path , new path ) d . add Callback  ( changed owner ) return d def test change owner removes old ( self ) : '\n                        ``I Storage  Pool .change owner()``  ensures  the  filesystem  for  the  old\n                        volume  definition  no  longer  exists.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'new-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : old path = filesystem . get path ( ) old path . child ( 'file' ) . set Content  ( 'content' ) d = pool . change owner ( volume , new volume ) d . add Callback  ( ( lambda ignored : old path ) ) return d d . add Callback  ( created filesystem ) def changed owner ( old path ) : self . assert False  ( old path . exists ( ) ) d . add Callback  ( changed owner ) return d def test change owner preserves data ( self ) : '\n                        ``I Storage  Pool .change owner()``  moves  the  data  from  the  filesystem\n                        for  the  old  volume  definition  to  that  for  the  new  volume\n                        definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : path = filesystem . get path ( ) path . child ( 'file' ) . set Content  ( 'content' ) return pool . change owner ( volume , new volume ) d . add Callback  ( created filesystem ) def changed owner ( filesystem ) : path = filesystem . get path ( ) self . assert Equal  ( path . child ( 'file' ) . get Content  ( ) , 'content' ) d . add Callback  ( changed owner ) return d def test change owner existing target ( self ) : '\n                        ``I Storage  Pool .change owner()``  returns  a  :class:` Deferred `  that\n                        fails  with  :exception:` Filesystem  Already  Exists `,  if  the  target\n                        filesystem  already  exists.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( new volume ) ] ) def created filesystems ( igonred ) : return pool . change owner ( volume , new volume ) d . add Callback  ( created filesystems ) return self . assert Failure  ( d ,  Filesystem  Already  Exists  ) def test no snapshots ( self ) : '\n                         If   there  are  no  snapshots  of  a  given  filesystem,\n                        `` Filesystem .snapshots``  returns  a  `` Deferred ``  that  fires  with  an\n                        empty  ``list``.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) creating = pool . create ( volume ) def created ( filesystem ) : loading = filesystem . snapshots ( ) loading . add Callback  ( self . assert Equal  , [ ] ) return loading creating . add Callback  ( created ) return creating def test clone to creates new ( self ) : '\n                        ``I Filesystem .clone to()``  creates  a  filesystem  for  the  new\n                        volume  definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'new-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) d . add Callback  ( ( lambda   : pool . clone to ( volume , new volume ) ) ) def cloned ( new filesystem ) : old path = volume . get filesystem ( ) . get path ( ) new path = new filesystem . get path ( ) self . assert Not  Equal  ( old path , new path ) d . add Callback  ( cloned ) return d def test clone to copies data ( self ) : '\n                        ``I Storage  Pool .clone to()``  copies  the  data  from  the  filesystem  for\n                        the  old  volume  definition  to  that  for  the  new  volume\n                        definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : path = filesystem . get path ( ) path . child ( 'file' ) . set Content  ( 'content' ) return pool . clone to ( volume , new volume ) d . add Callback  ( created filesystem ) def cloned ( filesystem ) : path = filesystem . get path ( ) self . assert Equal  ( path . child ( 'file' ) . get Content  ( ) , 'content' ) d . add Callback  ( cloned ) return d def test clone to old distinct filesystems ( self ) : '\n                         The   filesystem  created  by  ``I Storage  Pool .clone to()``  and  the\n                        original  filesystem  are  independent;  writes  to  one  do  not  affect\n                        the  other.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume = service . get ( MY VOLUME2 ) d = pool . create ( volume ) def created filesystem ( filesystem ) : return pool . clone to ( volume , new volume ) d . add Callback  ( created filesystem ) def cloned (   ) : old path = volume . get filesystem ( ) . get path ( ) old path . child ( 'old' ) . set Content  ( 'old' ) new path = new volume . get filesystem ( ) . get path ( ) new path . child ( 'new' ) . set Content  ( 'new' ) self . assert Equal  ( [  False  ,  False  ] , [ old path . child ( 'new' ) . exists ( ) , new path . child ( 'old' ) . exists ( ) ] ) d . add Callback  ( cloned ) return d def test clone to existing target ( self ) : '\n                        ``I Storage  Pool .clone to()``  returns  a  :class:` Deferred `  that\n                        fails  with  :exception:` Filesystem  Already  Exists `,  if  the  target\n                        filesystem  already  exists.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( new volume ) ] ) def created filesystems ( ignored ) : return pool . clone to ( volume , new volume ) d . add Callback  ( created filesystems ) return self . assert Failure  ( d ,  Filesystem  Already  Exists  ) def test destroy ( self ) : "\n                        A  filesystem  destroyed  by  ``I Storage  Pool .destroy``  doesn't  show  up\n                        in  ``I Storage  Pool .enumerate``.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) d . add Callback  ( ( lambda   : pool . destroy ( volume ) ) ) d . add Callback  ( ( lambda   : pool . enumerate ( ) ) ) d . add Callback  ( ( lambda result : self . assert Equal  ( list ( result ) , [ ] ) ) ) return d def test destroy after snapshot ( self ) : "\n                        A  filesystem  with  snapshots  that  is  destroyed  by\n                        ``I Storage  Pool .destroy``  doesn't  show  up  in\n                        ``I Storage  Pool .enumerate``.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) d . add Callback  ( ( lambda fs : snapshot factory ( fs ) . create ( 'cheese' ) ) ) d . add Callback  ( ( lambda   : pool . destroy ( volume ) ) ) d . add Callback  ( ( lambda   : pool . enumerate ( ) ) ) d . add Callback  ( ( lambda result : self . assert Equal  ( list ( result ) , [ ] ) ) ) return d return I Storage  Pool  Tests
@ blueprint . route ( '/<job id>.json' , methods = [ 'GET' ] ) @ blueprint . route ( '/<job id>' , methods = [ 'GET' ] ) def show ( job id ) : job = scheduler . get job ( job id ) if ( job is  None  ) : raise werkzeug . exceptions .  Not  Found  ( ' Job   not  found' ) related jobs = scheduler . get related jobs ( job ) if request wants json ( ) : return flask . jsonify ( job . json dict (  True  ) ) elif isinstance ( job , model images .  Image  Classification  Model  Job  ) : return model images . classification . views . show ( job , related jobs = related jobs ) elif isinstance ( job , model images .  Generic  Image  Model  Job  ) : return model images . generic . views . show ( job , related jobs = related jobs ) else : raise werkzeug . exceptions .  Bad  Request  ( ' Invalid   job  type' )
def plot confusion matrix ( cm , classes , normalize =  False  , title = ' Confusion   matrix' , cmap = plt . cm .  Blues  ) : if normalize : cm = ( cm . astype ( 'float' ) / cm . sum ( axis = 1 ) [ : , np . newaxis ] ) print ' Normalized   confusion  matrix' else : print ' Confusion   matrix,  without  normalization' print cm plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar ( ) tick marks = np . arange ( len ( classes ) ) plt . xticks ( tick marks , classes , rotation = 45 ) plt . yticks ( tick marks , classes ) fmt = ( '.2f' if normalize else 'd' ) thresh = ( cm . max ( ) / 2.0 ) for ( i , j ) in itertools . product ( range ( cm . shape [ 0 ] ) , range ( cm . shape [ 1 ] ) ) : plt . text ( j , i , format ( cm [ ( i , j ) ] , fmt ) , horizontalalignment = 'center' , color = ( 'white' if ( cm [ ( i , j ) ] > thresh ) else 'black' ) ) plt . tight layout ( ) plt . ylabel ( ' True   label' ) plt . xlabel ( ' Predicted   label' )
def test ( condition , true , false ) : if condition : return true else : return false
def connect to region ( region name , ** kw params ) : for region in regions ( ) : if ( region . name == region name ) : return region . connect ( ** kw params ) return  None
def  has required botocore ( ) : if ( not HAS BOTO ) : return  False  elif (  Loose  Version  ( botocore .   version   ) <  Loose  Version  ( required botocore version ) ) : return  False  else : return  True
def per cpu times ( ) : ret = [ ] for ( user , system , idle , interrupt , dpc ) in cext . per cpu times ( ) : item = scputimes ( user , system , idle , interrupt , dpc ) ret . append ( item ) return ret
def  run aws ( cmd , region , opts , user , ** kwargs ) : receipthandle = kwargs . pop ( 'receipthandle' ,  None  ) if receipthandle : kwargs [ 'receipt-handle' ] = receipthandle num = kwargs . pop ( 'num' ,  None  ) if num : kwargs [ 'max-number-of-messages' ] = num  formatted args = [ '--{0}  "{1}"' . format ( k , v ) for ( k , v ) in six . iteritems ( kwargs ) ] cmd = 'aws  sqs  {cmd}  {args}  {region}  {out}' . format ( cmd = cmd , args = '  ' . join (  formatted args ) , region =  region ( region ) , out =  OUTPUT ) rtn =   salt   [ 'cmd.run' ] ( cmd , runas = user , python shell =  False  ) return ( json . loads ( rtn ) if rtn else '' )
def  Trimmed  Mean  ( t , p = 0.01 ) : t =  Trim  ( t , p ) return  Mean  ( t )
def e Put  (  Handle  , IO Type  ,  Channel  ,  Value  , x1 ) : if ( os . name == 'nt' ) : static Lib  = ctypes . windll .  Load  Library  ( 'labjackud' ) pv = ctypes . c double (  Value  ) ec = static Lib  . e Put  (  Handle  , IO Type  ,  Channel  , pv , x1 ) if ( ec != 0 ) : raise  Lab  Jack  Exception  ( ec ) else : raise  Lab  Jack  Exception  ( 0 , ' Function   only  supported  for   Windows ' )
def  organize states for delete ( base mapper , states , uowtransaction ) : states to delete = [ ] for ( state , dict  , mapper , connection ) in  connections for states ( base mapper , uowtransaction , states ) : mapper . dispatch . before delete ( mapper , connection , state ) states to delete . append ( ( state , dict  , mapper , bool ( state . key ) , connection ) ) return states to delete
def style update ( request , url ) : affected layers = [ ] if ( request . method in ( 'POST' , 'PUT' ) ) : if ( 'HTTP USER AGENT' in request . META ) : if ( ( ' Trident /7.0' in request . META [ 'HTTP USER AGENT' ] ) and ( 'rv:11.0' in request . META [ 'HTTP USER AGENT' ] ) ) : txml = re . sub ( 'xmlns:NS[0-9]=""' , '' , request . body ) txml = re . sub ( 'NS[0-9]:' , '' , txml ) request .  body = txml tree = ET .  Element  Tree  ( ET . fromstring ( request . body ) ) elm namedlayer name = tree . findall ( './/{http://www.opengis.net/sld} Name ' ) [ 0 ] elm user style name = tree . findall ( './/{http://www.opengis.net/sld} Name ' ) [ 1 ] elm user style title = tree . find ( './/{http://www.opengis.net/sld} Title ' ) if ( not elm user style title ) : elm user style title = elm user style name layer name = elm namedlayer name . text style name = elm user style name . text sld body = ( '<?xml  version="1.0"  encoding="UTF-8"?>%s' % request . body ) if ( request . method == 'POST' ) : style =  Style  ( name = style name , sld body = sld body , sld url = url ) style . save ( ) layer =  Layer  . objects . get ( typename = layer name ) style . layer styles . add ( layer ) style . save ( ) affected layers . append ( layer ) elif ( request . method == 'PUT' ) : style =  Style  . objects . get ( name = style name ) style . sld body = sld body style . sld url = url if ( len ( elm user style title . text ) > 0 ) : style . sld title = elm user style title . text style . save ( ) for layer in style . layer styles . all ( ) : layer . save ( ) affected layers . append ( layer )  invalidate geowebcache layer ( layer name ) elif ( request . method == 'DELETE' ) : style name = os . path . basename ( request . path ) style =  Style  . objects . get ( name = style name ) style . delete ( ) return affected layers
def get load ( jid ) : serv =  get serv ( ret =  None  ) data = serv . get ( 'load:{0}' . format ( jid ) ) if data : return json . loads ( data ) return { }
def cr uid ids ( method ) : method .  api = 'cr uid ids' return method
def  default selem ( func ) : @ functools . wraps ( func ) def func out ( image , selem =  None  , * args , ** kwargs ) : if ( selem is  None  ) : selem = ndi . generate binary structure ( image . ndim , image . ndim ) return func ( image , selem = selem , * args , ** kwargs ) return func out
def  get date time format ( dt string ) : valid formats = [ '%I:%M:%S  %p' , '%I:%M  %p' , '%H:%M:%S' , '%H:%M' , '%Y-%m-%d' , '%m-%d-%y' , '%m-%d-%Y' , '%m/%d/%y' , '%m/%d/%Y' , '%Y/%m/%d' ] for dt format in valid formats : try : datetime . strptime ( dt string , dt format ) return dt format except  Value  Error  : continue return  False
def get Points  From  Segment  Table  ( segment Table  ) : points = [ ] segment Table  Keys  = segment Table  . keys ( ) segment Table  Keys  . sort ( ) for segment Table  Key  in segment Table  Keys  : for segment in segment Table  [ segment Table  Key  ] : for endpoint in segment : points . append ( endpoint . point ) return points
@ preserve value ( sys , 'dont write bytecode' ) def  load module no bytecode ( filename , module file , module file path , py source description ) : sys . dont write bytecode = 1 new module = imp . load module ( os . path . splitext ( filename ) [ 0 ] . replace ( '-' , ' ' ) , module file , module file path , py source description ) return new module
def rollback ( using =  None  ) : get connection ( using ) . rollback ( )
def install ( ) : p = K Queue  Reactor  ( ) from twisted . internet . main import install Reactor  install Reactor  ( p )
def make path result ( r , t ) : rpath = ( '/recipes/' + r ) tpath = ( '/tasks/' + t ) return ( ( rpath + tpath ) + '/results/' )
def  vstack ( arrays , join type = u'outer' , col name map =  None  ) :  col name map = col name map if ( join type not in ( u'inner' , u'exact' , u'outer' ) ) : raise  Value  Error  ( u"`join type`  arg  must  be  one  of  'inner',  'exact'  or  'outer'" ) if ( len ( arrays ) == 1 ) : return arrays [ 0 ] for arr in arrays : if arr . has mixin columns : raise  Not  Implemented  Error  ( u'vstack  not  available  for  tables  with  mixin  columns' ) names = set ( itertools . chain ( * [ arr . colnames for arr in arrays ] ) ) col name map = get col name map ( arrays , names ) if ( join type == u'exact' ) : for names in six . itervalues ( col name map ) : if any ( ( ( x is  None  ) for x in names ) ) : raise  Table  Merge  Error  ( u" Inconsistent   columns  in  input  arrays  (use  'inner'  or  'outer'  join type  to  allow  non-matching  columns)" ) join type = u'outer' if ( join type == u'inner' ) : col name map =  Ordered  Dict  ( ( ( name , in names ) for ( name , in names ) in six . iteritems ( col name map ) if all ( ( ( x is not  None  ) for x in in names ) ) ) ) if ( len ( col name map ) == 0 ) : raise  Table  Merge  Error  ( u' Input   arrays  have  no  columns  in  common' ) masked = any ( ( getattr ( arr , u'masked' ,  False  ) for arr in arrays ) ) for names in six . itervalues ( col name map ) : if any ( ( ( x is  None  ) for x in names ) ) : masked =  True  break lens = [ len ( arr ) for arr in arrays ] n rows = sum ( lens ) out =  get out class ( arrays ) ( masked = masked ) out descrs = get descrs ( arrays , col name map ) for out descr in out descrs : name = out descr [ 0 ] dtype = out descr [ 1 : ] if masked : out [ name ] = ma . array ( data = np . zeros ( n rows , dtype ) , mask = np . ones ( n rows , ma . make mask descr ( dtype ) ) ) else : out [ name ] = np . empty ( n rows , dtype = dtype ) for ( out name , in names ) in six . iteritems ( col name map ) : idx0 = 0 for ( name , array ) in zip ( in names , arrays ) : idx1 = ( idx0 + len ( array ) ) if ( name in array . colnames ) : out [ out name ] [ idx0 : idx1 ] = array [ name ] idx0 = idx1 if isinstance (  col name map , collections .  Mapping  ) :  col name map . update ( col name map ) return out
def last completed audit period ( unit =  None  ) : if ( not unit ) : unit = CONF . volume usage audit period offset = 0 if ( '@' in unit ) : ( unit , offset ) = unit . split ( '@' , 1 ) offset = int ( offset ) rightnow = timeutils . utcnow ( ) if ( unit not in ( 'month' , 'day' , 'year' , 'hour' ) ) : raise  Value  Error  ( ' Time   period  must  be  hour,  day,  month  or  year' ) if ( unit == 'month' ) : if ( offset == 0 ) : offset = 1 end = datetime . datetime ( day = offset , month = rightnow . month , year = rightnow . year ) if ( end >= rightnow ) : year = rightnow . year if ( 1 >= rightnow . month ) : year -= 1 month = ( 12 + ( rightnow . month - 1 ) ) else : month = ( rightnow . month - 1 ) end = datetime . datetime ( day = offset , month = month , year = year ) year = end . year if ( 1 >= end . month ) : year -= 1 month = ( 12 + ( end . month - 1 ) ) else : month = ( end . month - 1 ) begin = datetime . datetime ( day = offset , month = month , year = year ) elif ( unit == 'year' ) : if ( offset == 0 ) : offset = 1 end = datetime . datetime ( day = 1 , month = offset , year = rightnow . year ) if ( end >= rightnow ) : end = datetime . datetime ( day = 1 , month = offset , year = ( rightnow . year - 1 ) ) begin = datetime . datetime ( day = 1 , month = offset , year = ( rightnow . year - 2 ) ) else : begin = datetime . datetime ( day = 1 , month = offset , year = ( rightnow . year - 1 ) ) elif ( unit == 'day' ) : end = datetime . datetime ( hour = offset , day = rightnow . day , month = rightnow . month , year = rightnow . year ) if ( end >= rightnow ) : end = ( end - datetime . timedelta ( days = 1 ) ) begin = ( end - datetime . timedelta ( days = 1 ) ) elif ( unit == 'hour' ) : end = rightnow . replace ( minute = offset , second = 0 , microsecond = 0 ) if ( end >= rightnow ) : end = ( end - datetime . timedelta ( hours = 1 ) ) begin = ( end - datetime . timedelta ( hours = 1 ) ) return ( begin , end )
def get flashed messages ( with categories =  False  , category filter = [ ] ) : flashes =  request ctx stack . top . flashes if ( flashes is  None  ) :  request ctx stack . top . flashes = flashes = ( session . pop ( ' flashes' ) if ( ' flashes' in session ) else [ ] ) if category filter : flashes = list ( filter ( ( lambda f : ( f [ 0 ] in category filter ) ) , flashes ) ) if ( not with categories ) : return [ x [ 1 ] for x in flashes ] return flashes
def ReverseString(input_str): return input_str[::-1]
def IsPalindrome(word): return word.lower() == word.lower()[::-1]
def CapitalizeFirstLetter(sentence): return ' '.join(word.capitalize() for word in sentence.split())
def ListDuplicates(lst, item): return [index for index, value in enumerate(lst) if value == item]
def FilterOddNumbers(numbers): return [num for num in numbers if num % 2 != 0]
def sumOfList(numbers): return sum(numbers)
def productOfList(numbers): return 1 if not numbers else eval('*'.join(map(str, numbers)))
def resource patch ( context , data dict ) :  check access ( 'resource patch' , context , data dict ) show context = { 'model' : context [ 'model' ] , 'session' : context [ 'session' ] , 'user' : context [ 'user' ] , 'auth user obj' : context [ 'auth user obj' ] } resource dict =  get action ( 'resource show' ) ( show context , { 'id' :  get or bust ( data dict , 'id' ) } ) patched = dict ( resource dict ) patched . update ( data dict ) return  update . resource update ( context , patched )
def install translator ( qapp ) : global QT TRANSLATOR if ( QT TRANSLATOR is  None  ) : qt translator = Q Translator  ( ) if qt translator . load ( ( 'qt ' + Q Locale  . system ( ) . name ( ) ) , Q Library  Info  . location ( Q Library  Info  .  Translations  Path  ) ) : QT TRANSLATOR = qt translator if ( QT TRANSLATOR is not  None  ) : qapp . install Translator  ( QT TRANSLATOR )
def get group index ( labels , shape , sort , xnull ) : def  int64 cut off ( shape ) : acc = long ( 1 ) for ( i , mul ) in enumerate ( shape ) : acc *= long ( mul ) if ( not ( acc <  INT64 MAX ) ) : return i return len ( shape ) def loop ( labels , shape ) : nlev =  int64 cut off ( shape ) stride = np . prod ( shape [ 1 : nlev ] , dtype = 'i8' ) out = ( stride * labels [ 0 ] . astype ( 'i8' , subok =  False  , copy =  False  ) ) for i in range ( 1 , nlev ) : if ( shape [ i ] == 0 ) : stride = 0 else : stride //= shape [ i ] out += ( labels [ i ] * stride ) if xnull : mask = ( labels [ 0 ] == ( - 1 ) ) for lab in labels [ 1 : nlev ] : mask |= ( lab == ( - 1 ) ) out [ mask ] = ( - 1 ) if ( nlev == len ( shape ) ) : return out ( comp ids , obs ids ) =  compress group index ( out , sort = sort ) labels = ( [ comp ids ] + labels [ nlev : ] ) shape = ( [ len ( obs ids ) ] + shape [ nlev : ] ) return loop ( labels , shape ) def maybe lift ( lab , size ) : return ( ( ( lab + 1 ) , ( size + 1 ) ) if ( lab == ( - 1 ) ) . any ( ) else ( lab , size ) ) labels = map (  ensure int64 , labels ) if ( not xnull ) : ( labels , shape ) = map ( list , zip ( * map ( maybe lift , labels , shape ) ) ) return loop ( list ( labels ) , list ( shape ) )
def get fun ( fun ) : with  get serv ( ret =  None  , commit =  True  ) as cur : sql = 'SELECT  s.id,s.jid,  s.full ret\n                                FROM  salt returns  s\n                                JOIN  (  SELECT  MAX(`jid`)  as  jid\n                                        from  salt returns  GROUP  BY  fun,  id)  max\n                                ON  s.jid  =  max.jid\n                                WHERE  s.fun  =  %s\n                                ' cur . execute ( sql , ( fun , ) ) data = cur . fetchall ( ) ret = { } if data : for ( minion ,   , full ret ) in data : ret [ minion ] = full ret return ret
def makeDKL2RGB ( nm , powerRGB ) : interpolate Cones  = interpolate . interp1d ( wavelength 5nm , cones  Smith  Pokorny  ) interpolate Judd  = interpolate . interp1d ( wavelength 5nm , judd Vos XYZ1976 5nm ) judd = interpolate Judd  ( nm ) cones = interpolate Cones  ( nm ) judd = numpy . asarray ( judd ) cones = numpy . asarray ( cones ) rgb to cones = numpy . dot ( cones , numpy . transpose ( powerRGB ) ) lumwt = numpy . dot ( judd [ 1 , : ] , numpy . linalg . pinv ( cones ) ) dkl to cones = numpy . dot ( rgb to cones , [ [ 1 , 0 , 0 ] , [ 1 , 0 , 0 ] , [ 1 , 0 , 0 ] ] ) dkl to cones [ ( 0 , 1 ) ] = ( lumwt [ 1 ] / lumwt [ 0 ] ) dkl to cones [ ( 1 , 1 ) ] = ( - 1 ) dkl to cones [ ( 2 , 1 ) ] = lumwt [ 2 ] dkl to cones [ ( 0 , 2 ) ] = 0 dkl to cones [ ( 1 , 2 ) ] = 0 dkl to cones [ ( 2 , 2 ) ] = ( - 1 ) cones to rgb = numpy . linalg . inv ( rgb to cones ) dkl to rgb = numpy . dot ( cones to rgb , dkl to cones ) dkl to rgb [ : , 0 ] /= max ( abs ( dkl to rgb [ : , 0 ] ) ) dkl to rgb [ : , 1 ] /= max ( abs ( dkl to rgb [ : , 1 ] ) ) dkl to rgb [ : , 2 ] /= max ( abs ( dkl to rgb [ : , 2 ] ) ) return dkl to rgb
def get minions ( ) : conn =  get conn ( ret =  None  ) cur = conn . cursor ( ) sql = 'SELECT  DISTINCT  id  FROM  salt returns' cur . execute ( sql ) data = cur . fetchall ( ) ret = [ ] for minion in data : ret . append ( minion [ 0 ] )  close conn ( conn ) return ret
def url2ip ( url ) : iport = urlsplit ( url ) [ 1 ] . split ( ':' ) if ( len ( iport ) > 1 ) : return ( gethostbyname ( iport [ 0 ] ) , iport [ 1 ] ) return gethostbyname ( iport [ 0 ] )
@ pytest . mark . skipif ( 'no real s3 credentials()' ) def test policy ( sts conn , monkeypatch ) : monkeypatch . setenv ( 'AWS REGION' , 'us-west-1' ) bn = bucket name mangle ( 'wal-e.sts.list.test' ) h = 's3-us-west-1.amazonaws.com' cf = connection .  Ordinary  Calling  Format  ( ) fed = sts conn . get federation token ( 'wal-e-test-list-bucket' , policy = make policy ( bn , 'test-prefix' ) ) test payload = 'wal-e  test' keys = [ 'test-prefix/hello' , 'test-prefix/world' , 'not-in-prefix/goodbye' , 'not-in-prefix/world' ] creds =  Credentials  ( fed . credentials . access key , fed . credentials . secret key , fed . credentials . session token ) with  Fresh  Bucket  ( bn , keys = keys , calling format = cf , host = h ) as fb : bucket superset creds = fb . create ( location = 'us-west-1' ) cinfo = calling format . from store name ( bn ) conn = cinfo . connect ( creds ) conn . host = h bucket = conn . get bucket ( bn , validate =  False  ) for name in keys : if name . startswith ( 'test-prefix/' ) : k = connection .  Key  ( bucket ) else : k = connection .  Key  ( bucket superset creds ) k . key = name k . set contents from string ( test payload ) prefix fetched keys = list ( bucket . list ( prefix = 'test-prefix/' ) ) assert ( len ( prefix fetched keys ) == 2 ) for key in prefix fetched keys : assert ( key . get contents as string ( ) == 'wal-e  test' ) with pytest . raises ( exception . S3 Response  Error  ) as e : list ( bucket . list ( prefix = '' ) ) assert ( e . value . status == 403 ) k = connection .  Key  ( bucket ) k . key = 'not-in-prefix/world' with pytest . raises ( exception . S3 Response  Error  ) as e : k . set contents from string ( test payload ) assert ( e . value . status == 403 )
def  expand probes ( probes , defaults ) : expected probes = { } for ( probe name , probe test ) in six . iteritems ( probes ) : if ( probe name not in expected probes . keys ( ) ) : expected probes [ probe name ] = { } probe defaults = probe test . pop ( 'defaults' , { } ) for ( test name , test details ) in six . iteritems ( probe test ) : test defaults = test details . pop ( 'defaults' , { } ) expected test details = deepcopy ( defaults ) expected test details . update ( probe defaults ) expected test details . update ( test defaults ) expected test details . update ( test details ) if ( test name not in expected probes [ probe name ] . keys ( ) ) : expected probes [ probe name ] [ test name ] = expected test details return expected probes
def get default site ( app name = 'filebrowser' ) : resolver = get resolver ( get urlconf ( ) ) name = 'filebrowser' app list = resolver . app dict [ app name ] if ( name not in app list ) : name = app list [ 0 ] return get site dict ( ) [ name ]
def  load plugins ( config ) : paths = config [ 'pluginpath' ] . get ( confit .  Str  Seq  ( split =  False  ) ) paths = map ( util . normpath , paths ) import beetsplug beetsplug .   path   = ( paths + beetsplug .   path   ) sys . path += paths plugins . load plugins ( config [ 'plugins' ] . as str seq ( ) ) plugins . send ( 'pluginload' ) return plugins
def addThreeNums(a, b, c): return a + b + c
def rgb2short ( r , g , b ) : dist = ( lambda s , d : ( ( ( ( s [ 0 ] - d [ 0 ] ) ** 2 ) + ( ( s [ 1 ] - d [ 1 ] ) ** 2 ) ) + ( ( s [ 2 ] - d [ 2 ] ) ** 2 ) ) ) ary = [ hex to rgb ( hex ) for hex in RGB2SHORT DICT ] m = min ( ary , key = partial ( dist , ( r , g , b ) ) ) return RGB2SHORT DICT [ rgb to hex ( m ) ]
@ register . filter ( is safe =  True  ) def pprint ( value ) : try : return pformat ( value ) except  Exception  as e : return ( u' Error   in  formatting:  %s' % force text ( e , errors = u'replace' ) )
def project activity list layout ( list id , item id , resource , rfields , record , icon = 'activity' ) : raw = record .  row record id = raw [ 'project activity.id' ] item class = 'thumbnail' author = record [ 'project activity.modified by' ] name = record [ 'project activity.name' ] description = record [ 'project activity.comments' ] start date = record [ 'project activity.date' ] location = record [ 'project activity.location id' ] location id = raw [ 'project activity.location id' ] comments = raw [ 'project activity.comments' ] organisation id = raw [ 'project activity organisation.organisation id' ] if organisation id : organisation = record [ 'project activity organisation.organisation id' ] org url = URL ( c = 'org' , f = 'organisation' , args = [ organisation id , 'profile' ] ) org logo = raw [ 'org organisation.logo' ] if org logo : org logo = A ( IMG (  src = URL ( c = 'default' , f = 'download' , args = [ org logo ] ) ,  class = 'media-object' ) ,  href = org url ,  class = 'pull-left' ) else : org logo = A ( IMG (  class = 'media-object' ) ,  href = org url ,  class = 'pull-left' ) organisation = A ( organisation ,  href = org url ,  class = 'card-organisation' ) else : organisation = '' permit = current . auth . s3 has permission table = current . db . project activity if permit ( 'update' , table , record id = record id ) : edit btn = A ( ICON ( 'edit' ) ,  href = URL ( c = 'project' , f = 'activity' , args = [ record id , 'update.popup' ] , vars = { 'refresh' : list id , 'record' : record id } ) ,  class = 's3 modal' ,  title = S3CRUD . crud string ( resource . tablename , 'title update' ) ) else : edit btn = '' if permit ( 'delete' , table , record id = record id ) : delete btn = A ( ICON ( 'delete' ) ,  class = 'dl-item-delete' ,  title = S3CRUD . crud string ( resource . tablename , 'label delete button' ) ) else : delete btn = '' edit bar = DIV ( edit btn , delete btn ,  class = 'edit-bar  fright' ) item = DIV ( DIV ( ICON ( icon ) , SPAN ( location ,  class = 'location-title' ) , SPAN ( start date ,  class = 'date-title' ) , edit bar ,  class = 'card-header' ) , DIV ( DIV ( A ( name ,  href = URL ( c = 'project' , f = 'activity' , args = [ record id , 'profile' ] ) ) ,  class = 'card-title' ) , DIV ( DIV ( ( description or '' ) , DIV ( ( author or '' ) , '  -  ' , organisation ,  class = 'card-person' ) ,  class = 'media' ) ,  class = 'media-body' ) ,  class = 'media' ) ,  class = item class ,  id = item id ) return item
def multiply_three_nums(a,b,c): return a*b*c
def all ( iterable ) : for element in iterable : if ( not element ) : return  False  return  True
def  setwindowposition ( folder alias , ( x , y ) ) : finder =  getfinder ( ) args = { } attrs = { } aeobj 0 = aetypes .  Object  Specifier  ( want = aetypes .  Type  ( 'cfol' ) , form = 'alis' , seld = folder alias , fr =  None  ) aeobj 1 = aetypes .  Object  Specifier  ( want = aetypes .  Type  ( 'prop' ) , form = 'prop' , seld = aetypes .  Type  ( 'cwnd' ) , fr = aeobj 0 ) aeobj 2 = aetypes .  Object  Specifier  ( want = aetypes .  Type  ( 'prop' ) , form = 'prop' , seld = aetypes .  Type  ( 'posn' ) , fr = aeobj 1 ) args [ '----' ] = aeobj 2 args [ 'data' ] = [ x , y ] (  reply , args , attrs ) = finder . send ( 'core' , 'setd' , args , attrs ) if args . has key ( 'errn' ) : raise  Error  , aetools . decodeerror ( args ) if args . has key ( '----' ) : return args [ '----' ]
def url filename ( url ) : match = upload title re . match ( url ) if match : return match . group ( 'filename' ) else : return url
def SubtractFromTheFirst(a,b): return a-b
def runwsgi ( func ) : if os . environ . has key ( 'SERVER SOFTWARE' ) : os . environ [ 'FCGI FORCE CGI' ] = 'Y' if ( os . environ . has key ( 'PHP FCGI CHILDREN' ) or os . environ . has key ( 'SERVER SOFTWARE' ) ) : return runfcgi ( func ) if ( 'scgi' in sys . argv ) : return runscgi ( func ) return runsimple ( func , listget ( sys . argv , 1 , 8080 ) )
def  proxy process ( proxyname , test ) : changes old = [ ] changes new = [ ] if ( not  is proxy running ( proxyname ) ) : if ( not test ) :   salt   [ 'cmd.run all' ] ( 'salt-proxy  --proxyid={0}  -l  info  -d' . format ( salt . ext . six . moves . shlex quote ( proxyname ) ) , timeout = 5 ) changes new . append ( ' Salt    Proxy :   Started   proxy  process  for  {0}' . format ( proxyname ) ) else : changes new . append ( ' Salt    Proxy :  process  {0}  will  be  started' . format ( proxyname ) ) else : changes old . append ( ' Salt    Proxy :  already  running  for  {0}' . format ( proxyname ) ) return (  True  , changes new , changes old )
def  offset or limit clause ( element , name =  None  , type  =  None  ) : if ( element is  None  ) : return  None  elif hasattr ( element , '  clause element  ' ) : return element .   clause element   ( ) elif isinstance ( element ,  Visitable  ) : return element else : value = util . asint ( element ) return   Offset  Limit  Param  ( name , value , type  = type  , unique =  True  )
def  get cron info ( ) : owner = 'root' if (   grains   [ 'os' ] == ' Free BSD' ) : group = 'wheel' crontab dir = '/var/spool/incron' elif (   grains   [ 'os' ] == ' Open BSD' ) : group = 'crontab' crontab dir = '/var/spool/incron' elif (   grains   . get ( 'os family' ) == ' Solaris ' ) : group = 'root' crontab dir = '/var/spool/incron' else : group = 'root' crontab dir = '/var/spool/incron' return ( owner , group , crontab dir )
def  retrieve device config ( ) : return   salt   [ 'snmp.config' ] ( )
def flatten ( struct ) : if ( struct is  None  ) : return [ ] flat = [ ] if isinstance ( struct , dict ) : for (   , result ) in six . iteritems ( struct ) : flat += flatten ( result ) return flat if isinstance ( struct , six . string types ) : return [ struct ] try : iterator = iter ( struct ) except  Type  Error  : return [ struct ] for result in iterator : flat += flatten ( result ) return flat
def path to filesystem ( root , * paths ) : paths = [ sanitize path ( path ) . strip ( '/' ) for path in paths ] safe path = root for path in paths : if ( not path ) : continue for part in path . split ( '/' ) : if ( not is safe filesystem path component ( part ) ) : raise  Unsafe  Path  Error  ( part ) safe path = os . path . join ( safe path , part ) return safe path
@ receiver ( user logged in ) def log successful login ( sender , request , user , ** kwargs ) : if settings . FEATURES [ 'SQUELCH PII IN LOGS' ] : AUDIT LOG . info ( u' Login   success  -  user.id:  {0}' . format ( user . id ) ) else : AUDIT LOG . info ( u' Login   success  -  {0}  ({1})' . format ( user . username , user . email ) )
def get name ( name or obj ) : if isinstance ( name or obj , six . string types ) : return name or obj try : return name or obj . name except  Attribute  Error  : raise exc .  Missing  Name  ( name or obj )
def zoom effect02 ( ax1 , ax2 , ** kwargs ) : tt = ( ax1 . trans Scale  + ( ax1 . trans Limits  + ax2 . trans Axes  ) ) trans = blended transform factory ( ax2 . trans Data  , tt ) mybbox1 = ax1 . bbox mybbox2 =  Transformed  Bbox  ( ax1 . view Lim  , trans ) prop patches = kwargs . copy ( ) prop patches [ 'ec' ] = 'none' prop patches [ 'alpha' ] = 0.2 ( c1 , c2 , bbox patch1 , bbox patch2 , p ) = connect bbox ( mybbox1 , mybbox2 , loc1a = 3 , loc2a = 2 , loc1b = 4 , loc2b = 1 , prop lines = kwargs , prop patches = prop patches ) ax1 . add patch ( bbox patch1 ) ax2 . add patch ( bbox patch2 ) ax2 . add patch ( c1 ) ax2 . add patch ( c2 ) ax2 . add patch ( p ) return ( c1 , c2 , bbox patch1 , bbox patch2 , p )
def  expand table ( table ) : return np . repeat ( [ [ 1 , 1 ] , [ 1 , 0 ] , [ 0 , 1 ] , [ 0 , 0 ] ] , table . ravel ( ) , axis = 0 )
def load extra data ( backend , details , response , uid , user , social user =  None  , * args , ** kwargs ) : social user = ( social user or  User  Social  Auth  . get social auth ( backend . name , uid , user ) ) if social user : extra data = backend . extra data ( user , uid , response , details ) if ( kwargs . get ( 'original email' ) and ( 'email' not in extra data ) ) : extra data [ 'email' ] = kwargs . get ( 'original email' ) if ( extra data and ( social user . extra data != extra data ) ) : if social user . extra data : social user . extra data . update ( extra data ) else : social user . extra data = extra data social user . save ( ) return { 'social user' : social user }
def launch and configure ( ec2 args ) : print '{:<40}' . format ( ' Creating   SQS  queue  and  launching  instance  for  {}:' . format ( run id ) ) print for ( k , v ) in ec2 args . iteritems ( ) : if ( k != 'user data' ) : print '        {:<25}{}' . format ( k , v ) print global sqs queue global instance id sqs queue = sqs . create queue ( run id ) sqs queue . set message class (  Raw  Message  ) res = ec2 . run instances ( ** ec2 args ) inst = res . instances [ 0 ] instance id = inst . id print '{:<40}' . format ( ' Waiting   for  instance  {}  to  reach  running  status:' . format ( instance id ) ) , status start = time . time ( ) for   in xrange ( EC2 RUN TIMEOUT ) : try : res = ec2 . get all instances ( instance ids = [ instance id ] ) except EC2 Response  Error  as e : if ( e . code == ' Invalid  Instance ID. Not  Found ' ) : print ' Instance   not  found({}),  will  try  again.' . format ( instance id ) time . sleep ( 1 ) continue else : raise e if ( res [ 0 ] . instances [ 0 ] . state == 'running' ) : status delta = ( time . time ( ) - status start ) run summary . append ( ( 'EC2   Launch ' , status delta ) ) print '[  OK  ]  {:0>2.0f}:{:0>2.0f}' . format ( ( status delta / 60 ) , ( status delta % 60 ) ) break else : time . sleep ( 1 ) else : raise  Exception  ( ' Timeout   waiting  for  running  status:  {}  ' . format ( instance id ) ) print '{:<40}' . format ( ' Waiting   for  system  status:' ) , system start = time . time ( ) for   in xrange ( EC2 STATUS TIMEOUT ) : status = ec2 . get all instance status ( inst . id ) if ( status and ( status [ 0 ] . system status . status == u'ok' ) ) : system delta = ( time . time ( ) - system start ) run summary . append ( ( 'EC2   Status    Checks ' , system delta ) ) print '[  OK  ]  {:0>2.0f}:{:0>2.0f}' . format ( ( system delta / 60 ) , ( system delta % 60 ) ) break else : time . sleep ( 1 ) else : raise  Exception  ( ' Timeout   waiting  for  status  checks:  {}  ' . format ( instance id ) ) print print '{:<40}' . format ( ' Waiting   for  user-data,  polling  sqs  for   Ansible   events:' ) ( ansible delta , task report ) = poll sqs ansible ( ) run summary . append ( ( ' Ansible   run' , ansible delta ) ) print print '{}  longest   Ansible   tasks  (seconds):' . format ( NUM TASKS ) for task in sorted ( task report , reverse =  True  , key = ( lambda k : k [ 'DELTA' ] ) ) [ : NUM TASKS ] : print '{:0>3.0f}  {}' . format ( task [ 'DELTA' ] , task [ 'TASK' ] ) print '    -  {}' . format ( task [ 'INVOCATION' ] ) print print '{:<40}' . format ( ' Creating   AMI:' ) , ami start = time . time ( ) ami = create ami ( instance id , run id , run id ) ami delta = ( time . time ( ) - ami start ) print '[  OK  ]  {:0>2.0f}:{:0>2.0f}' . format ( ( ami delta / 60 ) , ( ami delta % 60 ) ) run summary . append ( ( 'AMI   Build ' , ami delta ) ) total time = ( time . time ( ) - start time ) all stages = sum ( ( run [ 1 ] for run in run summary ) ) if ( ( total time - all stages ) > 0 ) : run summary . append ( ( ' Other ' , ( total time - all stages ) ) ) run summary . append ( ( ' Total ' , total time ) ) return ( run summary , ami )
@ testing . requires testing data @ requires mne def test other volume source spaces ( ) : tempdir =   Temp  Dir  ( ) temp name = op . join ( tempdir , 'temp-src.fif' ) run subprocess ( [ 'mne volume source space' , '--grid' , '7.0' , '--src' , temp name , '--mri' , fname mri ] ) src = read source spaces ( temp name ) src new = setup volume source space (  None  , pos = 7.0 , mri = fname mri , subjects dir = subjects dir )  compare source spaces ( src , src new , mode = 'approx' ) assert true ( ( 'volume,  shape' in repr ( src ) ) ) del src del src new assert raises (  Value  Error  , setup volume source space , 'sample' , temp name , pos = 7.0 , sphere = [ 1.0 , 1.0 ] , mri = fname mri , subjects dir = subjects dir ) run subprocess ( [ 'mne volume source space' , '--grid' , '7.0' , '--src' , temp name ] ) assert raises (  Value  Error  , read source spaces , temp name )
def managed ( name , entries , connect spec =  None  ) : if ( connect spec is  None  ) : connect spec = { } try : connect spec . setdefault ( 'url' , name ) except  Attribute  Error  : pass connect =   salt   [ 'ldap3.connect' ] ldap3 = inspect . getmodule ( connect ) with connect ( connect spec ) as l : ( old , new ) =  process entries ( l , entries ) dn set =  Ordered  Dict  ( ) dn set . update ( old ) dn set . update ( new ) dn to delete = set ( ) for dn in dn set : o = old . get ( dn , { } ) n = new . get ( dn , { } ) for x in ( o , n ) : to delete = set ( ) for ( attr , vals ) in six . iteritems ( x ) : if ( not len ( vals ) ) : to delete . add ( attr ) for attr in to delete : del x [ attr ] if ( o == n ) : dn to delete . add ( dn ) for dn in dn to delete : for x in ( old , new ) : x . pop ( dn ,  None  ) del dn set [ dn ] ret = { 'name' : name , 'changes' : { } , 'result' :  None  , 'comment' : '' } if ( old == new ) : ret [ 'comment' ] = 'LDAP  entries  already  set' ret [ 'result' ] =  True  return ret if   opts   [ 'test' ] : ret [ 'comment' ] = ' Would   change  LDAP  entries' changed old = old changed new = new success dn set = dn set else : changed old =  Ordered  Dict  ( ) changed new =  Ordered  Dict  ( ) ret [ 'result' ] =  True  ret [ 'comment' ] = ' Successfully   updated  LDAP  entries' errs = [ ] success dn set =  Ordered  Dict  ( ) for dn in dn set : o = old . get ( dn , { } ) n = new . get ( dn , { } ) try : if len ( o ) : if len ( n ) : op = 'modify' assert ( o != n )   salt   [ 'ldap3.change' ] ( l , dn , o , n ) else : op = 'delete'   salt   [ 'ldap3.delete' ] ( l , dn ) else : op = 'add' assert len ( n )   salt   [ 'ldap3.add' ] ( l , dn , n ) changed old [ dn ] = o changed new [ dn ] = n success dn set [ dn ] =  True  except ldap3 . LDAP Error  : log . exception ( 'failed  to  %s  entry  %s' , op , dn ) errs . append ( ( op , dn ) ) continue if len ( errs ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ( 'failed  to  ' + ',  ' . join ( ( ( ( op + '  entry  ' ) + dn ) for ( op , dn ) in errs ) ) ) for dn in success dn set : o = changed old . get ( dn , { } ) n = changed new . get ( dn , { } ) changes = { } ret [ 'changes' ] [ dn ] = changes for ( x , xn ) in ( ( o , 'old' ) , ( n , 'new' ) ) : if ( not len ( x ) ) : changes [ xn ] =  None  continue changes [ xn ] = dict ( ( ( attr , sorted ( vals ) ) for ( attr , vals ) in six . iteritems ( x ) if ( o . get ( attr , ( ) ) != n . get ( attr , ( ) ) ) ) ) return ret
def arbitrary ( module name , func name , args , kwargs = { } ) : if module name . startswith ( 'calibre plugins' ) : from calibre . customize . ui import find plugin find plugin module = importlib . import module ( module name ) func = getattr ( module , func name ) return func ( * args , ** kwargs )
def countVowels(word): return sum(1 for char in word if char.lower() in "aeiou")
def reverseString(input_str): return input_str[::-1]
def isPalindrome(word): return word.lower() == word.lower()[::-1]
def capitalizeFirstLetter(sentence): return ' '.join(word.capitalize() for word in sentence.split())
def listDuplicates(lst, item): return [index for index, value in enumerate(lst) if value == item]
def filterOddNumbers(numbers): return [num for num in numbers if num % 2 != 0]
def create api deployment ( rest Api  Id  , stage Name  , stage Description  = '' , description = '' , cache Cluster  Enabled  =  False  , cache Cluster  Size  = '0.5' , variables =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : try : variables = ( dict ( ) if ( variables is  None  ) else variables ) conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) deployment = conn . create deployment ( rest Api  Id  = rest Api  Id  , stage Name  = stage Name  , stage Description  = stage Description  , description = description , cache Cluster  Enabled  = cache Cluster  Enabled  , cache Cluster  Size  = cache Cluster  Size  , variables = variables ) return { 'created' :  True  , 'deployment' :  convert datetime str ( deployment ) } except  Client  Error  as e : return { 'created' :  False  , 'error' : salt . utils . boto3 . get error ( e ) }
def sdm spoly ( f , g , O , K , phantom =  None  ) : if ( ( not f ) or ( not g ) ) : return sdm zero ( ) LM1 = sdm LM ( f ) LM2 = sdm LM ( g ) if ( LM1 [ 0 ] != LM2 [ 0 ] ) : return sdm zero ( ) LM1 = LM1 [ 1 : ] LM2 = LM2 [ 1 : ] lcm = monomial lcm ( LM1 , LM2 ) m1 = monomial div ( lcm , LM1 ) m2 = monomial div ( lcm , LM2 ) c = K . quo ( ( - sdm LC ( f , K ) ) , sdm LC ( g , K ) ) r1 = sdm add ( sdm mul term ( f , ( m1 , K . one ) , O , K ) , sdm mul term ( g , ( m2 , c ) , O , K ) , O , K ) if ( phantom is  None  ) : return r1 r2 = sdm add ( sdm mul term ( phantom [ 0 ] , ( m1 , K . one ) , O , K ) , sdm mul term ( phantom [ 1 ] , ( m2 , c ) , O , K ) , O , K ) return ( r1 , r2 )
def attach network interface ( device index , name =  None  , network interface id =  None  , instance name =  None  , instance id =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : if ( not salt . utils . exactly one ( ( name , network interface id ) ) ) : raise  Salt  Invocation  Error  ( " Exactly   one  (but  not  both)  of  'name'  or  'network interface id'  must  be  provided." ) if ( not salt . utils . exactly one ( ( instance name , instance id ) ) ) : raise  Salt  Invocation  Error  ( " Exactly   one  (but  not  both)  of  'instance name'  or  'instance id'  must  be  provided." ) conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) r = { } result =  get network interface ( conn , name , network interface id ) if ( 'error' in result ) : return result eni = result [ 'result' ] try : info =  describe network interface ( eni ) network interface id = info [ 'id' ] except  Key  Error  : r [ 'error' ] = { 'message' : 'ID  not  found  for  this  network  interface.' } return r if instance name : try : instance id = get id ( name = instance name , region = region , key = key , keyid = keyid , profile = profile ) except boto . exception .  Boto  Server  Error  as e : log . error ( e ) return  False  try : r [ 'result' ] = conn . attach network interface ( network interface id , instance id , device index ) except boto . exception . EC2 Response  Error  as e : r [ 'error' ] =   utils   [ 'boto.get error' ] ( e ) return r
def  point along a line ( x0 , y0 , x1 , y1 , d ) : ( dx , dy ) = ( ( x0 - x1 ) , ( y0 - y1 ) ) ff = ( d / ( ( ( dx * dx ) + ( dy * dy ) ) ** 0.5 ) ) ( x2 , y2 ) = ( ( x0 - ( ff * dx ) ) , ( y0 - ( ff * dy ) ) ) return ( x2 , y2 )
def upgrade ( migrate engine ) : meta =  Meta  Data  ( ) meta . bind = migrate engine volume type projects =  Table  ( 'volume type projects' , meta , autoload =  True  ) if ( migrate engine . name == 'postgresql' ) : sql = ( 'ALTER  TABLE  volume type projects  ALTER  COLUMN  deleted  ' + 'TYPE  INTEGER  USING  deleted::integer' ) migrate engine . execute ( sql ) else : volume type projects . c . deleted . alter (  Integer  )
def cert from key info ( key info , ignore age =  False  ) : res = [ ] for x509 data in key info . x509 data : x509 certificate = x509 data . x509 certificate cert = x509 certificate . text . strip ( ) cert = '\n' . join ( split len ( '' . join ( [ s . strip ( ) for s in cert . split ( ) ] ) , 64 ) ) if ( ignore age or active cert ( cert ) ) : res . append ( cert ) else : logger . info ( ' Inactive   cert' ) return res
def instance group update ( context , group uuid , values ) : return IMPL . instance group update ( context , group uuid , values )
def  tree to bitstrs ( tree ) : clades bitstrs = { } term names = [ term . name for term in tree . find clades ( terminal =  True  ) ] for clade in tree . find clades ( terminal =  False  ) : bitstr =  clade to bitstr ( clade , term names ) clades bitstrs [ clade ] = bitstr return clades bitstrs
def timeuntil ( value , arg =  None  ) : from django . utils . timesince import timesince from datetime import datetime if ( not value ) : return '' if arg : return timesince ( arg , value ) return timesince ( datetime . now ( ) , value )
def main ( ) : module =  Ansible  Module  ( argument spec = { 'table' : { 'required' :  True  } , 'record' : { 'required' :  True  } , 'col' : { 'required' :  True  } , 'key' : { 'required' :  True  } , 'value' : { 'required' :  True  } , 'timeout' : { 'default' : 5 , 'type' : 'int' } } , supports check mode =  True  ) params set ( module )
def test hashbang ( ) : entry = tokenize ( '#!this  is  a  comment\n' ) assert ( entry == [ ] )
def analyze modules ( project , task handle = taskhandle .  Null  Task  Handle  ( ) ) : resources = project . get python files ( ) job set = task handle . create jobset ( ' Analyzing    Modules ' , len ( resources ) ) for resource in resources : job set . started job ( resource . path ) analyze module ( project , resource ) job set . finished job ( )
def   virtual   ( ) : if   opts   [ 'master tops' ] . get ( 'ext nodes' ) : return  True  return  False
def disassociate api key stagekeys ( api Key  , stagekeyslist , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : try : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) pvlist = [ ( '/stages' , stagekey ) for stagekey in stagekeyslist ] response =  api key patch remove ( conn , api Key  , pvlist ) return { 'disassociated' :  True  } except  Client  Error  as e : return { 'disassociated' :  False  , 'error' : salt . utils . boto3 . get error ( e ) }
@ then ( u'we  see  database  dropped' ) def step see db dropped ( context ) :  expect exact ( context , u'DROP  DATABASE' , timeout = 2 )
@ bdd . when ( bdd . parsers . parse ( 'I  wait  for  the  javascript  message  "{message}"' ) ) def javascript message when ( quteproc , message ) : quteproc . wait for js ( message )
def  git Present  ( ) : try : gitvers = subprocess . check output ( 'git  --version' . split ( ) , stderr = subprocess . PIPE ) except (  Called  Process  Error  , OS Error  ) : gitvers = '' return bool ( gitvers . startswith ( 'git  version' ) )
def  Is  Auto  Generated  ( xml str ) : try : xml root =  Element  Tree  . fromstring ( xml str ) return ( ( xml root . tag == 'datastore-indexes' ) and   Boolean  Attribute  ( xml root . attrib . get ( 'auto Generate ' , 'false' ) ) ) except  Element  Tree  .  Parse  Error  : return  False
def sum_of_list(numbers): return sum(numbers)
def product_of_list(numbers): return 1 if not numbers else eval('*'.join(map(str, numbers)))
def count_vowels(word): return sum(1 for char in word if char.lower() in "aeiou")
def jnp zeros ( n , nt ) : return jnyn zeros ( n , nt ) [ 1 ]
def get Tooth  Profile  Rack  ( derivation ) : addendum Side  = ( derivation . quarter Wavelength  - ( derivation . addendum * derivation . tan Pressure  ) ) addendum Complex  = complex ( addendum Side  , derivation . addendum ) dedendum Side  = ( derivation . quarter Wavelength  + ( derivation . dedendum * derivation . tan Pressure  ) ) dedendum Complex  = complex ( dedendum Side  , ( - derivation . dedendum ) ) tooth Profile  = [ dedendum Complex  ] if ( derivation . root Bevel  > 0.0 ) : mirror Point  = complex ( ( derivation . wavelength - dedendum Side  ) , ( - derivation . dedendum ) ) tooth Profile  = get Bevel  Path  ( addendum Complex  , derivation . root Bevel  , dedendum Complex  , mirror Point  ) if ( derivation . tip Bevel  > 0.0 ) : mirror Point  = complex ( ( - addendum Complex  . real ) , addendum Complex  . imag ) bevel Path  = get Bevel  Path  ( dedendum Complex  , derivation . tip Bevel  , addendum Complex  , mirror Point  ) bevel Path  . reverse ( ) tooth Profile  += bevel Path  else : tooth Profile  . append ( addendum Complex  ) return euclidean . get Mirror  Path  ( get Thickness  Multiplied  Path  ( tooth Profile  , derivation . tooth Thickness  Multiplier  ) )
def delete policy ( vhost , name , runas =  None  ) : if ( ( runas is  None  ) and ( not salt . utils . is windows ( ) ) ) : runas = salt . utils . get user ( ) res =   salt   [ 'cmd.run all' ] ( [   context   [ 'rabbitmqctl' ] , 'clear policy' , '-p' , vhost , name ] , runas = runas , python shell =  False  ) log . debug ( ' Delete   policy:  {0}' . format ( res [ 'stdout' ] ) ) return  format response ( res , ' Deleted ' )
def set time ( time ) : time format =  get date time format ( time ) dt obj = datetime . strptime ( time , time format ) cmd = 'systemsetup  -settime  {0}' . format ( dt obj . strftime ( '%H:%M:%S' ) ) return salt . utils . mac utils . execute return success ( cmd )
def get Rectangular  Grid  ( diameter , loops Complex  , maximum Complex  , minimum Complex  , zigzag ) : demiradius = ( 0.25 * diameter ) x Start  = ( minimum Complex  . real - demiradius . real ) y = ( minimum Complex  . imag - demiradius . imag ) grid Path  = [ ] row Index  = 0 while ( y < maximum Complex  . imag ) : add Grid  Row  ( diameter , grid Path  , loops Complex  , maximum Complex  , row Index  , x Start  , y , zigzag ) y += diameter . imag row Index  += 1 return grid Path
def get discount modules ( ) : return load module instances ( 'SHUUP DISCOUNT MODULES' , 'discount module' )
def test sample wrong X ( ) : sm = SMOTEENN ( random state = RND SEED ) sm . fit ( X , Y ) assert raises (  Runtime  Error  , sm . sample , np . random . random ( ( 100 , 40 ) ) , np . array ( ( ( [ 0 ] * 50 ) + ( [ 1 ] * 50 ) ) ) )
def problem rheader ( r , tabs = [ ] ) : if ( r . representation == 'html' ) : if ( r . record is  None  ) : return  None  problem = r . record tabs = [ ( T ( ' Problems ' ) , 'problems' ) , ( T ( ' Solutions ' ) , 'solution' ) , ( T ( ' Discuss ' ) , 'discuss' ) , ( T ( ' Vote ' ) , 'vote' ) , ( T ( ' Scale   of   Results ' ) , 'results' ) ] duser = s3db . delphi  Delphi  User  ( problem . group id ) if duser . authorised : tabs . append ( ( T ( ' Edit ' ) ,  None  ) ) rheader tabs = s3 rheader tabs ( r , tabs ) rtable = TABLE ( TR ( TH ( ( '%s:  ' % T ( ' Problem ' ) ) ) , problem . name , TH ( ( '%s:  ' % T ( ' Active ' ) ) ) , problem . active ) , TR ( TH ( ( '%s:  ' % T ( ' Description ' ) ) ) , problem . description ) , TR ( TH ( ( '%s:  ' % T ( ' Criteria ' ) ) ) , problem . criteria ) ) if ( r . component and ( r . component name == 'solution' ) and r . component id ) : stable = s3db . delphi solution query = ( stable . id == r . component id ) solution = db ( query ) . select ( stable . name , stable . description , limitby = ( 0 , 1 ) ) . first ( ) rtable . append ( DIV ( TR ( TH ( ( '%s:  ' % T ( ' Solution ' ) ) ) , solution . name ) , TR ( TH ( ( '%s:  ' % T ( ' Description ' ) ) ) , solution . description ) ) ) rheader = DIV ( rtable , rheader tabs ) return rheader
def read ( handle , format ) : format = format . lower ( ) motifs = parse ( handle , format ) if ( len ( motifs ) == 0 ) : raise  Value  Error  ( ' No   motifs  found  in  handle' ) if ( len ( motifs ) > 1 ) : raise  Value  Error  ( ' More   than  one  motif  found  in  handle' ) motif = motifs [ 0 ] return motif
def is sequence of strings ( obj ) : if ( not iterable ( obj ) ) : return  False  if is string like ( obj ) : return  False  for o in obj : if ( not is string like ( o ) ) : return  False  return  True
def conv 3d ( incoming , nb filter , filter size , strides = 1 , padding = 'same' , activation = 'linear' , bias =  True  , weights init = 'uniform scaling' , bias init = 'zeros' , regularizer =  None  , weight decay = 0.001 , trainable =  True  , restore =  True  , reuse =  False  , scope =  None  , name = ' Conv 3D' ) : input shape = utils . get incoming shape ( incoming ) assert ( len ( input shape ) == 5 ) , ' Incoming    Tensor   shape  must  be  5-D' filter size = utils . autoformat filter conv3d ( filter size , input shape [ ( - 1 ) ] , nb filter ) strides = utils . autoformat stride 3d ( strides ) padding = utils . autoformat padding ( padding ) try : vscope = tf . variable scope ( scope , default name = name , values = [ incoming ] , reuse = reuse ) except  Exception  : vscope = tf . variable op scope ( [ incoming ] , scope , name , reuse = reuse ) with vscope as scope : name = scope . name W init = weights init if isinstance ( weights init , str ) : W init = initializations . get ( weights init ) ( ) W regul =  None  if regularizer : W regul = ( lambda x : losses . get ( regularizer ) ( x , weight decay ) ) W = vs . variable ( 'W' , shape = filter size , regularizer = W regul , initializer = W init , trainable = trainable , restore = restore ) tf . add to collection ( ( ( tf .  Graph  Keys  . LAYER VARIABLES + '/' ) + name ) , W ) b =  None  if bias : if isinstance ( bias init , str ) : bias init = initializations . get ( bias init ) ( ) b = vs . variable ( 'b' , shape = nb filter , initializer = bias init , trainable = trainable , restore = restore ) tf . add to collection ( ( ( tf .  Graph  Keys  . LAYER VARIABLES + '/' ) + name ) , b ) inference = tf . nn . conv3d ( incoming , W , strides , padding ) if b : inference = tf . nn . bias add ( inference , b ) if isinstance ( activation , str ) : inference = activations . get ( activation ) ( inference ) elif hasattr ( activation , '  call  ' ) : inference = activation ( inference ) else : raise  Value  Error  ( ' Invalid    Activation .' ) tf . add to collection ( tf .  Graph  Keys  . ACTIVATIONS , inference ) inference . scope = scope inference . W = W inference . b = b tf . add to collection ( ( ( tf .  Graph  Keys  . LAYER TENSOR + '/' ) + name ) , inference ) return inference
def capture screenshot for step ( step , when ) : if world . auto capture screenshots : scenario num = ( step . scenario . feature . scenarios . index ( step . scenario ) + 1 ) step num = ( step . scenario . steps . index ( step ) + 1 ) step func name = step . defined at . function . func name image name = '{prefix:03d}  {num:03d}  {name}  {postfix}' . format ( prefix = scenario num , num = step num , name = step func name , postfix = when ) world . capture screenshot ( image name )
def get sw login version ( ) : return '-' . join ( get sw version ( strip build num =  True  ) . split ( '-' ) [ 1 : ( - 2 ) ] )
def  in Filesystem  Namespace  ( path ) : return ( path [ : 1 ] != '\x00' )
def test sort ( ) : model =  create model ( [ [ ( 'B' , '' , '' , 1 ) , ( 'C' , '' , '' , 2 ) , ( 'A' , '' , '' , 0 ) ] ] ) filter model = sortfilter .  Completion  Filter  Model  ( model ) filter model . sort ( 0 ,  Qt  .  Ascending  Order  ) actual =  extract model data ( filter model ) assert ( actual == [ [ ( 'A' , '' , '' ) , ( 'B' , '' , '' ) , ( 'C' , '' , '' ) ] ] ) filter model . sort ( 0 ,  Qt  .  Descending  Order  ) actual =  extract model data ( filter model ) assert ( actual == [ [ ( 'C' , '' , '' ) , ( 'B' , '' , '' ) , ( 'A' , '' , '' ) ] ] )
def create dendrogram ( X , orientation = 'bottom' , labels =  None  , colorscale =  None  , distfun =  None  , linkagefun = ( lambda x : sch . linkage ( x , 'complete' ) ) ) : if ( ( not scp ) or ( not scs ) or ( not sch ) ) : raise  Import  Error  ( ' Figure  Factory .create dendrogram  requires  scipy,                                                          scipy.spatial  and  scipy.hierarchy' ) s = X . shape if ( len ( s ) != 2 ) : exceptions .  Plotly  Error  ( 'X  should  be  2-dimensional  array.' ) if ( distfun is  None  ) : distfun = scs . distance . pdist dendrogram =   Dendrogram  ( X , orientation , labels , colorscale , distfun = distfun , linkagefun = linkagefun ) return { 'layout' : dendrogram . layout , 'data' : dendrogram . data }
def  Set  Help  Menu  Other  Help  ( main Menu  ) : global helpID Map  if ( helpID Map  is  None  ) : helpID Map  = { } cmdID = win32ui . ID HELP OTHER exclude List  = [ ' Main    Python    Documentation ' , ' Pythonwin    Reference ' ] first List  =  List  All  Help  Files  ( ) exclude Fnames  = [ ] for ( desc , fname ) in first List  : if ( desc in exclude List  ) : exclude Fnames  . append ( fname ) help Descs  = [ ] for ( desc , fname ) in first List  : if ( fname not in exclude Fnames  ) : helpID Map  [ cmdID ] = ( desc , fname ) win32ui .  Get  Main  Frame  ( ) .  Hook  Command  (  Handle  Help  Other  Command  , cmdID ) cmdID = ( cmdID + 1 ) help Menu  = main Menu  .  Get  Sub  Menu  ( ( main Menu  .  Get  Menu  Item  Count  ( ) - 1 ) ) other Help  Menu  Pos  = 2 other Menu  = help Menu  .  Get  Sub  Menu  ( other Help  Menu  Pos  ) while other Menu  .  Get  Menu  Item  Count  ( ) : other Menu  .  Delete  Menu  ( 0 , win32con . MF BYPOSITION ) if helpID Map  : for ( id , ( desc , fname ) ) in helpID Map  . iteritems ( ) : other Menu  .  Append  Menu  ( ( win32con . MF ENABLED | win32con . MF STRING ) , id , desc ) else : help Menu  .  Enable  Menu  Item  ( other Help  Menu  Pos  , ( win32con . MF BYPOSITION | win32con . MF GRAYED ) )
def pretty name ( name ) : name = ( name [ 0 ] . upper ( ) + name [ 1 : ] ) return name . replace ( ' ' , '  ' )
def list snapshots ( domain =  None  ) : ret = dict ( ) for vm domain in  get domain ( iterable =  True  , * ( ( domain and [ domain ] ) or list ( ) ) ) : ret [ vm domain . name ( ) ] = ( [  parse snapshot description ( snap ) for snap in vm domain . list All  Snapshots  ( ) ] or 'N/A' ) return ret
def branch list ( remote =  False  ) : if remote : return for each ref basename ( u'refs/remotes' ) else : return for each ref basename ( u'refs/heads' )
def to ( location , code = falcon . HTTP 302 ) : raise falcon . http status . HTTP Status  ( code , { 'location' : location } )
def main ( args = sys . argv [ 1 : ] , env =  Environment  ( ) , custom log error =  None  ) : args = decode args ( args , env . stdin encoding ) plugin manager . load installed plugins ( ) def log error ( msg , * args , ** kwargs ) : msg = ( msg % args ) level = kwargs . get ( 'level' , 'error' ) assert ( level in [ 'error' , 'warning' ] ) env . stderr . write ( ( '\nhttp:  %s:  %s\n' % ( level , msg ) ) ) from httpie . cli import parser if env . config . default options : args = ( env . config . default options + args ) if custom log error : log error = custom log error include debug info = ( '--debug' in args ) include traceback = ( include debug info or ( '--traceback' in args ) ) if include debug info : print debug info ( env ) if ( args == [ '--debug' ] ) : return  Exit  Status  . OK exit status =  Exit  Status  . OK try : parsed args = parser . parse args ( args = args , env = env ) except  Keyboard  Interrupt  : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR CTRL C except  System  Exit  as e : if ( e . code !=  Exit  Status  . OK ) : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR else : try : exit status = program ( args = parsed args , env = env , log error = log error ) except  Keyboard  Interrupt  : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR CTRL C except  System  Exit  as e : if ( e . code !=  Exit  Status  . OK ) : env . stderr . write ( '\n' ) if include traceback : raise exit status =  Exit  Status  . ERROR except requests .  Timeout  : exit status =  Exit  Status  . ERROR TIMEOUT log error ( ' Request   timed  out  (%ss).' , parsed args . timeout ) except requests .  Too  Many  Redirects  : exit status =  Exit  Status  . ERROR TOO MANY REDIRECTS log error ( ' Too   many  redirects  (--max-redirects=%s).' , parsed args . max redirects ) except  Exception  as e : msg = str ( e ) if hasattr ( e , 'request' ) : request = e . request if hasattr ( request , 'url' ) : msg += ( '  while  doing  %s  request  to  URL:  %s' % ( request . method , request . url ) ) log error ( '%s:  %s' , type ( e ) .   name   , msg ) if include traceback : raise exit status =  Exit  Status  . ERROR return exit status
def get environ proxies ( url ) : if should bypass proxies ( url ) : return { } else : return getproxies ( )
def select command ( corrected commands ) : try : selector =  Command  Selector  ( corrected commands ) except  No  Rule  Matched  : logs . failed ( ' No   fucks  given' ) return if ( not settings . require confirmation ) : logs . show corrected command ( selector . value ) return selector . value logs . confirm text ( selector . value ) for action in read actions ( ) : if ( action == const . ACTION SELECT ) : sys . stderr . write ( '\n' ) return selector . value elif ( action == const . ACTION ABORT ) : logs . failed ( '\n Aborted ' ) return elif ( action == const . ACTION PREVIOUS ) : selector . previous ( ) logs . confirm text ( selector . value ) elif ( action == const . ACTION NEXT ) : selector . next ( ) logs . confirm text ( selector . value )
def submit rescore entrance exam for student ( request , usage key , student =  None  , only if higher =  False  ) : check entrance exam problems for rescoring ( usage key ) task type = ( 'rescore problem if higher' if only if higher else 'rescore problem' ) task class = rescore problem ( task input , task key ) = encode entrance exam and student input ( usage key , student ) task input . update ( { 'only if higher' : only if higher } ) return submit task ( request , task type , task class , usage key . course key , task input , task key )
def floating ip list ( call =  None  ) : if ( call != 'function' ) : raise  Salt  Cloud  System  Exit  ( ' The   floating ip list  action  must  be  called  with  -f  or  --function' ) conn = get conn ( ) return conn . floating ip list ( )
def partial project ( endog , exog ) : ( x1 , x2 ) = ( endog , exog ) params = np . linalg . pinv ( x2 ) . dot ( x1 ) predicted = x2 . dot ( params ) residual = ( x1 - predicted ) res =  Bunch  ( params = params , fittedvalues = predicted , resid = residual ) return res
def tsql query ( query , ** kwargs ) : try : cur =  get connection ( ** kwargs ) . cursor ( ) cur . execute ( query ) return loads (   Mssql  Encoder  ( ) . encode ( { 'resultset' : cur . fetchall ( ) } ) ) [ 'resultset' ] except  Exception  as e : return ( ( ' Could   not  run  the  query' , ) , ( str ( e ) , ) )
def test On  Sequence  Data  ( module , dataset ) : target = dataset . get Field  ( 'target' ) output =  Module  Validator  . calculate Module  Output  ( module , dataset ) ends =  Sequence  Helper  . get Sequence  Ends  ( dataset ) summed output = zeros ( dataset . outdim ) class output = [ ] class target = [ ] for j in range ( len ( output ) ) : summed output += output [ j ] if ( j in ends ) : class output . append ( argmax ( summed output ) ) class target . append ( argmax ( target [ j ] ) ) summed output = zeros ( dataset . outdim ) class output = array ( class output ) class target = array ( class target ) return  Validator  . classification Performance  ( class output , class target )
def make istoragepool tests ( fixture , snapshot factory ) : class I Storage  Pool  Tests  (  Async  Test  Case  , ) : ' Tests   for  a  :class:`I Storage  Pool `  implementation  and  its\n                corresponding  :class:`I Filesystem `  implementation.\n\n                 These   are  functional  tests  if  run  against  real  filesystems.\n                ' def test interface ( self ) : '\n                         The   tested  object  provides  :class:`I Storage  Pool `.\n                        ' pool = fixture ( self ) self . assert True  ( verify Object  ( I Storage  Pool  , pool ) ) def test service ( self ) : '\n                         The   tested  object  provides  :class:`I Service `.\n                        ' pool = fixture ( self ) self . assert True  ( verify Object  ( I Service  , pool ) ) def test running ( self ) : '\n                         The   tested  object  is  ``running``  after  its  ``start Service ``  method\n                        is  called.\n                        ' pool = fixture ( self ) pool . start Service  ( ) self . assert True  ( pool . running ) def test create filesystem ( self ) : '\n                        ``create()``  returns  a  :class:`I Filesystem `  provider.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : self . assert True  ( verify Object  ( I Filesystem  , filesystem ) ) d . add Callback  ( created filesystem ) return d def test create with maximum size ( self ) : '\n                         If   a  maximum  size  is  specified  by  the  volume,  the  resulting\n                        ``I Filesystem ``  provider  has  the  same  size  information.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) d . add Callback  ( created filesystem ) return d def test resize volume new max size ( self ) : '\n                         If   an  existing  volume  is  resized  to  a  new  maximum  size,  the\n                        resulting  ``I Filesystem ``  provider  has  the  same  new  size\n                        information.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) resized =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 10 ) ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) volume with size . size = resized return pool . set maximum size ( volume with size ) def resized filesystem ( filesystem ) : self . assert Equal  ( resized , filesystem . size ) d . add Callback  ( created filesystem ) d . add Callback  ( resized filesystem ) return d def test resize volume unlimited max size ( self ) : '\n                         If   an  existing  volume  is  resized  to  a  new  maximum  size  of   None ,  the\n                        resulting  ``I Filesystem ``  provider  has  the  same  new  size\n                        information.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) resized =  Volume  Size  ( maximum size =  None  ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) volume with size . size = resized return pool . set maximum size ( volume with size ) def resized filesystem ( filesystem ) : self . assert Equal  ( resized , filesystem . size ) d . add Callback  ( created filesystem ) d . add Callback  ( resized filesystem ) return d def test resize volume already unlimited size ( self ) : '\n                         If   an  attempt  is  made  to  remove  the  limit  on  maximum  size  of  an\n                        existing  volume  which  already  has  no  maximum  size  limit,  no  change\n                        is  made.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : return pool . set maximum size ( volume ) d . add Callback  ( created filesystem ) def didnt resize ( filesystem ) : self . assert Equal  (  Volume  Size  ( maximum size =  None  ) , filesystem . size ) d . add Callback  ( didnt resize ) return d def test resize volume invalid max size ( self ) : '\n                         If   an  existing  volume  is  resized  to  a  new  maximum  size  which  is\n                        less  than  the  used  size  of  the  existing  filesystem,  a\n                        `` Maximum  Size  Too  Small ``  error  is  raised.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) size =  Volume  Size  ( maximum size = ( ( 1024 * 1024 ) * 1024 ) ) resized =  Volume  Size  ( maximum size = 1 ) volume with size =  Volume  ( node id = volume . node id , name = volume . name , service = volume . service , size = size ) d = pool . create ( volume with size ) def created filesystem ( filesystem ) : self . assert Equal  ( size , filesystem . size ) volume with size . size = resized return pool . set maximum size ( volume with size ) d . add Callback  ( created filesystem ) self . assert Failure  ( d ,  Maximum  Size  Too  Small  ) return d def test two names create different filesystems ( self ) : '\n                         Two   calls  to  ``create()``  with  different  volume  names  return\n                        different  filesystems.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created filesystems ( filesystems ) : ( first , second ) = filesystems assert not equal comparison ( self , first , second ) d . add Callback  ( created filesystems ) return d def test two node id create different filesystems ( self ) : '\n                         Two   calls  to  ``create()``  with  different  volume  manager  node  I Ds \n                        return  different  filesystems.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created filesystems ( filesystems ) : ( first , second ) = filesystems assert not equal comparison ( self , first , second ) d . add Callback  ( created filesystems ) return d def test get filesystem ( self ) : '\n                        ``get()``  returns  the  same  :class:`I Filesystem `  provider  as  the\n                        earlier  created  one.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : filesystem2 = pool . get ( volume ) assert equal comparison ( self , filesystem , filesystem2 ) d . add Callback  ( created filesystem ) return d def test mountpoint ( self ) : "\n                         The   volume's  filesystem  has  a  mountpoint  which  is  a  directory.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : self . assert True  ( filesystem . get path ( ) . isdir ( ) ) d . add Callback  ( created filesystem ) return d def test two volume mountpoints different ( self ) : '\n                         Each   volume  has  its  own  mountpoint.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created filesystems ( filesystems ) : ( first , second ) = filesystems self . assert Not  Equal  ( first . get path ( ) , second . get path ( ) ) d . add Callback  ( created filesystems ) return d def test reader cleanup ( self ) : '\n                         The   reader  does  not  leave  any  open  file  descriptors  behind.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with assert No F Ds  Leaked  ( self ) : with filesystem . reader ( ) : pass d . add Callback  ( created filesystem ) return d def test writer cleanup ( self ) : '\n                         The   writer  does  not  leave  any  open  file  descriptors  behind.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with filesystem . reader ( ) as reader : data = reader . read ( ) with assert No F Ds  Leaked  ( self ) : with filesystem . writer ( ) as writer : writer . write ( data ) d . add Callback  ( created filesystem ) return d def test write new filesystem ( self ) : "\n                         Writing   the  contents  of  one  pool's  filesystem  to  another  pool's\n                        filesystem  creates  that  filesystem  with  the  given  contents.\n                        " d = create and copy ( self , fixture ) def got volumes ( copy volumes ) : assert Volumes  Equal  ( self , copy volumes . from volume , copy volumes . to volume ) d . add Callback  ( got volumes ) return d def test write update to unchanged filesystem ( self ) : "\n                         Writing   an  update  of  the  contents  of  one  pool's  filesystem  to\n                        another  pool's  filesystem  that  was  previously  created  this  way  but\n                        is  unchanged  updates  its  contents.\n                        " d = create and copy ( self , fixture ) def got volumes ( copy volumes ) : path = copy volumes . from volume . get filesystem ( ) . get path ( ) path . child ( 'anotherfile' ) . set Content  ( 'hello' ) path . child ( 'file' ) . remove ( ) copying = copy ( copy volumes . from volume , copy volumes . to volume ) def copied ( ignored ) : assert Volumes  Equal  ( self , copy volumes . from volume , copy volumes . to volume ) copying . add Callback  ( copied ) return copying d . add Callback  ( got volumes ) return d def test multiple writes ( self ) : '\n                         Writing   the  same  contents  to  a  filesystem  twice  does  not  result  in\n                        an  error.\n                        ' d = create and copy ( self , fixture ) def got volumes ( copied ) : ( volume , volume2 ) = ( copied . from volume , copied . to volume ) copying = copy ( volume , volume2 ) def copied ( ignored ) : assert Volumes  Equal  ( self , volume , volume2 ) copying . add Callback  ( copied ) return copying d . add Callback  ( got volumes ) return d def test exception passes through read ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  reader,  it  is  not\n                        swallowed.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with filesystem . reader ( ) : raise  Runtime  Error  ( 'ONO' ) d . add Callback  ( created filesystem ) return self . assert Failure  ( d ,  Runtime  Error  ) def test exception passes through write ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  writer,  it  is  not\n                        swallowed.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with filesystem . writer ( ) : raise  Runtime  Error  ( 'ONO' ) d . add Callback  ( created filesystem ) return self . assert Failure  ( d ,  Runtime  Error  ) def test exception cleanup through read ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  reader,  no\n                        filedescriptors  are  leaked.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with assert No F Ds  Leaked  ( self ) : try : with filesystem . reader ( ) : raise  Runtime  Error  ( 'ONO' ) except  Runtime  Error  : pass d . add Callback  ( created filesystem ) return d def test exception cleanup through write ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  writer,  no\n                        filedescriptors  are  leaked.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) def created filesystem ( filesystem ) : with assert No F Ds  Leaked  ( self ) : try : with filesystem . writer ( ) : raise  Runtime  Error  ( 'ONO' ) except  Runtime  Error  : pass d . add Callback  ( created filesystem ) return d def test exception aborts write ( self ) : '\n                         If   an  exception  is  raised  in  the  context  of  the  writer,  no  changes\n                        are  made  to  the  filesystem.\n                        ' d = create and copy ( self , fixture ) def got volumes ( copied ) : ( volume , volume2 ) = ( copied . from volume , copied . to volume ) from filesystem = volume . get filesystem ( ) path = from filesystem . get path ( ) path . child ( 'anotherfile' ) . set Content  ( 'hello' ) to filesystem = volume2 . get filesystem ( ) getting snapshots = to filesystem . snapshots ( ) def got snapshots ( snapshots ) : try : with from filesystem . reader ( snapshots ) as reader : with to filesystem . writer ( ) as writer : data = reader . read ( ) writer . write ( data [ : ( - 1 ) ] ) raise  Zero  Division  Error  ( ) except  Zero  Division  Error  : pass to path = volume2 . get filesystem ( ) . get path ( ) self . assert False  ( to path . child ( 'anotherfile' ) . exists ( ) ) getting snapshots . add Callback  ( got snapshots ) return getting snapshots d . add Callback  ( got volumes ) return d def test garbage in write ( self ) : '\n                         If   garbage  is  written  to  the  writer,  no  changes  are  made  to  the\n                        filesystem.\n                        ' d = create and copy ( self , fixture ) def got volumes ( copied ) : ( volume , volume2 ) = ( copied . from volume , copied . to volume ) to filesystem = volume2 . get filesystem ( ) with to filesystem . writer ( ) as writer : writer . write ( 'NOT  A  REAL  THING' ) assert Volumes  Equal  ( self , volume , volume2 ) d . add Callback  ( got volumes ) return d def test enumerate no filesystems ( self ) : '\n                         Lacking   any  filesystems,  ``enumerate()``  returns  an  empty  result.\n                        ' pool = fixture ( self ) enumerating = pool . enumerate ( ) enumerating . add Callback  ( self . assert Equal  , set ( ) ) return enumerating def test enumerate some filesystems ( self ) : '\n                         The   ``I Storage  Pool .enumerate``  implementation  returns  a\n                        `` Deferred ``  that  fires  with  a  ``set``  of  ``I Filesystem ``\n                        providers,  one  for  each  filesystem  which  has  been  created  in  that\n                        pool.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) volume2 = service . get ( MY VOLUME2 ) creating = gather Results  ( [ pool . create ( volume ) , pool . create ( volume2 ) ] ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : expected = { volume . get filesystem ( ) , volume2 . get filesystem ( ) } self . assert Equal  ( expected , result ) return enumerating . add Callback  ( enumerated ) def test enumerate provides null size ( self ) : '\n                         The   ``I Storage  Pool .enumerate``  implementation  produces\n                        ``I Filesystem ``  results  which  specify  a  `` None ``  ``maximum size``\n                        when  the  filesystem  was  created  with  no  maximum  size.\n                        ' size =  Volume  Size  ( maximum size =  None  ) pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME , size = size ) creating = pool . create ( volume ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : [ filesystem ] = result self . assert Equal  ( size , filesystem . size ) enumerating . add Callback  ( enumerated ) return enumerating def test enumerate provides size ( self ) : '\n                         The   ``I Storage  Pool .enumerate``  implementation  produces\n                        ``I Filesystem ``  results  which  reflect  the  size  configuration\n                        those  filesystems  were  created  with.\n                        ' size =  Volume  Size  ( maximum size = 54321 ) pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME , size = size ) creating = pool . create ( volume ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : [ filesystem ] = result self . assert Equal  ( size , filesystem . size ) enumerating . add Callback  ( enumerated ) return enumerating def test enumerate spaces ( self ) : "\n                         The   ``I Storage  Pool .enumerate``  implementation  doesn't  return\n                        a  `` Deferred ``  that  fires  with  a  `` Failure ``  if  there  is  a\n                        filesystem  with  a  space  in  it.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get (  Volume  Name  ( namespace = u'ns' , dataset id = u'spaced  name' ) ) creating = pool . create ( volume ) def created ( ignored ) : return pool . enumerate ( ) enumerating = creating . add Callback  ( created ) def enumerated ( result ) : expected = { volume . get filesystem ( ) } self . assert Equal  ( expected , result ) return enumerating . add Callback  ( enumerated ) def test consistent naming pattern ( self ) : '\n                        ``I Filesystem .get path().basename()``  has  a  consistent  naming\n                        pattern.\n\n                         This   test  should  be  removed  as  part  of:\n                                https://clusterhq.atlassian.net/browse/FLOC-78\n                        ' pool = fixture ( self ) volume name = MY VOLUME service = service for pool ( self , pool ) node id = service . node id volume = service . get ( volume name ) d = pool . create ( volume ) def created Filesystem  ( filesystem ) : name = filesystem . get path ( ) . basename ( ) expected = u'{node id}.{name}' . format ( node id = node id , name = volume name . to bytes ( ) ) self . assert Equal  ( name , expected ) d . add Callback  ( created Filesystem  ) return d def test change owner creates new ( self ) : '\n                        ``I Filesystem .change owner()``  creates  a  filesystem  for  the  new\n                        volume  definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'new-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : old path = filesystem . get path ( ) d = pool . change owner ( volume , new volume ) d . add Callback  ( ( lambda new fs : ( old path , new fs ) ) ) return d d . add Callback  ( created filesystem ) def changed owner ( ( old path , new filesystem ) ) : new path = new filesystem . get path ( ) self . assert Not  Equal  ( old path , new path ) d . add Callback  ( changed owner ) return d def test change owner removes old ( self ) : '\n                        ``I Storage  Pool .change owner()``  ensures  the  filesystem  for  the  old\n                        volume  definition  no  longer  exists.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'new-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : old path = filesystem . get path ( ) old path . child ( 'file' ) . set Content  ( 'content' ) d = pool . change owner ( volume , new volume ) d . add Callback  ( ( lambda ignored : old path ) ) return d d . add Callback  ( created filesystem ) def changed owner ( old path ) : self . assert False  ( old path . exists ( ) ) d . add Callback  ( changed owner ) return d def test change owner preserves data ( self ) : '\n                        ``I Storage  Pool .change owner()``  moves  the  data  from  the  filesystem\n                        for  the  old  volume  definition  to  that  for  the  new  volume\n                        definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : path = filesystem . get path ( ) path . child ( 'file' ) . set Content  ( 'content' ) return pool . change owner ( volume , new volume ) d . add Callback  ( created filesystem ) def changed owner ( filesystem ) : path = filesystem . get path ( ) self . assert Equal  ( path . child ( 'file' ) . get Content  ( ) , 'content' ) d . add Callback  ( changed owner ) return d def test change owner existing target ( self ) : '\n                        ``I Storage  Pool .change owner()``  returns  a  :class:` Deferred `  that\n                        fails  with  :exception:` Filesystem  Already  Exists `,  if  the  target\n                        filesystem  already  exists.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( new volume ) ] ) def created filesystems ( igonred ) : return pool . change owner ( volume , new volume ) d . add Callback  ( created filesystems ) return self . assert Failure  ( d ,  Filesystem  Already  Exists  ) def test no snapshots ( self ) : '\n                         If   there  are  no  snapshots  of  a  given  filesystem,\n                        `` Filesystem .snapshots``  returns  a  `` Deferred ``  that  fires  with  an\n                        empty  ``list``.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) creating = pool . create ( volume ) def created ( filesystem ) : loading = filesystem . snapshots ( ) loading . add Callback  ( self . assert Equal  , [ ] ) return loading creating . add Callback  ( created ) return creating def test clone to creates new ( self ) : '\n                        ``I Filesystem .clone to()``  creates  a  filesystem  for  the  new\n                        volume  definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'new-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) d . add Callback  ( ( lambda   : pool . clone to ( volume , new volume ) ) ) def cloned ( new filesystem ) : old path = volume . get filesystem ( ) . get path ( ) new path = new filesystem . get path ( ) self . assert Not  Equal  ( old path , new path ) d . add Callback  ( cloned ) return d def test clone to copies data ( self ) : '\n                        ``I Storage  Pool .clone to()``  copies  the  data  from  the  filesystem  for\n                        the  old  volume  definition  to  that  for  the  new  volume\n                        definition.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = pool . create ( volume ) def created filesystem ( filesystem ) : path = filesystem . get path ( ) path . child ( 'file' ) . set Content  ( 'content' ) return pool . clone to ( volume , new volume ) d . add Callback  ( created filesystem ) def cloned ( filesystem ) : path = filesystem . get path ( ) self . assert Equal  ( path . child ( 'file' ) . get Content  ( ) , 'content' ) d . add Callback  ( cloned ) return d def test clone to old distinct filesystems ( self ) : '\n                         The   filesystem  created  by  ``I Storage  Pool .clone to()``  and  the\n                        original  filesystem  are  independent;  writes  to  one  do  not  affect\n                        the  other.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume = service . get ( MY VOLUME2 ) d = pool . create ( volume ) def created filesystem ( filesystem ) : return pool . clone to ( volume , new volume ) d . add Callback  ( created filesystem ) def cloned (   ) : old path = volume . get filesystem ( ) . get path ( ) old path . child ( 'old' ) . set Content  ( 'old' ) new path = new volume . get filesystem ( ) . get path ( ) new path . child ( 'new' ) . set Content  ( 'new' ) self . assert Equal  ( [  False  ,  False  ] , [ old path . child ( 'new' ) . exists ( ) , new path . child ( 'old' ) . exists ( ) ] ) d . add Callback  ( cloned ) return d def test clone to existing target ( self ) : '\n                        ``I Storage  Pool .clone to()``  returns  a  :class:` Deferred `  that\n                        fails  with  :exception:` Filesystem  Already  Exists `,  if  the  target\n                        filesystem  already  exists.\n                        ' pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) new volume =  Volume  ( node id = u'other-uuid' , name = MY VOLUME2 , service = service ) d = gather Results  ( [ pool . create ( volume ) , pool . create ( new volume ) ] ) def created filesystems ( ignored ) : return pool . clone to ( volume , new volume ) d . add Callback  ( created filesystems ) return self . assert Failure  ( d ,  Filesystem  Already  Exists  ) def test destroy ( self ) : "\n                        A  filesystem  destroyed  by  ``I Storage  Pool .destroy``  doesn't  show  up\n                        in  ``I Storage  Pool .enumerate``.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) d . add Callback  ( ( lambda   : pool . destroy ( volume ) ) ) d . add Callback  ( ( lambda   : pool . enumerate ( ) ) ) d . add Callback  ( ( lambda result : self . assert Equal  ( list ( result ) , [ ] ) ) ) return d def test destroy after snapshot ( self ) : "\n                        A  filesystem  with  snapshots  that  is  destroyed  by\n                        ``I Storage  Pool .destroy``  doesn't  show  up  in\n                        ``I Storage  Pool .enumerate``.\n                        " pool = fixture ( self ) service = service for pool ( self , pool ) volume = service . get ( MY VOLUME ) d = pool . create ( volume ) d . add Callback  ( ( lambda fs : snapshot factory ( fs ) . create ( 'cheese' ) ) ) d . add Callback  ( ( lambda   : pool . destroy ( volume ) ) ) d . add Callback  ( ( lambda   : pool . enumerate ( ) ) ) d . add Callback  ( ( lambda result : self . assert Equal  ( list ( result ) , [ ] ) ) ) return d return I Storage  Pool  Tests
@ blueprint . route ( '/<job id>.json' , methods = [ 'GET' ] ) @ blueprint . route ( '/<job id>' , methods = [ 'GET' ] ) def show ( job id ) : job = scheduler . get job ( job id ) if ( job is  None  ) : raise werkzeug . exceptions .  Not  Found  ( ' Job   not  found' ) related jobs = scheduler . get related jobs ( job ) if request wants json ( ) : return flask . jsonify ( job . json dict (  True  ) ) elif isinstance ( job , model images .  Image  Classification  Model  Job  ) : return model images . classification . views . show ( job , related jobs = related jobs ) elif isinstance ( job , model images .  Generic  Image  Model  Job  ) : return model images . generic . views . show ( job , related jobs = related jobs ) else : raise werkzeug . exceptions .  Bad  Request  ( ' Invalid   job  type' )
def plot confusion matrix ( cm , classes , normalize =  False  , title = ' Confusion   matrix' , cmap = plt . cm .  Blues  ) : if normalize : cm = ( cm . astype ( 'float' ) / cm . sum ( axis = 1 ) [ : , np . newaxis ] ) print ' Normalized   confusion  matrix' else : print ' Confusion   matrix,  without  normalization' print cm plt . imshow ( cm , interpolation = 'nearest' , cmap = cmap ) plt . title ( title ) plt . colorbar ( ) tick marks = np . arange ( len ( classes ) ) plt . xticks ( tick marks , classes , rotation = 45 ) plt . yticks ( tick marks , classes ) fmt = ( '.2f' if normalize else 'd' ) thresh = ( cm . max ( ) / 2.0 ) for ( i , j ) in itertools . product ( range ( cm . shape [ 0 ] ) , range ( cm . shape [ 1 ] ) ) : plt . text ( j , i , format ( cm [ ( i , j ) ] , fmt ) , horizontalalignment = 'center' , color = ( 'white' if ( cm [ ( i , j ) ] > thresh ) else 'black' ) ) plt . tight layout ( ) plt . ylabel ( ' True   label' ) plt . xlabel ( ' Predicted   label' )
def test ( condition , true , false ) : if condition : return true else : return false
def connect to region ( region name , ** kw params ) : for region in regions ( ) : if ( region . name == region name ) : return region . connect ( ** kw params ) return  None
def  has required botocore ( ) : if ( not HAS BOTO ) : return  False  elif (  Loose  Version  ( botocore .   version   ) <  Loose  Version  ( required botocore version ) ) : return  False  else : return  True
def per cpu times ( ) : ret = [ ] for ( user , system , idle , interrupt , dpc ) in cext . per cpu times ( ) : item = scputimes ( user , system , idle , interrupt , dpc ) ret . append ( item ) return ret
def  run aws ( cmd , region , opts , user , ** kwargs ) : receipthandle = kwargs . pop ( 'receipthandle' ,  None  ) if receipthandle : kwargs [ 'receipt-handle' ] = receipthandle num = kwargs . pop ( 'num' ,  None  ) if num : kwargs [ 'max-number-of-messages' ] = num  formatted args = [ '--{0}  "{1}"' . format ( k , v ) for ( k , v ) in six . iteritems ( kwargs ) ] cmd = 'aws  sqs  {cmd}  {args}  {region}  {out}' . format ( cmd = cmd , args = '  ' . join (  formatted args ) , region =  region ( region ) , out =  OUTPUT ) rtn =   salt   [ 'cmd.run' ] ( cmd , runas = user , python shell =  False  ) return ( json . loads ( rtn ) if rtn else '' )
def  Trimmed  Mean  ( t , p = 0.01 ) : t =  Trim  ( t , p ) return  Mean  ( t )
def e Put  (  Handle  , IO Type  ,  Channel  ,  Value  , x1 ) : if ( os . name == 'nt' ) : static Lib  = ctypes . windll .  Load  Library  ( 'labjackud' ) pv = ctypes . c double (  Value  ) ec = static Lib  . e Put  (  Handle  , IO Type  ,  Channel  , pv , x1 ) if ( ec != 0 ) : raise  Lab  Jack  Exception  ( ec ) else : raise  Lab  Jack  Exception  ( 0 , ' Function   only  supported  for   Windows ' )
def  organize states for delete ( base mapper , states , uowtransaction ) : states to delete = [ ] for ( state , dict  , mapper , connection ) in  connections for states ( base mapper , uowtransaction , states ) : mapper . dispatch . before delete ( mapper , connection , state ) states to delete . append ( ( state , dict  , mapper , bool ( state . key ) , connection ) ) return states to delete
def style update ( request , url ) : affected layers = [ ] if ( request . method in ( 'POST' , 'PUT' ) ) : if ( 'HTTP USER AGENT' in request . META ) : if ( ( ' Trident /7.0' in request . META [ 'HTTP USER AGENT' ] ) and ( 'rv:11.0' in request . META [ 'HTTP USER AGENT' ] ) ) : txml = re . sub ( 'xmlns:NS[0-9]=""' , '' , request . body ) txml = re . sub ( 'NS[0-9]:' , '' , txml ) request .  body = txml tree = ET .  Element  Tree  ( ET . fromstring ( request . body ) ) elm namedlayer name = tree . findall ( './/{http://www.opengis.net/sld} Name ' ) [ 0 ] elm user style name = tree . findall ( './/{http://www.opengis.net/sld} Name ' ) [ 1 ] elm user style title = tree . find ( './/{http://www.opengis.net/sld} Title ' ) if ( not elm user style title ) : elm user style title = elm user style name layer name = elm namedlayer name . text style name = elm user style name . text sld body = ( '<?xml  version="1.0"  encoding="UTF-8"?>%s' % request . body ) if ( request . method == 'POST' ) : style =  Style  ( name = style name , sld body = sld body , sld url = url ) style . save ( ) layer =  Layer  . objects . get ( typename = layer name ) style . layer styles . add ( layer ) style . save ( ) affected layers . append ( layer ) elif ( request . method == 'PUT' ) : style =  Style  . objects . get ( name = style name ) style . sld body = sld body style . sld url = url if ( len ( elm user style title . text ) > 0 ) : style . sld title = elm user style title . text style . save ( ) for layer in style . layer styles . all ( ) : layer . save ( ) affected layers . append ( layer )  invalidate geowebcache layer ( layer name ) elif ( request . method == 'DELETE' ) : style name = os . path . basename ( request . path ) style =  Style  . objects . get ( name = style name ) style . delete ( ) return affected layers
def get load ( jid ) : serv =  get serv ( ret =  None  ) data = serv . get ( 'load:{0}' . format ( jid ) ) if data : return json . loads ( data ) return { }
def cr uid ids ( method ) : method .  api = 'cr uid ids' return method
def  default selem ( func ) : @ functools . wraps ( func ) def func out ( image , selem =  None  , * args , ** kwargs ) : if ( selem is  None  ) : selem = ndi . generate binary structure ( image . ndim , image . ndim ) return func ( image , selem = selem , * args , ** kwargs ) return func out
def  get date time format ( dt string ) : valid formats = [ '%I:%M:%S  %p' , '%I:%M  %p' , '%H:%M:%S' , '%H:%M' , '%Y-%m-%d' , '%m-%d-%y' , '%m-%d-%Y' , '%m/%d/%y' , '%m/%d/%Y' , '%Y/%m/%d' ] for dt format in valid formats : try : datetime . strptime ( dt string , dt format ) return dt format except  Value  Error  : continue return  False
def get Points  From  Segment  Table  ( segment Table  ) : points = [ ] segment Table  Keys  = segment Table  . keys ( ) segment Table  Keys  . sort ( ) for segment Table  Key  in segment Table  Keys  : for segment in segment Table  [ segment Table  Key  ] : for endpoint in segment : points . append ( endpoint . point ) return points
@ preserve value ( sys , 'dont write bytecode' ) def  load module no bytecode ( filename , module file , module file path , py source description ) : sys . dont write bytecode = 1 new module = imp . load module ( os . path . splitext ( filename ) [ 0 ] . replace ( '-' , ' ' ) , module file , module file path , py source description ) return new module
def rollback ( using =  None  ) : get connection ( using ) . rollback ( )
def install ( ) : p = K Queue  Reactor  ( ) from twisted . internet . main import install Reactor  install Reactor  ( p )
def make path result ( r , t ) : rpath = ( '/recipes/' + r ) tpath = ( '/tasks/' + t ) return ( ( rpath + tpath ) + '/results/' )
def  vstack ( arrays , join type = u'outer' , col name map =  None  ) :  col name map = col name map if ( join type not in ( u'inner' , u'exact' , u'outer' ) ) : raise  Value  Error  ( u"`join type`  arg  must  be  one  of  'inner',  'exact'  or  'outer'" ) if ( len ( arrays ) == 1 ) : return arrays [ 0 ] for arr in arrays : if arr . has mixin columns : raise  Not  Implemented  Error  ( u'vstack  not  available  for  tables  with  mixin  columns' ) names = set ( itertools . chain ( * [ arr . colnames for arr in arrays ] ) ) col name map = get col name map ( arrays , names ) if ( join type == u'exact' ) : for names in six . itervalues ( col name map ) : if any ( ( ( x is  None  ) for x in names ) ) : raise  Table  Merge  Error  ( u" Inconsistent   columns  in  input  arrays  (use  'inner'  or  'outer'  join type  to  allow  non-matching  columns)" ) join type = u'outer' if ( join type == u'inner' ) : col name map =  Ordered  Dict  ( ( ( name , in names ) for ( name , in names ) in six . iteritems ( col name map ) if all ( ( ( x is not  None  ) for x in in names ) ) ) ) if ( len ( col name map ) == 0 ) : raise  Table  Merge  Error  ( u' Input   arrays  have  no  columns  in  common' ) masked = any ( ( getattr ( arr , u'masked' ,  False  ) for arr in arrays ) ) for names in six . itervalues ( col name map ) : if any ( ( ( x is  None  ) for x in names ) ) : masked =  True  break lens = [ len ( arr ) for arr in arrays ] n rows = sum ( lens ) out =  get out class ( arrays ) ( masked = masked ) out descrs = get descrs ( arrays , col name map ) for out descr in out descrs : name = out descr [ 0 ] dtype = out descr [ 1 : ] if masked : out [ name ] = ma . array ( data = np . zeros ( n rows , dtype ) , mask = np . ones ( n rows , ma . make mask descr ( dtype ) ) ) else : out [ name ] = np . empty ( n rows , dtype = dtype ) for ( out name , in names ) in six . iteritems ( col name map ) : idx0 = 0 for ( name , array ) in zip ( in names , arrays ) : idx1 = ( idx0 + len ( array ) ) if ( name in array . colnames ) : out [ out name ] [ idx0 : idx1 ] = array [ name ] idx0 = idx1 if isinstance (  col name map , collections .  Mapping  ) :  col name map . update ( col name map ) return out
def last completed audit period ( unit =  None  ) : if ( not unit ) : unit = CONF . volume usage audit period offset = 0 if ( '@' in unit ) : ( unit , offset ) = unit . split ( '@' , 1 ) offset = int ( offset ) rightnow = timeutils . utcnow ( ) if ( unit not in ( 'month' , 'day' , 'year' , 'hour' ) ) : raise  Value  Error  ( ' Time   period  must  be  hour,  day,  month  or  year' ) if ( unit == 'month' ) : if ( offset == 0 ) : offset = 1 end = datetime . datetime ( day = offset , month = rightnow . month , year = rightnow . year ) if ( end >= rightnow ) : year = rightnow . year if ( 1 >= rightnow . month ) : year -= 1 month = ( 12 + ( rightnow . month - 1 ) ) else : month = ( rightnow . month - 1 ) end = datetime . datetime ( day = offset , month = month , year = year ) year = end . year if ( 1 >= end . month ) : year -= 1 month = ( 12 + ( end . month - 1 ) ) else : month = ( end . month - 1 ) begin = datetime . datetime ( day = offset , month = month , year = year ) elif ( unit == 'year' ) : if ( offset == 0 ) : offset = 1 end = datetime . datetime ( day = 1 , month = offset , year = rightnow . year ) if ( end >= rightnow ) : end = datetime . datetime ( day = 1 , month = offset , year = ( rightnow . year - 1 ) ) begin = datetime . datetime ( day = 1 , month = offset , year = ( rightnow . year - 2 ) ) else : begin = datetime . datetime ( day = 1 , month = offset , year = ( rightnow . year - 1 ) ) elif ( unit == 'day' ) : end = datetime . datetime ( hour = offset , day = rightnow . day , month = rightnow . month , year = rightnow . year ) if ( end >= rightnow ) : end = ( end - datetime . timedelta ( days = 1 ) ) begin = ( end - datetime . timedelta ( days = 1 ) ) elif ( unit == 'hour' ) : end = rightnow . replace ( minute = offset , second = 0 , microsecond = 0 ) if ( end >= rightnow ) : end = ( end - datetime . timedelta ( hours = 1 ) ) begin = ( end - datetime . timedelta ( hours = 1 ) ) return ( begin , end )
def reorder suite ( suite , classes , reverse =  False  ) : class count = len ( classes ) suite class = type ( suite ) bins = [  Ordered  Set  ( ) for i in range ( ( class count + 1 ) ) ] partition suite by type ( suite , classes , bins , reverse = reverse ) reordered suite = suite class ( ) for i in range ( ( class count + 1 ) ) : reordered suite . add Tests  ( bins [ i ] ) return reordered suite
def delete dhcp options ( dhcp options id =  None  , dhcp options name =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : return  delete resource ( resource = 'dhcp options' , name = dhcp options name , resource id = dhcp options id , region = region , key = key , keyid = keyid , profile = profile )
def pyramid laplacian ( image , max layer = ( - 1 ) , downscale = 2 , sigma =  None  , order = 1 , mode = 'reflect' , cval = 0 ) :  check factor ( downscale ) image = img as float ( image ) if ( sigma is  None  ) : sigma = ( ( 2 * downscale ) / 6.0 ) layer = 0 rows = image . shape [ 0 ] cols = image . shape [ 1 ] smoothed image =  smooth ( image , sigma , mode , cval ) ( yield ( image - smoothed image ) ) while ( layer != max layer ) : layer += 1 out rows = math . ceil ( ( rows / float ( downscale ) ) ) out cols = math . ceil ( ( cols / float ( downscale ) ) ) resized image = resize ( smoothed image , ( out rows , out cols ) , order = order , mode = mode , cval = cval ) smoothed image =  smooth ( resized image , sigma , mode , cval ) prev rows = rows prev cols = cols rows = resized image . shape [ 0 ] cols = resized image . shape [ 1 ] if ( ( prev rows == rows ) and ( prev cols == cols ) ) : break ( yield ( resized image - smoothed image ) )
def import module ( name , required =  True  ) : try :   import   ( name , globals ( ) , locals ( ) , [ ] ) except  Import  Error  : if ( ( not required ) and module not found ( ) ) : return  None  raise return sys . modules [ name ]
def mapping ( data source , geom name = 'geom' , layer key = 0 , multi geom =  False  ) : if isinstance ( data source , str ) : data source =  Data  Source  ( data source ) elif isinstance ( data source ,  Data  Source  ) : pass else : raise  Type  Error  ( ' Data   source  parameter  must  be  a  string  or  a   Data  Source   object.' )  mapping = { } for field in data source [ layer key ] . fields : mfield = field . lower ( ) if ( mfield [ ( - 1 ) : ] == ' ' ) : mfield += 'field'  mapping [ mfield ] = field gtype = data source [ layer key ] . geom type if multi geom : gtype . to multi ( )  mapping [ geom name ] = str ( gtype ) . upper ( ) return  mapping
def patfilter ( names , pat ) : if ( pat not in  pat cache ) :  pat cache [ pat ] = re . compile (  translate pattern ( pat ) ) match =  pat cache [ pat ] . match return filter ( match , names )
def first ( value ) : try : return value [ 0 ] except  Index  Error  : return u''
def setup platform ( hass , config , add devices callback , discovery info =  None  ) : import RF Xtrx  as rfxtrxmod switches = rfxtrx . get devices from config ( config ,  Rfxtrx  Switch  ) add devices callback ( switches ) def switch update ( event ) : ' Callback   for  sensor  updates  from  the  RF Xtrx   gateway.' if ( ( not isinstance ( event . device , rfxtrxmod .  Lighting  Device  ) ) or event . device . known to be dimmable or event . device . known to be rollershutter ) : return new device = rfxtrx . get new device ( event , config ,  Rfxtrx  Switch  ) if new device : add devices callback ( [ new device ] ) rfxtrx . apply received command ( event ) if ( switch update not in rfxtrx . RECEIVED EVT SUBSCRIBERS ) : rfxtrx . RECEIVED EVT SUBSCRIBERS . append ( switch update )
def  save and remove module ( name , orig modules ) : if ( name not in sys . modules ) :   import   ( name ) del sys . modules [ name ] for modname in list ( sys . modules ) : if ( ( modname == name ) or modname . startswith ( ( name + '.' ) ) ) : orig modules [ modname ] = sys . modules [ modname ] del sys . modules [ modname ]
def  wait until running ( instance ) : with start action ( action type = u'flocker:provision:aws:wait until running' , instance id = instance . id ) as context :  poll while ( ( lambda :  node is booting ( instance ) ) , repeat ( 1 , INSTANCE TIMEOUT ) ) context . add success fields ( instance state = instance . state ) context . add success fields ( instance state reason = instance . state reason ) if ( instance . state != u'running' ) : raise  Failed  To  Run  ( instance . state reason )
def dict to numpy array ( d , mapping =  None  ) : try : return dict to numpy array2 ( d , mapping ) except (  Attribute  Error  ,  Type  Error  ) : return dict to numpy array1 ( d , mapping )
def  is suggestion handled ( thread id , exploration id ) : thread = feedback models .  Feedback  Thread  Model  . get by exp and thread id ( exploration id , thread id ) return ( thread . status in [ feedback models . STATUS CHOICES FIXED , feedback models . STATUS CHOICES IGNORED ] )
def test records ( test data ) : ds =  Chart  Data  Source  . from data ( test data . records data ) assert ( len ( ds . columns ) == 2 ) assert ( len ( ds . index ) == 4 )
def ccovf ( x , y , unbiased =  True  , demean =  True  ) : n = len ( x ) if demean : xo = ( x - x . mean ( ) ) yo = ( y - y . mean ( ) ) else : xo = x yo = y if unbiased : xi = np . ones ( n ) d = np . correlate ( xi , xi , 'full' ) else : d = n return ( np . correlate ( xo , yo , 'full' ) / d ) [ ( n - 1 ) : ]
def serializers ( opts ) : return  Lazy  Loader  (  module dirs ( opts , 'serializers' ) , opts , tag = 'serializers' )
def get overlap for ( doc , doctype , fieldname , value =  None  ) : existing = frappe . db . sql ( u'select  name,  from time,  to time  from  `tab{0}`\n DCTB  DCTB where  `{1}`=%(val)s  and  schedule date  =  %(schedule date)s  and\n DCTB  DCTB (\n DCTB  DCTB  DCTB (from time  >  %(from time)s  and  from time  <  %(to time)s)  or\n DCTB  DCTB  DCTB (to time  >  %(from time)s  and  to time  <  %(to time)s)  or\n DCTB  DCTB  DCTB (%(from time)s  >  from time  and  %(from time)s  <  to time)  or\n DCTB  DCTB  DCTB (%(from time)s  =  from time  and  %(to time)s  =  to time))\n DCTB  DCTB and  name!=%(name)s' . format ( doctype , fieldname ) , { u'schedule date' : doc . schedule date , u'val' : ( value or doc . get ( fieldname ) ) , u'from time' : doc . from time , u'to time' : doc . to time , u'name' : ( doc . name or u' No    Name ' ) } , as dict =  True  ) return ( existing [ 0 ] if existing else  None  )
def  create Target  Dirs  ( ) : if ( not os . path . isdir ( paths . POCSUITE OUTPUT PATH ) ) : try : if ( not os . path . isdir ( paths . POCSUITE OUTPUT PATH ) ) : os . makedirs ( paths . POCSUITE OUTPUT PATH , 493 ) warn Msg  = ( "using  '%s'  as  the  output  directory" % paths . POCSUITE OUTPUT PATH ) logger . log ( CUSTOM LOGGING . WARNING , warn Msg  ) except ( OS Error  , IO Error  ) as ex : try : temp Dir  = tempfile . mkdtemp ( prefix = 'pocsuiteoutput' ) except  Exception  as   : err Msg  = ( "unable  to  write  to  the  temporary  directory  ('%s').  " %   ) err Msg  += ' Please   make  sure  that  your  disk  is  not  full  and  ' err Msg  += 'that  you  have  sufficient  write  permissions  to  ' err Msg  += 'create  temporary  files  and/or  directories' raise  Pocsuite  System  Exception  ( err Msg  ) warn Msg  = 'unable  to  create  regular  output  directory  ' warn Msg  += ( "'%s'  (%s).  " % ( paths . POCSUITE OUTPUT PATH , get Unicode  ( ex ) ) ) warn Msg  += ( " Using   temporary  directory  '%s'  instead" % get Unicode  ( temp Dir  ) ) logger . log ( CUSTOM LOGGING . WARNING , warn Msg  ) paths . POCUSITE OUTPUT PATH = temp Dir
def draw nx ( G , pos , ** kwds ) : draw ( G , pos , ** kwds )
def start clientbrowser ( config , args ) : logger . info ( ' Start   client  mode  (browser)' ) global client from glances . client browser import  Glances  Client  Browser  client =  Glances  Client  Browser  ( config = config , args = args ) client . serve forever ( ) client . end ( )
def random bucket name ( prefix = 'awscli-s3integ-' , num random = 10 ) : return ( prefix + random chars ( num random ) )
def normalize formset dict ( formset , attr list ) : assert isinstance ( formset ,  Base  Simple  Form  Set  ) res = [ ] for form in formset . forms : res . append ( normalize form dict ( form , attr list ) ) return res
def  open ( filepath , * args , ** kwargs ) : if ( not os . path . exists ( filepath ) ) : filepath = os . path . join ( '..' , filepath ) return open ( filepath , 'rb' , * args , ** kwargs )
@ task def setup ( ctx ) : copy settings ( ctx , addons =  True  ) packages ( ctx ) requirements ( ctx , addons =  True  , dev =  True  ) build js config files ( ctx ) assets ( ctx , dev =  True  , watch =  False  )
def open pathname ( pathname , verbose = 0 ) : try : refno =  Res  . FS Open  Resource  File  ( pathname , u'' , 1 ) except  Res  .  Error  as arg : if ( arg [ 0 ] != ( - 199 ) ) : raise else : return refno pathname =  decode ( pathname , verbose = verbose ) refno =  Res  . FS Open  Resource  File  ( pathname , u'' , 1 )
def get preamble ( ) : latex preamble = rc Params  . get ( u'pgf.preamble' , u'' ) if ( type ( latex preamble ) == list ) : latex preamble = u'\n' . join ( latex preamble ) return latex preamble
def cla Model  Control  Disable TP Learning  Cb  ( cla Model  ) : assert isinstance ( cla Model  , CLA Model  ) cla Model  .  getTP Region  ( ) . set Parameter  ( 'learning Mode ' ,  False  ) return
def s3 roles permitted ( name = 'roles permitted' , ** attr ) : T = current . T represent = S3 Represent  ( lookup = 'auth group' , fields = [ 'role' ] ) if ( 'label' not in attr ) : attr [ 'label' ] = T ( ' Roles    Permitted ' ) if ( 'sortby' not in attr ) : attr [ 'sortby' ] = 'role' if ( 'represent' not in attr ) : attr [ 'represent' ] = represent if ( 'requires' not in attr ) : attr [ 'requires' ] = IS EMPTY OR ( IS ONE OF ( current . db , 'auth group.id' , represent , multiple =  True  ) ) if ( 'comment' not in attr ) : attr [ 'comment' ] = DIV (  class = 'tooltip' ,  title = ( '%s|%s' % ( T ( ' Roles    Permitted ' ) , T ( ' If   this  record  should  be  restricted  then  select  which  role(s)  are  permitted  to  access  the  record  here.' ) ) ) ) if ( 'ondelete' not in attr ) : attr [ 'ondelete' ] = 'RESTRICT' f = S3 Reusable  Field  ( name , 'list:reference  auth group' , ** attr ) return f ( )
def create mount target ( filesystemid , subnetid , ipaddress =  None  , securitygroups =  None  , keyid =  None  , key =  None  , profile =  None  , region =  None  , ** kwargs ) : client =  get conn ( key = key , keyid = keyid , profile = profile , region = region ) return client . create mount point (  File  System  Id  = filesystemid ,  Subnet  Id  = subnetid ,  Ip  Address  = ipaddress ,  Security  Groups  = securitygroups )
def del job files ( job paths ) : for path in job paths : if ( path and clip path ( path ) . lower ( ) . startswith ( cfg . download dir . get path ( ) . lower ( ) ) ) : remove all ( path , recursive =  True  )
def showwarning ( message , category , filename , lineno , file =  None  ) : if ( file is  None  ) : file = sys . stderr try : file . write ( formatwarning ( message , category , filename , lineno ) ) except IO Error  : pass
def run vcs tool ( path , action ) : info = get vcs info ( get vcs root ( path ) ) tools = info [ 'actions' ] [ action ] for ( tool , args ) in tools : if programs . find program ( tool ) : programs . run program ( tool , args , cwd = path ) return else : cmdnames = [ name for ( name , args ) in tools ] raise  Action  Tool  Not  Found  ( info [ 'name' ] , action , cmdnames )
def set policy ( table = 'filter' , chain =  None  , policy =  None  , family = 'ipv4' ) : if ( not chain ) : return ' Error :   Chain   needs  to  be  specified' if ( not policy ) : return ' Error :   Policy   needs  to  be  specified' wait = ( '--wait' if  has option ( '--wait' , family ) else '' ) cmd = '{0}  {1}  -t  {2}  -P  {3}  {4}' . format (  iptables cmd ( family ) , wait , table , chain , policy ) out =   salt   [ 'cmd.run' ] ( cmd ) return out
def test formatters (  Chart  ) : if (  Chart  .  dual or (  Chart  ==  Box  ) ) : return chart =  Chart  ( formatter = ( lambda x , chart , serie : ( '%s%s$' % ( x , serie . title ) ) ) ) chart . add ( ' a' , [ 1 , 2 , { 'value' : 3 , 'formatter' : ( lambda x : ( u ( '%s\xc2\xa5' ) % x ) ) } ] ) chart . add ( ' b' , [ 4 , 5 , 6 ] , formatter = ( lambda x : ( u ( '%s\xe2\x82\xac' ) % x ) ) ) chart . x labels = [ 2 , 4 , 6 ] chart . x labels major = [ 4 ] q = chart . render pyquery ( ) assert ( set ( [ v . text for v in q ( '.value' ) ] ) == set ( ( ( u ( '4\xe2\x82\xac' ) , u ( '5\xe2\x82\xac' ) , u ( '6\xe2\x82\xac' ) , '1 a$' , '2 a$' , u ( '3\xc2\xa5' ) ) + ( ( '6 a$' , u ( '15\xe2\x82\xac' ) ) if (  Chart  in (  Pie  ,  Solid  Gauge  ) ) else ( ) ) ) ) )
def uni print ( statement , out file =  None  ) : if ( out file is  None  ) : out file = sys . stdout try : out file . write ( statement ) except  Unicode  Encode  Error  : new encoding = getattr ( out file , 'encoding' , 'ascii' ) if ( new encoding is  None  ) : new encoding = 'ascii' new statement = statement . encode ( new encoding , 'replace' ) . decode ( new encoding ) out file . write ( new statement ) out file . flush ( )
@ retry on failure def test inet pton ( ) : if ( not is cli ) : return socket . inet pton ( socket . AF INET , '127.0.0.1' )  Assert  Error  ( socket . error , socket . inet pton , socket . AF INET , 'garbage  dkfjdkfjdkfj' )
def get Path  ( edges , path Indexes  , loop , z ) : path = [ ] for path Index  Index  in xrange ( len ( path Indexes  ) ) : path Index  = path Indexes  [ path Index  Index  ] edge = edges [ path Index  ] carve Intersection  = get Carve  Intersection  From  Edge  ( edge , loop , z ) path . append ( carve Intersection  ) return path
def synchronous ( tlockname ) : def  synched ( func ) : @ wraps ( func ) def  synchronizer ( self , * args , ** kwargs ) : tlock = getattr ( self , tlockname ) logger . debug ( ( 'acquiring  lock  %r  for  %s' % ( tlockname , func .   name   ) ) ) with tlock : logger . debug ( ( 'acquired  lock  %r  for  %s' % ( tlockname , func .   name   ) ) ) result = func ( self , * args , ** kwargs ) logger . debug ( ( 'releasing  lock  %r  for  %s' % ( tlockname , func .   name   ) ) ) return result return  synchronizer return  synched
def local binary pattern ( image , P , R , method = 'default' ) : assert nD ( image , 2 ) methods = { 'default' : ord ( 'D' ) , 'ror' : ord ( 'R' ) , 'uniform' : ord ( 'U' ) , 'nri uniform' : ord ( 'N' ) , 'var' : ord ( 'V' ) } image = np . ascontiguousarray ( image , dtype = np . double ) output =  local binary pattern ( image , P , R , methods [ method . lower ( ) ] ) return output
def handle Newest  ( qry ) : try : get User  Name  ( ) except : return  skype Error  ( ) qry = qry . decode ( 'utf8' ) try : if ( ( ':' in qry ) and ( qry . partition ( ':' ) [ 0 ] in map ( ( lambda s : s [ 0 ] ) ,  read Friends  ( ) ) ) ) : return  send Message  Wait  ( qry ) else : return  find Newest  ( ) except  Environment  Error  : return  Py  Fred  ( 'ch.xtin.skypingalfred.error' ,  False  ) . add Item  ( 'skypeupdate' , 'skype  update' , ' No    Skype    Friends    Found ' , ' Use   skype  update  to  cache  friends!' ,  True  , 'update' ) . toXML ( ) except : return  Py  Fred  .  Generic  Error  ( )
def issue section ( issue ) : labels = issue . get ( 'labels' , [ ] ) for label in labels : if ( not label [ 'name' ] . startswith ( 'type:  ' ) ) : continue if ( label [ 'name' ] in LOG SECTION ) : return LOG SECTION [ label [ 'name' ] ] elif ( label [ 'name' ] in IGNORE ISSUE TYPE ) : return  None  else : logging . warn ( 'unknown  issue  type:  "{}"  for:  {}' . format ( label [ 'name' ] , issue line ( issue ) ) ) return  None
@ transaction . non atomic requests @ ensure csrf cookie @ cache control ( no cache =  True  , no store =  True  , must revalidate =  True  ) @ require global staff @ require http methods ( [ 'POST' , 'DELETE' ] ) def certificate invalidation view ( request , course id ) : course key =  Course  Key  . from string ( course id ) try : certificate invalidation data = parse request data ( request ) certificate = validate request data and get certificate ( certificate invalidation data , course key ) except  Value  Error  as error : return  Json  Response  ( { 'message' : error . message } , status = 400 ) if ( request . method == 'POST' ) : try : certificate invalidation = invalidate certificate ( request , certificate , certificate invalidation data ) except  Value  Error  as error : return  Json  Response  ( { 'message' : error . message } , status = 400 ) return  Json  Response  ( certificate invalidation ) elif ( request . method == 'DELETE' ) : try : re validate certificate ( request , course key , certificate ) except  Value  Error  as error : return  Json  Response  ( { 'message' : error . message } , status = 400 ) return  Json  Response  ( { } , status = 204 )
def index ( ) : s3 redirect default ( URL ( f = 'alert' ) )
def is 1pexp ( t , only process constants =  True  ) : if ( t . owner and ( t . owner . op == tensor . add ) ) : ( scalars , scalar inputs , nonconsts ) = opt . scalarconsts rest ( t . owner . inputs , only process constants = only process constants ) if ( len ( nonconsts ) == 1 ) : maybe exp = nonconsts [ 0 ] if ( maybe exp . owner and ( maybe exp . owner . op == tensor . exp ) ) : if scalars : scal sum = scalars [ 0 ] for s in scalars [ 1 : ] : scal sum = ( scal sum + s ) if numpy . allclose ( scal sum , 1 ) : return (  False  , maybe exp . owner . inputs [ 0 ] ) if config . warn . identify 1pexp bug : warnings . warn ( " Although   your  current  code  is  fine,  please  note  that   Theano   versions  prior  to  0.5  (more  specifically,  prior  to  commit  7987b51  on  2011-12-18)  may  have  yielded  an  incorrect  result.   To   remove  this  warning,  either  set  the  `warn.identify 1pexp bug`  config  option  to   False ,  or  `warn.ignore bug before`  to  at  least  '0.4.1'." ) return  None
def clips array ( array , rows widths =  None  , cols widths =  None  , bg color =  None  ) : array = np . array ( array ) sizes array = np . array ( [ [ c . size for c in line ] for line in array ] ) if ( rows widths is  None  ) : rows widths = sizes array [ : , : , 1 ] . max ( axis = 1 ) if ( cols widths is  None  ) : cols widths = sizes array [ : , : , 0 ] . max ( axis = 0 ) xx = np . cumsum ( ( [ 0 ] + list ( cols widths ) ) ) yy = np . cumsum ( ( [ 0 ] + list ( rows widths ) ) ) for ( j , ( x , cw ) ) in list ( enumerate ( zip ( xx [ : ( - 1 ) ] , cols widths ) ) ) : for ( i , ( y , rw ) ) in list ( enumerate ( zip ( yy [ : ( - 1 ) ] , rows widths ) ) ) : clip = array [ ( i , j ) ] ( w , h ) = clip . size if ( ( w < cw ) or ( h < rw ) ) : clip =  Composite  Video  Clip  ( [ clip . set pos ( 'center' ) ] , size = ( cw , rw ) , bg color = bg color ) . set duration ( clip . duration ) array [ ( i , j ) ] = clip . set pos ( ( x , y ) ) return  Composite  Video  Clip  ( array . flatten ( ) , size = ( xx [ ( - 1 ) ] , yy [ ( - 1 ) ] ) , bg color = bg color )
def escape byte string ( s ) : s =  replace specials ( s ) try : return s . decode ( 'ASCII' ) except  Unicode  Decode  Error  : pass if IS PYTHON3 : s new = bytearray ( ) ( append , extend ) = ( s new . append , s new . extend ) for b in s : if ( b >= 128 ) : extend ( ( '\\%3o' % b ) . encode ( 'ASCII' ) ) else : append ( b ) return s new . decode ( 'ISO-8859-1' ) else : l = [ ] append = l . append for c in s : o = ord ( c ) if ( o >= 128 ) : append ( ( '\\%3o' % o ) ) else : append ( c ) return join bytes ( l ) . decode ( 'ISO-8859-1' )
def contains ( name , value ) : ret = { 'name' : name , 'result' :  False  , 'comment' : '' , 'changes' : { } } if ( name not in   reg   ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Value   {0}  not  in  register' . format ( name ) return ret try : if ( value in   reg   [ name ] [ 'val' ] ) : ret [ 'result' ] =  True  except  Type  Error  : pass return ret
def  wait for step ( emr connection , step , jobflowid , sleeptime ) : sleep ( 180 ) start = time ( ) step state = get step state ( emr connection , jobflowid , step . name , update =  True  ) while ( step state in ( LIVE STATES + [ PENDING ] ) ) : sleep ( sleeptime ) step state = get step state ( emr connection , jobflowid , step . name ) end = time ( ) print ( '%s  took  %0.2fs  (exit:  %s)' % ( step . name , ( end - start ) , step state ) ) return step state
def write block summary report ( course data ) : ( block summary counts , unique course counts ) =  get block summary totals ( course data ) with open ( 'xblock summary counts.csv' , 'wb' ) as csvfile : summary writer = csv . writer ( csvfile , delimiter = ',' , quotechar = '"' , quoting = csv . QUOTE MINIMAL ) summary writer . writerow ( [ 'XBLOCK NAME' , 'UNIQUE COURSES' , 'NUM TOTAL INSTANCES' ] ) for block type in sorted ( block summary counts ) : block count = block summary counts . get ( block type ) summary writer . writerow ( [ block type , str ( unique course counts [ block type ] ) , str ( block count ) ] ) csvfile . close ( )
def fix line ending ( content ) : return content . replace ( '\r\n' , '\n' ) . replace ( '\r' , '\n' )
def assert equal none ( logical line ) :  start re = re . compile ( 'assert Equal \\(.*?,\\s+ None \\)$' )  end re = re . compile ( 'assert Equal \\( None ,' ) if (  start re . search ( logical line ) or  end re . search ( logical line ) ) : ( yield ( 0 , 'N318:  assert Equal (A,   None )  or  assert Equal ( None ,  A)  sentences  not  allowed.   Use   assert Is  None (A)  instead.' ) )  start re = re . compile ( 'assert Is ( Not )?\\( None ,' )  end re = re . compile ( 'assert Is ( Not )?\\(.*,\\s+ None \\)$' ) if (  start re . search ( logical line ) or  end re . search ( logical line ) ) : ( yield ( 0 , 'N318:  assert Is  Not (A,   None )  or  assert Is  Not ( None ,  A)  must  not  be  used.   Use   assert Is  None (A)  or  assert Is  Not  None (A)  instead.' ) )
def dmp ground LC ( f , u , K ) : while u : f = dmp LC ( f , K ) u -= 1 return dup LC ( f , K )
def describe identity pools (  Identity  Pool  Name  ,  Identity  Pool  Id  =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) try : ids =  find identity pool ids (  Identity  Pool  Name  ,  Identity  Pool  Id  , conn ) if ids : results = [ ] for pool id in ids : response = conn . describe identity pool (  Identity  Pool  Id  = pool id ) response . pop ( ' Response  Metadata ' ,  None  ) results . append ( response ) return { 'identity pools' : results } else : return { 'identity pools' :  None  } except  Client  Error  as e : return { 'error' : salt . utils . boto3 . get error ( e ) }
def token list to text ( tokenlist ) :  Zero  Width  Escape  =  Token  .  Zero  Width  Escape  return u'' . join ( ( item [ 1 ] for item in tokenlist if ( item [ 0 ] !=  Zero  Width  Escape  ) ) )
def connect ( receiver , signal =  Any  , sender =  Any  , weak =  True  ) : if ( signal is  None  ) : raise errors .  Dispatcher  Type  Error  ( ( ' Signal   cannot  be   None   (receiver=%r  sender=%r)' % ( receiver , sender ) ) ) if weak : receiver = saferef . safe Ref  ( receiver , on Delete  =  remove Receiver  ) senderkey = id ( sender ) signals = connections . setdefault ( senderkey , { } ) if ( sender not in (  None  ,  Anonymous  ,  Any  ) ) : def remove ( object , senderkey = senderkey ) :  remove Sender  ( senderkey = senderkey ) try : weak Sender  = weakref . ref ( sender , remove ) senders [ senderkey ] = weak Sender  except : pass receiverID = id ( receiver ) if signals . has key ( signal ) : receivers = signals [ signal ]  remove Old  Back  Refs  ( senderkey , signal , receiver , receivers ) else : receivers = signals [ signal ] = [ ] try : current = senders Back  . get ( receiverID ) if ( current is  None  ) : senders Back  [ receiverID ] = current = [ ] if ( senderkey not in current ) : current . append ( senderkey ) except : pass receivers . append ( receiver )
def  compute neighbors ( image , structure , offset ) : structure [ tuple ( offset ) ] = 0 locations = np . transpose ( np . nonzero ( structure ) ) sqdistances = np . sum ( ( ( locations - offset ) ** 2 ) , axis = 1 ) neighborhood = ( np . ravel multi index ( locations . T , image . shape ) - np . ravel multi index ( offset , image . shape ) ) . astype ( np . int32 ) sorted neighborhood = neighborhood [ np . argsort ( sqdistances ) ] return sorted neighborhood
def cx Uniform  ( ind1 , ind2 , indpb ) : size = min ( len ( ind1 ) , len ( ind2 ) ) for i in xrange ( size ) : if ( random . random ( ) < indpb ) : ( ind1 [ i ] , ind2 [ i ] ) = ( ind2 [ i ] , ind1 [ i ] ) return ( ind1 , ind2 )
def test possible string format functions ( ) : t = Q Table  ( [ ( [ 1 , 2 ] * u . m ) ] ) t [ 'col0' ] . info . format = '%.3f' assert ( t . pformat ( ) == [ '  col0' , '    m    ' , '-----' , '1.000' , '2.000' ] ) t [ 'col0' ] . info . format = 'hi  {:.3f}' assert ( t . pformat ( ) == [ '    col0    ' , '      m        ' , '--------' , 'hi  1.000' , 'hi  2.000' ] ) t [ 'col0' ] . info . format = '.4f' assert ( t . pformat ( ) == [ '  col0  ' , '    m      ' , '------' , '1.0000' , '2.0000' ] )
def is classifier ( estimator ) : return ( getattr ( estimator , ' estimator type' ,  None  ) == 'classifier' )
def get boost ( obj ) : boost = max ( log10 ( ( 1 + get popularity ( obj ) ) ) , 1.0 ) if ( obj . status in VALID STATUSES ) : boost *= BOOST MULTIPLIER FOR PUBLIC CONTENT return boost
def fromstring ( html , guess charset =  True  , parser =  None  ) : if ( not isinstance ( html ,  strings ) ) : raise  Type  Error  ( 'string  required' ) doc = document fromstring ( html , parser = parser , guess charset = guess charset ) start = html [ : 50 ] . lstrip ( ) . lower ( ) if ( start . startswith ( '<html' ) or start . startswith ( '<!doctype' ) ) : return doc head =  find tag ( doc , 'head' ) if len ( head ) : return doc body =  find tag ( doc , 'body' ) if ( ( len ( body ) == 1 ) and ( ( not body . text ) or ( not body . text . strip ( ) ) ) and ( ( not body [ ( - 1 ) ] . tail ) or ( not body [ ( - 1 ) ] . tail . strip ( ) ) ) ) : return body [ 0 ] if  contains block level tag ( body ) : body . tag = 'div' else : body . tag = 'span' return body
def guarded mul ( left , right ) : if ( not isinstance ( left , numbers .  Integral  ) ) : pass elif ( not isinstance ( right , numbers .  Integral  ) ) : pass elif ( ( left in ( 0 , 1 ) ) or ( right in ( 0 , 1 ) ) ) : pass elif ( ( left . bit length ( ) + right . bit length ( ) ) > 664386 ) : raise  Value  Error  ( u' Value   is  too  large  to  be  handled  in  limited  time  and  memory.' ) return operator . mul ( left , right )
def get ec2 driver ( aws ) : ec2 = get driver (  Provider  . EC2 ) ( aws [ 'access key' ] , aws [ 'secret access token' ] , region = aws [ 'region' ] ) return ec2
def p inclusive or expression 2 ( t ) : pass
def get variable values sorted ( variable ) : if variable . is continuous : return [ ] try : return sorted ( variable . values , key = int ) except  Value  Error  : return variable . values
def  grains ( host , protocol =  None  , port =  None  ) : ( username , password ) = find credentials ( DETAILS [ 'host' ] ) ret =   salt   [ 'vsphere.system info' ] ( host = host , username = username , password = password , protocol = protocol , port = port ) GRAINS CACHE . update ( ret ) return GRAINS CACHE
def skip if ( predicate , reason =  None  ) : reason = ( reason or predicate .   name   ) def decorate ( fn ) : fn name = fn .   name   def maybe ( * args , ** kw ) : if predicate ( ) : msg = ( "'%s'  skipped:  %s" % ( fn name , reason ) ) raise  Skip  Test  ( msg ) else : return fn ( * args , ** kw ) return function named ( maybe , fn name ) return decorate
def  get block types from json file ( xblock json file ) : if ( not os . path . isfile ( xblock json file ) ) : print ( 'x Block   configuration  file  does  not  exist:  %s' % xblock json file ) sys . exit ( 2 ) with open ( xblock json file , 'r' ) as json file : type set = set ( ) try : json data = json . loads ( json file . read ( ) ) except  Value  Error  as e : print ( 'x Block   configuration  file  does  not  match  the  expected  layout  and  is  missing  "data"  list:  %s' % xblock json file ) sys . exit ( e . message ) if ( 'data' in json data ) : xblock type list = json data [ 'data' ] for xblock in xblock type list : type set . add ( xblock [ 'name' ] ) return type set else : print ( 'x Block   configuration  file  does  not  match  the  expected  layout  and  is  missing  "data"  list:  %s' % xblock json file ) sys . exit ( 2 )
def  test ( ) : if sys . argv [ 1 : ] : if sys . argv [ 2 : ] : sys . stderr . write ( 'usage:  python  dis.py  [-|file]\n' ) sys . exit ( 2 ) fn = sys . argv [ 1 ] if ( ( not fn ) or ( fn == '-' ) ) : fn =  None  else : fn =  None  if ( fn is  None  ) : f = sys . stdin else : f = open ( fn ) source = f . read ( ) if ( fn is not  None  ) : f . close ( ) else : fn = '<stdin>' code = compile ( source , fn , 'exec' ) dis ( code )
def test expression formatting ( ) : G =  Gaussian 1D G2 =  Gaussian 2D M = ( G + G ) assert ( M .  format expression ( ) == u'[0]  +  [1]' ) M = ( ( G + G ) + G ) assert ( M .  format expression ( ) == u'[0]  +  [1]  +  [2]' ) M = ( G + ( G * G ) ) assert ( M .  format expression ( ) == u'[0]  +  [1]  *  [2]' ) M = ( ( G * G ) + G ) assert ( M .  format expression ( ) == u'[0]  *  [1]  +  [2]' ) M = ( ( G + ( G * G ) ) + G ) assert ( M .  format expression ( ) == u'[0]  +  [1]  *  [2]  +  [3]' ) M = ( ( G + G ) * ( G + G ) ) assert ( M .  format expression ( ) == u'([0]  +  [1])  *  ([2]  +  [3])' ) M = ( ( G * G ) + ( G * G ) ) assert ( M .  format expression ( ) == u'[0]  *  [1]  +  [2]  *  [3]' ) M = ( G ** G ) assert ( M .  format expression ( ) == u'[0]  **  [1]' ) M = ( G + ( G ** G ) ) assert ( M .  format expression ( ) == u'[0]  +  [1]  **  [2]' ) M = ( ( G + G ) ** G ) assert ( M .  format expression ( ) == u'([0]  +  [1])  **  [2]' ) M = ( ( G + G ) | G ) assert ( M .  format expression ( ) == u'[0]  +  [1]  |  [2]' ) M = ( G + ( G | G ) ) assert ( M .  format expression ( ) == u'[0]  +  ([1]  |  [2])' ) M = ( ( G & G ) | G2 ) assert ( M .  format expression ( ) == u'[0]  &  [1]  |  [2]' ) M = ( G & ( G | G ) ) assert ( M .  format expression ( ) == u'[0]  &  ([1]  |  [2])' )
def parse cost limit ( source ) : cost pos = source . pos digits = parse count ( source ) try : return int ( digits ) except  Value  Error  : pass raise error ( 'bad  fuzzy  cost  limit' , source . string , cost pos )
def test hermite kochanek bartels (  Chart  , datas ) : chart =  Chart  ( interpolate = 'hermite' , interpolation parameters = { 'type' : 'kochanek bartels' , 'b' : ( - 1 ) , 'c' : 1 , 't' : 1 } ) chart = make data ( chart , datas ) assert chart . render ( ) chart =  Chart  ( interpolate = 'hermite' , interpolation parameters = { 'type' : 'kochanek bartels' , 'b' : ( - 1 ) , 'c' : ( - 8 ) , 't' : 0 } ) chart = make data ( chart , datas ) assert chart . render ( ) chart =  Chart  ( interpolate = 'hermite' , interpolation parameters = { 'type' : 'kochanek bartels' , 'b' : 0 , 'c' : 10 , 't' : ( - 1 ) } ) chart = make data ( chart , datas ) assert chart . render ( )
def find variables ( data , include named =  True  ) : found = set ( ) def f ( val ) : if ( not isinstance ( val , basestring ) ) : return for match in  Template  . pattern . finditer ( val ) : name = ( ( include named and match . group ( 'named' ) ) or match . group ( 'braced' ) ) if ( name is not  None  ) : found . add ( name ) recursive walk ( f , data ) return found
def match hostname ( cert , hostname ) : if ( not cert ) : raise  Value  Error  ( u'empty  or  no  certificate' ) dnsnames = [ ] san = cert . get ( u'subject Alt  Name ' , ( ) ) for ( key , value ) in san : if ( key == u'DNS' ) : if  dnsname match ( value , hostname ) : return dnsnames . append ( value ) if ( not dnsnames ) : for sub in cert . get ( u'subject' , ( ) ) : for ( key , value ) in sub : if ( key == u'common Name ' ) : if  dnsname match ( value , hostname ) : return dnsnames . append ( value ) if ( len ( dnsnames ) > 1 ) : raise  Certificate  Error  ( ( u"hostname  %r  doesn't  match  either  of  %s" % ( hostname , u',  ' . join ( map ( repr , dnsnames ) ) ) ) ) elif ( len ( dnsnames ) == 1 ) : if ( ( sys . version info [ : 3 ] < ( 2 , 7 , 3 ) ) and ( dnsnames [ 0 ] == u'calibre-ebook.com' ) ) : return raise  Certificate  Error  ( ( u"hostname  %r  doesn't  match  %r" % ( hostname , dnsnames [ 0 ] ) ) ) else : raise  Certificate  Error  ( u'no  appropriate  common Name   or  subject Alt  Name   fields  were  found' )
def get Elements  By  Local  Name  ( child Nodes  , local Name  ) : elements By  Local  Name  = get Child  Elements  By  Local  Name  ( child Nodes  , local Name  ) for child Node  in child Nodes  : if ( child Node  . get Node  Type  ( ) == 1 ) : elements By  Local  Name  += child Node  . get Elements  By  Local  Name  ( local Name  ) return elements By  Local  Name
def splitdrive ( p ) : if ( len ( p ) > 1 ) : normp = p . replace ( altsep , sep ) if ( ( normp [ 0 : 2 ] == ( sep * 2 ) ) and ( normp [ 2 ] != sep ) ) : index = normp . find ( sep , 2 ) if ( index == ( - 1 ) ) : return ( '' , p ) index2 = normp . find ( sep , ( index + 1 ) ) if ( index2 == ( index + 1 ) ) : return ( '' , p ) if ( index2 == ( - 1 ) ) : index2 = len ( p ) return ( p [ : index2 ] , p [ index2 : ] ) if ( normp [ 1 ] == ':' ) : return ( p [ : 2 ] , p [ 2 : ] ) return ( '' , p )
def link ( src , dst ) : if ( os . name == u'nt' ) : if ( ctypes . windll . kernel32 .  Create  Hard  Link W ( dst , src , 0 ) == 0 ) : raise ctypes .  Win  Error  ( ) else : os . link ( src , dst )
def  clear namespace ( ) : ok names = set ( default backend .   dict   ) ok names . update ( [ 'gl2' , 'glplus' ] ) NS = globals ( ) for name in list ( NS . keys ( ) ) : if name . lower ( ) . startswith ( 'gl' ) : if ( name not in ok names ) : del NS [ name ]
def  Flag  Overrider  ( ** flag kwargs ) : def  Decorator  ( f ) : ' Allow   a  function  to  safely  change  flags,  restoring  them  on  return.' def  Decorated  ( * args , ** kwargs ) : global FLAGS old flags = copy . copy ( FLAGS ) for ( k , v ) in flag kwargs . items ( ) : setattr ( FLAGS , k , v ) try : return f ( * args , ** kwargs ) finally : FLAGS = old flags return  Decorated  return  Decorator
def get integration manager ( ) : global  integration manager if ( not  integration manager ) : from reviewboard . integrations . models import  Integration  Config   integration manager =  Integration  Manager  (  Integration  Config  ) return  integration manager
def singleton ( cls ) :  instances = { } def get instance ( * args , ** kwargs ) : if ( cls not in  instances ) :  instances [ cls ] = cls ( * args , ** kwargs ) return  instances [ cls ] return get instance
def register task phase ( name , before =  None  , after =  None  ) : if ( before and after ) : raise  Register  Exception  ( u' You   can  only  give  either  before  or  after  for  a  phase.' ) if ( ( not before ) and ( not after ) ) : raise  Register  Exception  ( u' You   must  specify  either  a  before  or  after  phase.' ) if ( ( name in task phases ) or ( name in  new phase queue ) ) : raise  Register  Exception  ( ( u' Phase   %s  already  exists.' % name ) ) def add phase ( phase name , before , after ) : if ( ( before is not  None  ) and ( before not in task phases ) ) : return  False  if ( ( after is not  None  ) and ( after not in task phases ) ) : return  False  phase methods [ phase name ] = ( u'on task ' + phase name ) if ( before is  None  ) : task phases . insert ( ( task phases . index ( after ) + 1 ) , phase name ) if ( after is  None  ) : task phases . insert ( task phases . index ( before ) , phase name ) return  True  if ( not add phase ( name , before , after ) ) :  new phase queue [ name ] = [ before , after ] for ( phase name , args ) in list (  new phase queue . items ( ) ) : if add phase ( phase name , * args ) : del  new phase queue [ phase name ]
def compute g ( n ) : a = compute a ( ( 2 * n ) ) g = [ ] for k in range ( n ) : g . append ( ( ( mp . sqrt ( 2 ) * mp . rf ( 0.5 , k ) ) * a [ ( 2 * k ) ] ) ) return g
def set diff chunk generator class ( renderer ) : assert renderer globals ( ) [ u' generator' ] = renderer
def  should use proxy ( url , no proxy =  None  ) : if ( no proxy is  None  ) : no proxy effective = os . environ . get ( 'no proxy' , '' ) else : no proxy effective = no proxy url Obj  = urlparse . urlparse (  url as string ( url ) ) for np in [ h . strip ( ) for h in no proxy effective . split ( ',' ) ] : if ( url Obj  . hostname == np ) : return  False  return  True
def  task info format ( task info ref ) : if ( task info ref is  None  ) : return { } return { 'task id' : task info ref [ 'task id' ] , 'input' : task info ref [ 'input' ] , 'result' : task info ref [ 'result' ] , 'message' : task info ref [ 'message' ] }
def plugin cache dir ( ) : return os . path . join ( tempfile . gettempdir ( ) , ' Ulti  Snips  test vim plugins' )
def keywords ( text ) : NUM KEYWORDS = 10 text = split words ( text ) if text : num words = len ( text ) text = [ x for x in text if ( x not in stopwords ) ] freq = { } for word in text : if ( word in freq ) : freq [ word ] += 1 else : freq [ word ] = 1 min size = min ( NUM KEYWORDS , len ( freq ) ) keywords = sorted ( freq . items ( ) , key = ( lambda x : ( x [ 1 ] , x [ 0 ] ) ) , reverse =  True  ) keywords = keywords [ : min size ] keywords = dict ( ( ( x , y ) for ( x , y ) in keywords ) ) for k in keywords : article Score  = ( ( keywords [ k ] * 1.0 ) / max ( num words , 1 ) ) keywords [ k ] = ( ( article Score  * 1.5 ) + 1 ) return dict ( keywords ) else : return dict ( )
def mod aggregate ( low , chunks , running ) : pkgs = [ ] agg enabled = [ 'installed' , 'latest' , 'removed' , 'purged' ] if ( low . get ( 'fun' ) not in agg enabled ) : return low for chunk in chunks : tag = salt . utils . gen state tag ( chunk ) if ( tag in running ) : continue if ( chunk . get ( 'state' ) == 'pkg' ) : if ( '  agg  ' in chunk ) : continue if ( chunk . get ( 'fun' ) != low . get ( 'fun' ) ) : continue if ( chunk . get ( 'fromrepo' ) != low . get ( 'fromrepo' ) ) : continue if ( 'pkgs' in chunk ) : pkgs . extend ( chunk [ 'pkgs' ] ) chunk [ '  agg  ' ] =  True  elif ( 'name' in chunk ) : pkgs . append ( chunk [ 'name' ] ) chunk [ '  agg  ' ] =  True  if pkgs : if ( 'pkgs' in low ) : low [ 'pkgs' ] . extend ( pkgs ) else : low [ 'pkgs' ] = pkgs return low
def after script ( ) : destroy cmd = [ 'terraform' , 'destroy' , '-force' ] logging . info ( ' Destroying   cloud  provider  resources' ) sys . exit ( run cmd ( destroy cmd ) )
def get preferred submodules ( ) : if ( 'submodules' in modules db ) : return modules db [ 'submodules' ] mods = [ 'numpy' , 'scipy' , 'sympy' , 'pandas' , 'networkx' , 'statsmodels' , 'matplotlib' , 'sklearn' , 'skimage' , 'mpmath' , 'os' , 'PIL' , ' Open GL' , 'array' , 'audioop' , 'binascii' , 'c Pickle ' , 'c String IO' , 'cmath' , 'collections' , 'datetime' , 'errno' , 'exceptions' , 'gc' , 'imageop' , 'imp' , 'itertools' , 'marshal' , 'math' , 'mmap' , 'msvcrt' , 'nt' , 'operator' , 'parser' , 'rgbimg' , 'signal' , 'strop' , 'sys' , 'thread' , 'time' , 'wx' , 'xxsubtype' , 'zipimport' , 'zlib' , 'nose' , ' Py  Qt 4' , ' Py  Side ' , 'os.path' ] submodules = [ ] for m in mods : submods = get submodules ( m ) submodules += submods modules db [ 'submodules' ] = submodules return submodules
def recover ( request , uidb64 =  None  , token =  None  ) :  User  Model  = get user model ( ) try : uid = force text ( urlsafe base64 decode ( uidb64 ) ) user =  User  Model  .  default manager . get ( pk = uid ) except (  Type  Error  ,  Value  Error  ,  Overflow  Error  ,  User  Model  .  Does  Not  Exist  ) : user =  None  if ( user and default token generator . check token ( user , token ) ) : temp pwd = uuid . uuid4 ( ) . hex user . set password ( temp pwd ) user . save ( ) user = authenticate ( username = user . username , password = temp pwd ) user . set unusable password ( ) user . save ( ) login ( request , user ) return redirect ( 'users.recover done' ) return render ( request , 'users/recover failed.html' )
def test hash ( ) : d0 = dict ( a = dict ( a = 0.1 , b = 'fo' , c = 1 ) , b = [ 1 , 'b' ] , c = ( ) , d = np . ones ( 3 ) , e =  None  ) d0 [ 1 ] =  None  d0 [ 2.0 ] = '123' d1 = deepcopy ( d0 ) assert true ( ( len ( object diff ( d0 , d1 ) ) == 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) == 0 ) ) assert equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 [ 'data' ] = np . ones ( 3 , int ) d1 [ 'd' ] [ 0 ] = 0 assert not equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 = deepcopy ( d0 ) assert equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 [ 'a' ] [ 'a' ] = 0.11 assert true ( ( len ( object diff ( d0 , d1 ) ) > 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) > 0 ) ) assert not equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 = deepcopy ( d0 ) assert equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 [ 'a' ] [ 'd' ] = 0 assert true ( ( len ( object diff ( d0 , d1 ) ) > 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) > 0 ) ) assert not equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 = deepcopy ( d0 ) assert equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 [ 'b' ] . append ( 0 ) assert true ( ( len ( object diff ( d0 , d1 ) ) > 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) > 0 ) ) assert not equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 = deepcopy ( d0 ) assert equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 [ 'e' ] = 'foo' assert true ( ( len ( object diff ( d0 , d1 ) ) > 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) > 0 ) ) assert not equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 = deepcopy ( d0 ) d2 = deepcopy ( d0 ) d1 [ 'e' ] =  String IO ( ) d2 [ 'e' ] =  String IO ( ) d2 [ 'e' ] . write ( 'foo' ) assert true ( ( len ( object diff ( d0 , d1 ) ) > 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) > 0 ) ) d1 = deepcopy ( d0 ) d1 [ 1 ] = 2 assert true ( ( len ( object diff ( d0 , d1 ) ) > 0 ) ) assert true ( ( len ( object diff ( d1 , d0 ) ) > 0 ) ) assert not equal ( object hash ( d0 ) , object hash ( d1 ) ) d1 = deepcopy ( d0 ) d2 = deepcopy ( d0 ) d1 [ 1 ] = ( x for x in d0 ) d2 [ 1 ] = ( x for x in d0 ) assert raises (  Runtime  Error  , object diff , d1 , d2 ) assert raises (  Runtime  Error  , object hash , d1 ) x = sparse . eye ( 2 , 2 , format = 'csc' ) y = sparse . eye ( 2 , 2 , format = 'csr' ) assert true ( ( 'type  mismatch' in object diff ( x , y ) ) ) y = sparse . eye ( 2 , 2 , format = 'csc' ) assert equal ( len ( object diff ( x , y ) ) , 0 ) y [ ( 1 , 1 ) ] = 2 assert true ( ( 'elements' in object diff ( x , y ) ) ) y = sparse . eye ( 3 , 3 , format = 'csc' ) assert true ( ( 'shape' in object diff ( x , y ) ) ) y = 0 assert true ( ( 'type  mismatch' in object diff ( x , y ) ) )
def test ada fit ( ) : ada = ADASYN ( random state = RND SEED ) ada . fit ( X , Y ) assert equal ( ada . min c  , 0 ) assert equal ( ada . maj c  , 1 ) assert equal ( ada . stats c  [ 0 ] , 8 ) assert equal ( ada . stats c  [ 1 ] , 12 )
@ register . simple tag def static ( path ) : return staticfiles storage . url ( path )
def is Large  Same  Direction  ( inset , loop , radius ) : if ( euclidean . is Widdershins  ( inset ) != euclidean . is Widdershins  ( loop ) ) : return  False  return ( get Is  Large  ( inset , radius ) and ( len ( inset ) > 2 ) )
def  tgrep conjunction action (  s ,  l , tokens , join char = u'&' ) : tokens = [ x for x in tokens if ( x != join char ) ] if ( len ( tokens ) == 1 ) : return tokens [ 0 ] else : return ( lambda ts : ( lambda n , m =  None  , l =  None  : all ( ( predicate ( n , m , l ) for predicate in ts ) ) ) ) ( tokens )
def  default pprint ( obj , p , cycle ) : klass = (  safe getattr ( obj , '  class  ' ,  None  ) or type ( obj ) ) if (  safe getattr ( klass , '  repr  ' ,  None  ) not in  baseclass reprs ) :  repr pprint ( obj , p , cycle ) return p . begin group ( 1 , '<' ) p . pretty ( klass ) p . text ( ( '  at  0x%x' % id ( obj ) ) ) if cycle : p . text ( '  ...' ) elif p . verbose : first =  True  for key in dir ( obj ) : if ( not key . startswith ( ' ' ) ) : try : value = getattr ( obj , key ) except  Attribute  Error  : continue if isinstance ( value , types .  Method  Type  ) : continue if ( not first ) : p . text ( ',' ) p . breakable ( ) p . text ( key ) p . text ( '=' ) step = ( len ( key ) + 1 ) p . indentation += step p . pretty ( value ) p . indentation -= step first =  False  p . end group ( 1 , '>' )
def build bond ( iface , ** settings ) : deb major =   grains   [ 'osrelease' ] [ : 1 ] opts =  parse settings bond ( settings , iface ) try : template = JINJA . get template ( 'conf.jinja' ) except jinja2 . exceptions .  Template  Not  Found  : log . error ( ' Could   not  load  template  conf.jinja' ) return '' data = template . render ( { 'name' : iface , 'bonding' : opts } ) if ( ( 'test' in settings ) and settings [ 'test' ] ) : return  read temp ( data )  write file ( iface , data ,  DEB NETWORK CONF FILES , '{0}.conf' . format ( iface ) ) path = os . path . join (  DEB NETWORK CONF FILES , '{0}.conf' . format ( iface ) ) if ( deb major == '5' ) : for line type in ( 'alias' , 'options' ) : cmd = [ 'sed' , '-i' , '-e' , '/^{0}\\s{1}.*/d' . format ( line type , iface ) , '/etc/modprobe.conf' ]   salt   [ 'cmd.run' ] ( cmd , python shell =  False  )   salt   [ 'file.append' ] ( '/etc/modprobe.conf' , path )   salt   [ 'kmod.load' ] ( 'bonding' )   salt   [ 'pkg.install' ] ( 'ifenslave-2.6' ) return  read file ( path )
def generative network ( z ) : with slim . arg scope ( [ slim . conv2d transpose ] , activation fn = tf . nn . elu , normalizer fn = slim . batch norm , normalizer params = { 'scale' :  True  } ) : net = tf . reshape ( z , [ M , 1 , 1 , d ] ) net = slim . conv2d transpose ( net , 128 , 3 , padding = 'VALID' ) net = slim . conv2d transpose ( net , 64 , 5 , padding = 'VALID' ) net = slim . conv2d transpose ( net , 32 , 5 , stride = 2 ) net = slim . conv2d transpose ( net , 1 , 5 , stride = 2 , activation fn =  None  ) net = slim . flatten ( net ) return net
def get file json ( path ) : with open ( path , u'r' ) as f : return json . load ( f )
def test depth first mro ( ) : class A ( object , ) : pass class B ( A , ) : pass class C ( A , ) : pass class D ( B , C , ) : pass class E ( D , object , ) : pass class G ( object , ) : pass class H ( G , ) : pass class I ( G , ) : pass class K ( H , I , object , ) : pass class L ( K , E , ) : pass  Are  Equal  ( L .   mro   , ( L , K , H , I , G , E , D , B , C , A , object ) )
def get svc avail path ( ) : return AVAIL SVR DIRS
def test pprint npfloat32 ( ) : dat = np . array ( [ 1.0 , 2.0 ] , dtype = np . float32 ) t =  Table  ( [ dat ] , names = [ 'a' ] ) t [ 'a' ] . format = '5.2f' assert ( str ( t [ 'a' ] ) == '    a    \n-----\n  1.00\n  2.00' )
def test interpolation (  Chart  ) : chart =  Chart  ( interpolate = 'cubic' ) chart . add ( '1' , [ 1 , 3 , 12 , 3 , 4 ] ) chart . add ( '2' , [ 7 , ( - 4 ) , 10 ,  None  , 8 , 3 , 1 ] ) q = chart . render pyquery ( ) assert ( len ( q ( '.legend' ) ) == 2 )
def run migrations offline ( ) : engine manager =  Engine  Manager  ( config . get required ( 'DATABASE HOSTS' ) , config . get required ( 'DATABASE USERS' ) , include disabled =  True  ) engine = engine manager . engines [ shard id ] context . configure ( engine = engine , url = engine . url ) with context . begin transaction ( ) : context . run migrations ( )
def store temp file ( filedata , filename , path =  None  ) : filename = get filename from path ( filename ) filename = filename [ : 100 ] options =  Config  ( ) if path : target path = path else : tmp path = options . cuckoo . get ( 'tmppath' , '/tmp' ) target path = os . path . join ( tmp path , 'cuckoo-tmp' ) if ( not os . path . exists ( target path ) ) : os . mkdir ( target path ) tmp dir = tempfile . mkdtemp ( prefix = 'upload ' , dir = target path ) tmp file path = os . path . join ( tmp dir , filename ) with open ( tmp file path , 'wb' ) as tmp file : if hasattr ( filedata , 'read' ) : chunk = filedata . read ( 1024 ) while chunk : tmp file . write ( chunk ) chunk = filedata . read ( 1024 ) else : tmp file . write ( filedata ) return tmp file path
def  get default tempdir ( ) : namer =   Random  Name  Sequence  ( ) dirlist =  candidate tempdir list ( ) for dir in dirlist : if ( dir !=  os . curdir ) : dir =  os . path . abspath ( dir ) for seq in range ( 100 ) : name = next ( namer ) filename =  os . path . join ( dir , name ) try : fd =  os . open ( filename ,  bin openflags , 384 ) try : try : with  io . open ( fd , 'wb' , closefd =  False  ) as fp : fp . write ( 'blat' ) finally :  os . close ( fd ) finally :  os . unlink ( filename ) return dir except  File  Exists  Error  : pass except  Permission  Error  : if ( (  os . name == 'nt' ) and  os . path . isdir ( dir ) and  os . access ( dir ,  os . W OK ) ) : continue break except OS Error  : break raise  File  Not  Found  Error  (  errno . ENOENT , ( ' No   usable  temporary  directory  found  in  %s' % dirlist ) )
def attach total voters to queryset ( queryset , as field = 'total voters' ) : model = queryset . model type = apps . get model ( 'contenttypes' , ' Content  Type ' ) . objects . get for model ( model ) sql = 'SELECT  coalesce(SUM(total voters),  0)  FROM  (\n                                SELECT  coalesce(votes votes.count,  0)  total voters\n                                    FROM  votes votes\n                                  WHERE  votes votes.content type id  =  {type id}\n                                      AND  votes votes.object id  =  {tbl}.id\n                    )  as  e' sql = sql . format ( type id = type . id , tbl = model .  meta . db table ) qs = queryset . extra ( select = { as field : sql } ) return qs
def  add theming locales ( ) : theme locale paths = settings . COMPREHENSIVE THEME LOCALE PATHS for locale path in theme locale paths : settings . LOCALE PATHS += ( path ( locale path ) , )
@ deprecated (  Version  ( ' Twisted ' , 15 , 3 , 0 ) , replacement = 'twisted.web.template' ) def output ( func , * args , ** kw ) : try : return func ( * args , ** kw ) except : log . msg ( ( ' Error   calling  %r:' % ( func , ) ) ) log . err ( ) return PRE ( ' An   error  occurred.' )
def gf factor sqf ( f , p , K , method =  None  ) : ( lc , f ) = gf monic ( f , p , K ) if ( gf degree ( f ) < 1 ) : return ( lc , [ ] ) method = ( method or query ( 'GF FACTOR METHOD' ) ) if ( method is not  None  ) : factors =  factor methods [ method ] ( f , p , K ) else : factors = gf zassenhaus ( f , p , K ) return ( lc , factors )
def present ( name , force =  False  , bare =  True  , template =  None  , separate git dir =  None  , shared =  None  , user =  None  , password =  None  ) : ret = { 'name' : name , 'result' :  True  , 'comment' : '' , 'changes' : { } } if os . path . isdir ( name ) : if ( bare and os . path . isfile ( os . path . join ( name , 'HEAD' ) ) ) : return ret elif ( ( not bare ) and ( os . path . isdir ( os . path . join ( name , '.git' ) ) or   salt   [ 'git.is worktree' ] ( name , user = user , password = password ) ) ) : return ret elif force : if   opts   [ 'test' ] : ret [ 'changes' ] [ 'new' ] = name ret [ 'changes' ] [ 'forced  init' ] =  True  return  neutral test ( ret , ' Target   directory  {0}  exists.   Since   force= True ,  the  contents  of  {0}  would  be  deleted,  and  a  {1}repository  would  be  initialized  in  its  place.' . format ( name , ( 'bare  ' if bare else '' ) ) ) log . debug ( ' Removing   contents  of  {0}  to  initialize  {1}repository  in  its  place  (force= True   set  in  git.present  state)' . format ( name , ( 'bare  ' if bare else '' ) ) ) try : if os . path . islink ( name ) : os . unlink ( name ) else : salt . utils . rm rf ( name ) except OS Error  as exc : return  fail ( ret , ' Unable   to  remove  {0}:  {1}' . format ( name , exc ) ) else : ret [ 'changes' ] [ 'forced  init' ] =  True  elif os . listdir ( name ) : return  fail ( ret , " Target   '{0}'  exists,  is  non-empty,  and  is  not  a  git  repository.   Set   the  'force'  option  to   True   to  remove  this  directory's  contents  and  proceed  with  initializing  a  repository" . format ( name ) ) if   opts   [ 'test' ] : ret [ 'changes' ] [ 'new' ] = name return  neutral test ( ret , ' New   {0}repository  would  be  created' . format ( ( 'bare  ' if bare else '' ) ) )   salt   [ 'git.init' ] ( cwd = name , bare = bare , template = template , separate git dir = separate git dir , shared = shared , user = user , password = password ) actions = [ ' Initialized   {0}repository  in  {1}' . format ( ( 'bare  ' if bare else '' ) , name ) ] if template : actions . append ( ' Template   directory  set  to  {0}' . format ( template ) ) if separate git dir : actions . append ( ' Gitdir   set  to  {0}' . format ( separate git dir ) ) message = '.  ' . join ( actions ) if ( len ( actions ) > 1 ) : message += '.' log . info ( message ) ret [ 'changes' ] [ 'new' ] = name ret [ 'comment' ] = message return ret
def unhex ( s ) : bits = 0 for c in s : c = bytes ( ( c , ) ) if ( '0' <= c <= '9' ) : i = ord ( '0' ) elif ( 'a' <= c <= 'f' ) : i = ( ord ( 'a' ) - 10 ) elif ( 'A' <= c <= 'F' ) : i = ( ord ( 'A' ) - 10 ) else : assert  False  , ( 'non-hex  digit  ' + repr ( c ) ) bits = ( ( bits * 16 ) + ( ord ( c ) - i ) ) return bits
def  guess autoescape ( template name ) : if ( ( template name is  None  ) or ( '.' not in template name ) ) : return  False  ext = template name . rsplit ( '.' , 1 ) [ 1 ] return ( ext in [ 'html' , 'htm' , 'xml' ] )
def walk ( top , topdown =  True  , followlinks =  False  ) : names = os . listdir ( top ) ( dirs , nondirs ) = ( [ ] , [ ] ) for name in names : if path . isdir ( path . join ( top , name ) ) : dirs . append ( name ) else : nondirs . append ( name ) if topdown : ( yield ( top , dirs , nondirs ) ) for name in dirs : fullpath = path . join ( top , name ) if ( followlinks or ( not path . islink ( fullpath ) ) ) : for x in walk ( fullpath , topdown , followlinks ) : ( yield x ) if ( not topdown ) : ( yield ( top , dirs , nondirs ) )
@ raises (  Value  Error  ) def test bootstrap arglength ( ) : algo . bootstrap ( np . arange ( 5 ) , np . arange ( 10 ) )
def parse strtime ( timestr , fmt = PERFECT TIME FORMAT ) : return datetime . datetime . strptime ( timestr , fmt )
def hash filehash ( filename ) : md4 = hashlib . new ( u'md4' ) . copy def gen ( f ) : while  True  : x = f . read ( 9728000 ) if x : ( yield x ) else : return def md4 hash ( data ) : m = md4 ( ) m . update ( data ) return m with open ( filename , u'rb' ) as f : a = gen ( f ) hashes = [ md4 hash ( data ) . digest ( ) for data in a ] if ( len ( hashes ) == 1 ) : return to hex ( hashes [ 0 ] ) else : return md4 hash ( reduce ( ( lambda a , d : ( a + d ) ) , hashes , u'' ) ) . hexd
def create connection ( dest pair , proxy type =  None  , proxy addr =  None  , proxy port =  None  , proxy username =  None  , proxy password =  None  , timeout =  None  ) : sock = socksocket ( ) if isinstance ( timeout , ( int , float ) ) : sock . settimeout ( timeout ) sock . set proxy ( proxy type , proxy addr , proxy port , proxy username , proxy password ) sock . connect ( dest pair ) return sock
def add Begin XML Tag  ( attributes , depth , local Name  , output , text = '' ) : depth Start  = ( ' DCTB ' * depth ) output . write ( ( '%s<%s%s>%s\n' % ( depth Start  , local Name  , get Attributes  String  ( attributes ) , text ) ) )
def exists ( name ) : with settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn only =  True  ) : return run ( ( 'getent  passwd  %(name)s' % locals ( ) ) ) . succeeded
def write Output  ( file Name  = '' ) : print '' print ' The   bottom  tool  is  parsing  the  file:' print os . path . basename ( file Name  ) print '' start Time  = time . time ( ) file Name  Suffix  = ( file Name  [ : file Name  . rfind ( '.' ) ] + ' bottom.svg' ) craft Text  = skeinforge craft . get Chain  Text  ( file Name  , 'bottom' ) if ( craft Text  == '' ) : return archive . write File  Text  ( file Name  Suffix  , craft Text  ) print '' print ' The   bottom  tool  has  created  the  file:' print file Name  Suffix  print '' print ( ' It   took  %s  to  craft  the  file.' % euclidean . get Duration  String  ( ( time . time ( ) - start Time  ) ) ) repository =  Bottom  Repository  ( ) settings . get Read  Repository  ( repository ) settings . openSVG Page  ( file Name  Suffix  , repository . svg Viewer  . value )
def initialize log data ( ids bcs added field ) : log data = { } for curr key in ids bcs added field . keys ( ) : base key = '' if curr key [ 0 ] : base key += ( curr key [ 0 ] + ',' ) if curr key [ 1 ] : base key += ( curr key [ 1 ] + ',' ) base key += ids bcs added field [ curr key ] log data [ base key ] = 0 return log data
@ control command ( args = [ ( u'task name' , text t ) , ( u'rate limit' , text t ) ] , signature = u'<task name>  <rate limit  (e.g.,  5/s  |  5/m  |  5/h)>' ) def rate limit ( state , task name , rate limit , ** kwargs ) : try : rate ( rate limit ) except  Value  Error  as exc : return nok ( u' Invalid   rate  limit  string:  {0!r}' . format ( exc ) ) try : state . app . tasks [ task name ] . rate limit = rate limit except  Key  Error  : logger . error ( u' Rate   limit  attempt  for  unknown  task  %s' , task name , exc info =  True  ) return nok ( u'unknown  task' ) state . consumer . reset rate limits ( ) if ( not rate limit ) : logger . info ( u' Rate   limits  disabled  for  tasks  of  type  %s' , task name ) return ok ( u'rate  limit  disabled  successfully' ) logger . info ( u' New   rate  limit  for  tasks  of  type  %s:  %s.' , task name , rate limit ) return ok ( u'new  rate  limit  set  successfully' )
def highlighting ( view , name , style , left , right ) : tag settings = sublime . load settings ( 'bh tag.sublime-settings' ) match style = tag settings . get ( 'tag style' , { } ) . get ( last mode ,  None  ) if ( ( match style is not  None  ) and ( style == match style ) ) : tag name = tag settings . get ( 'tag name' , { } ) . get ( last mode , '[\\w\\:\\.\\-]+' ) if ( left is not  None  ) : region = view . find ( tag name , left . begin ) left = left . move ( region . begin ( ) , region . end ( ) ) if ( right is not  None  ) : region = view . find ( tag name , right . begin ) right = right . move ( region . begin ( ) , region . end ( ) ) return ( left , right )
@ depends ( HAS PYVMOMI ) def get ntp config ( host , username , password , protocol =  None  , port =  None  , host names =  None  ) : service instance = salt . utils . vmware . get service instance ( host = host , username = username , password = password , protocol = protocol , port = port ) host names =  check hosts ( service instance , host , host names ) ret = { } for host name in host names : host ref =  get host ref ( service instance , host , host name = host name ) ntp config = host ref . config Manager  . date Time  System  . date Time  Info  . ntp Config  . server ret . update ( { host name : ntp config } ) return ret
@ pytest . fixture ( scope = u'session' ) def celery config ( ) : return { }
def update ( directory , composer =  None  , php =  None  , runas =  None  , prefer source =  None  , prefer dist =  None  , no scripts =  None  , no plugins =  None  , optimize =  None  , no dev =  None  , quiet =  False  , composer home = '/root' ) : result =  run composer ( 'update' , directory = directory , extra flags = '--no-progress' , composer = composer , php = php , runas = runas , prefer source = prefer source , prefer dist = prefer dist , no scripts = no scripts , no plugins = no plugins , optimize = optimize , no dev = no dev , quiet = quiet , composer home = composer home ) return result
@ validate ( 'tree' ) def valid field in tree ( arch ) : return all ( ( ( child . tag in ( 'field' , 'button' ) ) for child in arch . xpath ( '/tree/*' ) ) )
def  mt spectrum remove ( x , sfreq , line freqs , notch widths , window fun , threshold ) : n tapers = len ( window fun ) tapers odd = np . arange ( 0 , n tapers , 2 ) tapers even = np . arange ( 1 , n tapers , 2 ) tapers use = window fun [ tapers odd ] H0 = np . sum ( tapers use , axis = 1 ) H0 sq = sum squared ( H0 ) rads = ( ( 2 * np . pi ) * ( np . arange ( x . size ) / float ( sfreq ) ) ) ( x p , freqs ) =  mt spectra ( x [ np . newaxis , : ] , window fun , sfreq ) x p H0 = np . sum ( ( x p [ : , tapers odd , : ] * H0 [ np . newaxis , : , np . newaxis ] ) , axis = 1 ) A = ( x p H0 / H0 sq ) if ( line freqs is  None  ) : x hat = ( A * H0 [ : , np . newaxis ] ) num = ( ( ( n tapers - 1 ) * ( A * A . conj ( ) ) . real ) * H0 sq ) den = ( np . sum ( ( np . abs ( ( x p [ : , tapers odd , : ] - x hat ) ) ** 2 ) , 1 ) + np . sum ( ( np . abs ( x p [ : , tapers even , : ] ) ** 2 ) , 1 ) ) den [ ( den == 0 ) ] = np . inf f stat = ( num / den ) indices = np . where ( ( f stat > threshold ) ) [ 1 ] rm freqs = freqs [ indices ] else : indices 1 = np . unique ( [ np . argmin ( np . abs ( ( freqs - lf ) ) ) for lf in line freqs ] ) notch widths /= 2.0 indices 2 = [ np . logical and ( ( freqs > ( lf - nw ) ) , ( freqs < ( lf + nw ) ) ) for ( lf , nw ) in zip ( line freqs , notch widths ) ] indices 2 = np . where ( np . any ( np . array ( indices 2 ) , axis = 0 ) ) [ 0 ] indices = np . unique ( np . r  [ ( indices 1 , indices 2 ) ] ) rm freqs = freqs [ indices ] fits = list ( ) for ind in indices : c = ( 2 * A [ ( 0 , ind ) ] ) fit = ( np . abs ( c ) * np . cos ( ( ( freqs [ ind ] * rads ) + np . angle ( c ) ) ) ) fits . append ( fit ) if ( len ( fits ) == 0 ) : datafit = 0.0 else : datafit = np . sum ( np . atleast 2d ( fits ) , axis = 0 ) return ( ( x - datafit ) , rm freqs )
def config option show ( context , data dict ) : return { 'success' :  False  }
def quota destroy all by project ( context , project id ) : return IMPL . quota destroy all by project ( context , project id )
def  mathdefault ( s ) : if rc Params  [ u' internal.classic mode' ] : return ( u'\\mathdefault{%s}' % s ) else : return ( u'{%s}' % s )
def populate tables ( db , prefix , tmp prefix , bounds ) : bbox = ( 'ST  Set SRID(ST  Make  Box 2D(ST  Make  Point (%.6f,  %.6f),  ST  Make  Point (%.6f,  %.6f)),  900913)' % bounds ) db . execute ( 'BEGIN' ) for table in ( 'point' , 'line' , 'roads' , 'polygon' ) : db . execute ( ( 'DELETE  FROM  %(prefix)s %(table)s  WHERE  ST  Intersects (way,  %(bbox)s)' % locals ( ) ) ) db . execute ( ( 'INSERT  INTO  %(prefix)s %(table)s\n                                            SELECT  *  FROM  %(tmp prefix)s %(table)s\n                                            WHERE  ST  Intersects (way,  %(bbox)s)' % locals ( ) ) ) db . execute ( 'COMMIT' )
@ requires sklearn def test gat plot nonsquared ( ) : gat =  get data ( test times = dict ( start = 0.0 ) ) gat . plot ( ) ax = gat . plot diagonal ( ) scores = ax . get children ( ) [ 1 ] . get lines ( ) [ 2 ] . get ydata ( ) assert equals ( len ( scores ) , len ( gat . estimators  ) )
def strategy saturation largest first ( G , colors ) : distinct colors = { v : set ( ) for v in G } for i in range ( len ( G ) ) : if ( i == 0 ) : node = max ( G , key = G . degree ) ( yield node ) for v in G [ node ] : distinct colors [ v ] . add ( 0 ) else : saturation = { v : len ( c ) for ( v , c ) in distinct colors . items ( ) if ( v not in colors ) } node = max ( saturation , key = ( lambda v : ( saturation [ v ] , G . degree ( v ) ) ) ) ( yield node ) color = colors [ node ] for v in G [ node ] : distinct colors [ v ] . add ( color )
def get linode id from name ( name ) : nodes =  query ( 'linode' , 'list' ) [ 'DATA' ] linode id = '' for node in nodes : if ( name == node [ 'LABEL' ] ) : linode id = node [ 'LINODEID' ] return linode id if ( not linode id ) : raise  Salt  Cloud  Not  Found  ( ' The   specified  name,  {0},  could  not  be  found.' . format ( name ) )
def S white simple ( x ) : if ( x . ndim == 1 ) : x = x [ : ,  None  ] return np . dot ( x . T , x )
def did you mean units ( s , all units , deprecated units , format decomposed ) : def fix deprecated ( x ) : if ( x in deprecated units ) : results = [ ( x + u'  (deprecated)' ) ] decomposed =  try decomposed ( all units [ x ] , format decomposed ) if ( decomposed is not  None  ) : results . append ( decomposed ) return results return ( x , ) return did you mean ( s , all units , fix = fix deprecated )
def  find matching indices ( tree , bin X , left mask , right mask ) : left index = np . searchsorted ( tree , ( bin X & left mask ) ) right index = np . searchsorted ( tree , ( bin X | right mask ) , side = 'right' ) return ( left index , right index )
def test cons list ( ) : entry = tokenize ( '(a  .  [])' ) [ 0 ] assert ( entry ==  Hy  List  ( [  Hy  Symbol  ( 'a' ) ] ) ) assert ( type ( entry ) ==  Hy  List  ) entry = tokenize ( '(a  .  ())' ) [ 0 ] assert ( entry ==  Hy  Expression  ( [  Hy  Symbol  ( 'a' ) ] ) ) assert ( type ( entry ) ==  Hy  Expression  ) entry = tokenize ( '(a  b  .  {})' ) [ 0 ] assert ( entry ==  Hy  Dict  ( [  Hy  Symbol  ( 'a' ) ,  Hy  Symbol  ( 'b' ) ] ) ) assert ( type ( entry ) ==  Hy  Dict  )
def  disconnect session ( session ) : session [ 'client' ] . auth . logout ( session [ 'key' ] )
@ pytest . mark . django db def test verify user empty email ( trans member ) : with pytest . raises (  Email  Address  .  Does  Not  Exist  ) :  Email  Address  . objects . get ( user = trans member ) assert ( trans member . email == '' ) with pytest . raises (  Validation  Error  ) : accounts . utils . verify user ( trans member ) with pytest . raises (  Email  Address  .  Does  Not  Exist  ) :  Email  Address  . objects . get ( user = trans member )
def get occupied streams ( realm ) : subs filter =  Subscription  . objects . filter ( active =  True  , user profile  realm = realm , user profile  is active =  True  ) . values ( 'recipient id' ) stream ids =  Recipient  . objects . filter ( type =  Recipient  . STREAM , id  in = subs filter ) . values ( 'type id' ) return  Stream  . objects . filter ( id  in = stream ids , realm = realm , deactivated =  False  )
def get New  Repository  ( ) : return  Fill  Repository  ( )
def total seconds ( td ) : return ( ( td . microseconds + ( ( td . seconds + ( ( td . days * 24 ) * 3600 ) ) * ( 10 ** 6 ) ) ) / ( 10 ** 6 ) )
def object list ( request , queryset , paginate by =  None  , page =  None  , allow empty =  True  , template name =  None  , template loader = loader , extra context =  None  , context processors =  None  , template object name = 'object' , mimetype =  None  ) : if ( extra context is  None  ) : extra context = { } queryset = queryset .  clone ( ) if paginate by : paginator =  Paginator  ( queryset , paginate by , allow empty first page = allow empty ) if ( not page ) : page = request . GET . get ( 'page' , 1 ) try : page number = int ( page ) except  Value  Error  : if ( page == 'last' ) : page number = paginator . num pages else : raise  Http 404 try : page obj = paginator . page ( page number ) except  Invalid  Page  : raise  Http 404 c =  Request  Context  ( request , { ( '%s list' % template object name ) : page obj . object list , 'paginator' : paginator , 'page obj' : page obj , 'is paginated' : page obj . has other pages ( ) , 'results per page' : paginator . per page , 'has next' : page obj . has next ( ) , 'has previous' : page obj . has previous ( ) , 'page' : page obj . number , 'next' : page obj . next page number ( ) , 'previous' : page obj . previous page number ( ) , 'first on page' : page obj . start index ( ) , 'last on page' : page obj . end index ( ) , 'pages' : paginator . num pages , 'hits' : paginator . count , 'page range' : paginator . page range } , context processors ) else : c =  Request  Context  ( request , { ( '%s list' % template object name ) : queryset , 'paginator' :  None  , 'page obj' :  None  , 'is paginated' :  False  } , context processors ) if ( ( not allow empty ) and ( len ( queryset ) == 0 ) ) : raise  Http 404 for ( key , value ) in extra context . items ( ) : if callable ( value ) : c [ key ] = value ( ) else : c [ key ] = value if ( not template name ) : model = queryset . model template name = ( '%s/%s list.html' % ( model .  meta . app label , model .  meta . object name . lower ( ) ) ) t = template loader . get template ( template name ) return  Http  Response  ( t . render ( c ) , mimetype = mimetype )
def load passphrase from file ( ) : vf path = os . path . expanduser ( kVF Passphrase  File  ) assert ( os . access ( vf path , os . F OK ) and os . access ( vf path , os . R OK ) ) , ( '%s  must  exist  and  be  readable' % vf path ) with open ( vf path ) as f : user data = f . read ( ) return user data . strip ( '\n' )
@ register . simple tag ( takes context =  True  ) def zinnia loop template ( context , default template ) : ( matching , context object ) = get context first matching object ( context , [ 'category' , 'tag' , 'author' , 'pattern' , 'year' , 'month' , 'week' , 'day' ] ) context positions = get context loop positions ( context ) templates = loop template list ( context positions , context object , matching , default template , ENTRY LOOP TEMPLATES ) return select template ( templates )
def convert params ( exception =  Value  Error  , error = 400 ) : request = cherrypy . serving . request types = request . handler . callable .   annotations   with cherrypy . HTTP Error  . handle ( exception , error ) : for key in set ( types ) . intersection ( request . params ) : request . params [ key ] = types [ key ] ( request . params [ key ] )
def  tensordot as dot ( a , b , axes , dot , batched ) : ( a , b ) = ( as tensor variable ( a ) , as tensor variable ( b ) ) if ( ( not numpy . isscalar ( axes ) ) and ( len ( axes ) != 2 ) ) : raise  Value  Error  ( ( ' Axes   should  be  an  integer  or  a  list/tuple  of  len  2  (%s  was  provided)' % str ( axes ) ) ) elif numpy . isscalar ( axes ) : axes = int ( axes ) for ( operand name , operand ) in ( ( 'a' , a ) , ( 'b' , b ) ) : if ( axes > operand . ndim ) : raise  Value  Error  ( ( 'axes  can  not  be  larger  than  the  dimension  of  %s  (%s.ndim=%i,  axes=%i)' % ( operand name , operand name , operand . ndim , axes ) ) ) if ( batched and ( axes == operand . ndim ) ) : raise  Value  Error  ( ( 'axes  to  sum  over  must  not  include  the  batch  axis  of  %s  (%s.ndim=%i,  axes=%i)' % ( operand name , operand name , operand . ndim , axes ) ) ) batch axes = ( 1 if batched else 0 ) a outaxes = slice ( 0 , ( a . ndim - axes ) ) b outaxes = slice ( ( batch axes + axes ) , b . ndim ) outshape = concatenate ( [ a . shape [ a outaxes ] , b . shape [ b outaxes ] ] ) outbcast = ( a . broadcastable [ a outaxes ] + b . broadcastable [ b outaxes ] ) outndim = len ( outbcast ) a shape = ( [ 1 ] * 2 ) b shape = ( [ 1 ] * 2 ) for i in xrange ( 0 , axes ) : a shape [ 1 ] *= a . shape [ ( - ( i + 1 ) ) ] b shape [ 0 ] *= b . shape [ ( batch axes + i ) ] for i in xrange ( 0 , ( ( a . ndim - axes ) - batch axes ) ) : a shape [ 0 ] *= a . shape [ ( batch axes + i ) ] for i in xrange ( 0 , ( ( b . ndim - axes ) - batch axes ) ) : b shape [ 1 ] *= b . shape [ ( - ( i + 1 ) ) ] if batched : a shape . insert ( 0 , a . shape [ 0 ] ) b shape . insert ( 0 , b . shape [ 0 ] ) a reshaped = a . reshape ( a shape ) b reshaped = b . reshape ( b shape ) out reshaped = dot ( a reshaped , b reshaped ) out = out reshaped . reshape ( outshape , outndim ) return patternbroadcast ( out , outbcast ) else : axes = [  pack ( axes  ) for axes  in axes ] if ( len ( axes [ 0 ] ) != len ( axes [ 1 ] ) ) : raise  Value  Error  ( ' Axes   elements  must  have  the  same  length.' ) for ( i , ( operand name , operand ) ) in enumerate ( ( ( 'a' , a ) , ( 'b' , b ) ) ) : if ( len ( axes [ i ] ) > operand . ndim ) : raise  Value  Error  ( ( 'axes[%i]  should  be  array like  with  length  less  than  the  dimensions  of  %s  (%s.ndim=%i,  len(axes[0])=%i).' % ( i , operand name , operand name , operand . ndim , len ( axes [ i ] ) ) ) ) if ( ( len ( axes [ i ] ) > 0 ) and ( numpy . max ( axes [ i ] ) >= operand . ndim ) ) : raise  Value  Error  ( ( 'axes[%i]  contains  dimensions  greater  than  or  equal  to  %s.ndim  (%s.ndim=%i,  max(axes[0])=%i).' % ( i , operand name , operand name , operand . ndim , numpy . max ( numpy . array ( axes [ i ] ) ) ) ) ) if ( batched and ( 0 in axes [ i ] ) ) : raise  Value  Error  ( ( 'axes  to  sum  over  must  not  contain  the  batch  axis  (axes[%i]=%s)' % ( i , axes [ i ] ) ) ) batch axes = ( [ 0 ] if batched else [ ] ) other axes = [ [ x for x in xrange ( operand . ndim ) if ( ( x not in axes [ i ] ) and ( x not in batch axes ) ) ] for ( i , operand ) in enumerate ( ( a , b ) ) ] a shuffled = a . dimshuffle ( ( ( batch axes + other axes [ 0 ] ) + axes [ 0 ] ) ) b shuffled = b . dimshuffle ( ( ( batch axes + axes [ 1 ] ) + other axes [ 1 ] ) ) return  tensordot as dot ( a shuffled , b shuffled , len ( axes [ 0 ] ) , dot = dot , batched = batched )
def layer js ( ) : if ( settings . get security map ( ) and ( not s3 has role ( MAP ADMIN ) ) ) : auth . permission . fail ( ) tablename = ( '%s %s' % ( module , resourcename ) ) s3db . table ( tablename ) type = 'JS' LAYERS = T ( ( TYPE LAYERS FMT % type ) ) ADD NEW LAYER = T ( ( ADD NEW TYPE LAYER FMT % type ) ) EDIT LAYER = T ( ( EDIT TYPE LAYER FMT % type ) ) LIST LAYERS = T ( ( LIST TYPE LAYERS FMT % type ) ) NO LAYERS = T ( ( NO TYPE LAYERS FMT % type ) ) s3 . crud strings [ tablename ] =  Storage  ( label create = ADD LAYER , title display = LAYER DETAILS , title list = LAYERS , title update = EDIT LAYER , label list button = LIST LAYERS , label delete button = DELETE LAYER , msg record created = LAYER ADDED , msg record modified = LAYER UPDATED , msg record deleted = LAYER DELETED , msg list empty = NO LAYERS ) def prep ( r ) : if r . interactive : if ( r . component name == 'config' ) : ltable = s3db . gis layer config if ( r . method != 'update' ) : table = r . table query = ( ( ltable . layer id == table . layer id ) & ( table . id == r . id ) ) rows = db ( query ) . select ( ltable . config id ) ltable . config id . requires = IS ONE OF ( db , 'gis config.id' , '%(name)s' , not filterby = 'config id' , not filter opts = [ row . config id for row in rows ] ) return  True  s3 . prep = prep def postp ( r , output ) : if ( r . interactive and ( r . method != 'import' ) ) : if ( not r . component ) : inject enable ( output ) return output s3 . postp = postp output = s3 rest controller ( rheader = s3db . gis rheader ) return output
def  safe split ( estimator , X , y , indices , train indices =  None  ) : if ( hasattr ( estimator , 'kernel' ) and callable ( estimator . kernel ) and ( not isinstance ( estimator . kernel , GP Kernel  ) ) ) : raise  Value  Error  ( ' Cannot   use  a  custom  kernel  function.   Precompute   the  kernel  matrix  instead.' ) if ( not hasattr ( X , 'shape' ) ) : if getattr ( estimator , ' pairwise' ,  False  ) : raise  Value  Error  ( ' Precomputed   kernels  or  affinity  matrices  have  to  be  passed  as  arrays  or  sparse  matrices.' ) X subset = [ X [ idx ] for idx in indices ] elif getattr ( estimator , ' pairwise' ,  False  ) : if ( X . shape [ 0 ] != X . shape [ 1 ] ) : raise  Value  Error  ( 'X  should  be  a  square  kernel  matrix' ) if ( train indices is  None  ) : X subset = X [ np . ix  ( indices , indices ) ] else : X subset = X [ np . ix  ( indices , train indices ) ] else : X subset = safe indexing ( X , indices ) if ( y is not  None  ) : y subset = safe indexing ( y , indices ) else : y subset =  None  return ( X subset , y subset )
def add status query managers ( sender , ** kwargs ) : if ( not issubclass ( sender ,  Status  Model  ) ) : return for ( value , display ) in getattr ( sender , u'STATUS' , ( ) ) : try : sender .  meta . get field ( value ) raise  Improperly  Configured  ( ( u" Status  Model :   Model   '%s'  has  a  field  named  '%s'  which  conflicts  with  a  status  of  the  same  name." % ( sender .   name   , value ) ) ) except  Field  Does  Not  Exist  : pass sender . add to class ( value ,  Query  Manager  ( status = value ) )
def call url ( url , view kwargs =  None  ) : ( func name , func data ) = app . url map . bind ( '' ) . match ( url ) if ( view kwargs is not  None  ) : func data . update ( view kwargs ) view function = view functions [ func name ] rv = view function ( ** func data ) ( rv ,   ,   ,   ) = unpack ( rv ) if ( isinstance ( rv , werkzeug . wrappers .  Base  Response  ) and ( rv . status code in REDIRECT CODES ) ) : redirect url = rv . headers [ ' Location ' ] return call url ( redirect url ) return rv
def test consistency GPU parallel ( ) : if ( not cuda available ) : raise  Skip  Test  ( ' Optional   package  cuda  not  available' ) if ( config . mode == 'FAST COMPILE' ) : mode = 'FAST RUN' else : mode = config . mode seed = 12345 n samples = 5 n streams = 12 n substreams = 7 samples = [ ] curr rstate = numpy . array ( ( [ seed ] * 6 ) , dtype = 'int32' ) for i in range ( n streams ) : stream samples = [ ] rstate = [ curr rstate . copy ( ) ] for j in range ( 1 , n substreams ) : rstate . append ( rng mrg . ff 2p72 ( rstate [ ( - 1 ) ] ) ) rstate = numpy . asarray ( rstate ) . flatten ( ) tmp float buf = numpy . frombuffer ( rstate . data , dtype = 'float32' ) rstate = float32 shared constructor ( tmp float buf ) ( new rstate , sample ) = rng mrg . GPU mrg uniform . new ( rstate , ndim =  None  , dtype = 'float32' , size = ( n substreams , ) ) rstate . default update = new rstate sample . rstate = rstate sample . update = ( rstate , new rstate ) cpu sample = tensor . as tensor variable ( sample ) f = theano . function ( [ ] , cpu sample , mode = mode ) for k in range ( n samples ) : s = f ( ) stream samples . append ( s ) samples . append ( numpy . array ( stream samples ) . T . flatten ( ) ) curr rstate = rng mrg . ff 2p134 ( curr rstate ) samples = numpy . array ( samples ) . flatten ( ) assert numpy . allclose ( samples , java samples )
def get subclasses ( c ) : return ( c .   subclasses   ( ) + sum ( map ( get subclasses , c .   subclasses   ( ) ) , [ ] ) )
def function no Args  ( ) : return
def blacklist check ( path ) : ( head , tests dir ) = os . path . split ( path . dirname ) if ( tests dir != u'tests' ) : return  True  ( head , top module ) = os . path . split ( head ) return ( path . purebasename in IGNORED TESTS . get ( top module , [ ] ) )
def detect ( stream ) : try : openpyxl . reader . excel . load workbook ( stream ) return  True  except openpyxl . shared . exc .  Invalid  File  Exception  : pass
def pick plugin ( config , default , plugins , question , ifaces ) : if ( default is not  None  ) : filtered = plugins . filter ( ( lambda p ep : ( p ep . name == default ) ) ) else : if config . noninteractive mode : raise errors .  Missing  Commandline  Flag  ( " Missing   command  line  flags.   For   non-interactive  execution,  you  will  need  to  specify  a  plugin  on  the  command  line.     Run   with  '--help  plugins'  to  see  a  list  of  options,  and  see  https://eff.org/letsencrypt-plugins  for  more  detail  on  what  the  plugins  do  and  how  to  use  them." ) filtered = plugins . visible ( ) . ifaces ( ifaces ) filtered . init ( config ) verified = filtered . verify ( ifaces ) verified . prepare ( ) prepared = verified . available ( ) if ( len ( prepared ) > 1 ) : logger . debug ( ' Multiple   candidate  plugins:  %s' , prepared ) plugin ep = choose plugin ( list ( six . itervalues ( prepared ) ) , question ) if ( plugin ep is  None  ) : return  None  else : return plugin ep . init ( ) elif ( len ( prepared ) == 1 ) : plugin ep = list ( prepared . values ( ) ) [ 0 ] logger . debug ( ' Single   candidate  plugin:  %s' , plugin ep ) if plugin ep . misconfigured : return  None  return plugin ep . init ( ) else : logger . debug ( ' No   candidate  plugin' ) return  None
@ decorator . decorator def outplace ( f , clip , * a , ** k ) : newclip = clip . copy ( ) f ( newclip , * a , ** k ) return newclip
def escape ( inp ) : def conv ( obj ) : ' Convert   obj.' if isinstance ( obj , list ) : rv = as unicode ( ( ( '[' + ',' . join ( ( conv ( o ) for o in obj ) ) ) + ']' ) ) elif isinstance ( obj , dict ) : rv = as unicode ( ( ( '{' + ',' . join ( [ ( '%s:%s' % ( conv ( key ) , conv ( value ) ) ) for ( key , value ) in obj . iteritems ( ) ] ) ) + '}' ) ) else : rv = ( as unicode ( '"%s"' ) % as unicode ( obj ) . replace ( '"' , '\\"' ) ) return rv return conv ( inp )
@ register ( u'yank-last-arg' ) def yank last arg ( event ) : n = ( event . arg if event . arg present else  None  ) event . current buffer . yank last arg ( n )
def normalize ( pattern ) : result = [ ] non capturing groups = [ ] consume next =  True  pattern iter = next char ( iter ( pattern ) ) num args = 0 try : ( ch , escaped ) = next ( pattern iter ) except  Stop  Iteration  : return [ ( '' , [ ] ) ] try : while  True  : if escaped : result . append ( ch ) elif ( ch == '.' ) : result . append ( '.' ) elif ( ch == '|' ) : raise  Not  Implemented  Error  ( ' Awaiting    Implementation ' ) elif ( ch == '^' ) : pass elif ( ch == '$' ) : break elif ( ch == ')' ) : start = non capturing groups . pop ( ) inner =  Non  Capture  ( result [ start : ] ) result = ( result [ : start ] + [ inner ] ) elif ( ch == '[' ) : ( ch , escaped ) = next ( pattern iter ) result . append ( ch ) ( ch , escaped ) = next ( pattern iter ) while ( escaped or ( ch != ']' ) ) : ( ch , escaped ) = next ( pattern iter ) elif ( ch == '(' ) : ( ch , escaped ) = next ( pattern iter ) if ( ( ch != '?' ) or escaped ) : name = ( ' %d' % num args ) num args += 1 result . append (  Group  ( ( ( '%%(%s)s' % name ) , name ) ) ) walk to end ( ch , pattern iter ) else : ( ch , escaped ) = next ( pattern iter ) if ( ch in '!=<' ) : walk to end ( ch , pattern iter ) elif ( ch in 'i Lmsu #' ) : warnings . warn ( ( ' Using   (?%s)  in  url()  patterns  is  deprecated.' % ch ) ,  Removed  In  Django 21 Warning  ) walk to end ( ch , pattern iter ) elif ( ch == ':' ) : non capturing groups . append ( len ( result ) ) elif ( ch != 'P' ) : raise  Value  Error  ( ( " Non -reversible  reg-exp  portion:  '(?%s'" % ch ) ) else : ( ch , escaped ) = next ( pattern iter ) if ( ch not in ( '<' , '=' ) ) : raise  Value  Error  ( ( " Non -reversible  reg-exp  portion:  '(?P%s'" % ch ) ) if ( ch == '<' ) : terminal char = '>' else : terminal char = ')' name = [ ] ( ch , escaped ) = next ( pattern iter ) while ( ch != terminal char ) : name . append ( ch ) ( ch , escaped ) = next ( pattern iter ) param = '' . join ( name ) if ( terminal char != ')' ) : result . append (  Group  ( ( ( '%%(%s)s' % param ) , param ) ) ) walk to end ( ch , pattern iter ) else : result . append (  Group  ( ( ( '%%(%s)s' % param ) ,  None  ) ) ) elif ( ch in '*?+{' ) : ( count , ch ) = get quantifier ( ch , pattern iter ) if ch : consume next =  False  if ( count == 0 ) : if contains ( result [ ( - 1 ) ] ,  Group  ) : result [ ( - 1 ) ] =  Choice  ( [  None  , result [ ( - 1 ) ] ] ) else : result . pop ( ) elif ( count > 1 ) : result . extend ( ( [ result [ ( - 1 ) ] ] * ( count - 1 ) ) ) else : result . append ( ch ) if consume next : ( ch , escaped ) = next ( pattern iter ) else : consume next =  True  except  Stop  Iteration  : pass except  Not  Implemented  Error  : return [ ( '' , [ ] ) ] return list ( zip ( * flatten result ( result ) ) )
def  ssh args ( ssh bin , address , ec2 key pair file ) : if ( ec2 key pair file is  None  ) : raise  Value  Error  ( 'SSH  key  file  path  is   None ' ) return ( ssh bin + [ '-i' , ec2 key pair file , '-o' , ' Strict  Host  Key  Checking =no' , '-o' , ' User  Known  Hosts  File =/dev/null' , ( 'hadoop@%s' % ( address , ) ) ] )
def make twilio request ( method , uri , ** kwargs ) : headers = kwargs . get ( 'headers' , { } ) user agent = ( 'twilio-python/%s  ( Python   %s)' % (   version   , platform . python version ( ) ) ) headers [ ' User - Agent ' ] = user agent headers [ ' Accept - Charset ' ] = 'utf-8' if ( ( method == 'POST' ) and ( ' Content - Type ' not in headers ) ) : headers [ ' Content - Type ' ] = 'application/x-www-form-urlencoded' kwargs [ 'headers' ] = headers if ( ' Accept ' not in headers ) : headers [ ' Accept ' ] = 'application/json' if kwargs . pop ( 'use json extension' ,  False  ) : uri += '.json' resp = make request ( method , uri , ** kwargs ) if ( not resp . ok ) : try : error = json . loads ( resp . content ) code = error [ 'code' ] message = error [ 'message' ] except : code =  None  message = resp . content raise  Twilio  Rest  Exception  ( status = resp . status code , method = method , uri = resp . url , msg = message , code = code ) return resp
def serve ( request , path , document root =  None  , show indexes =  False  ) : path = posixpath . normpath ( unquote ( path ) ) path = path . lstrip ( u'/' ) newpath = u'' for part in path . split ( u'/' ) : if ( not part ) : continue ( drive , part ) = os . path . splitdrive ( part ) ( head , part ) = os . path . split ( part ) if ( part in ( os . curdir , os . pardir ) ) : continue newpath = os . path . join ( newpath , part ) . replace ( u'\\' , u'/' ) if ( newpath and ( path != newpath ) ) : return  Http  Response  Redirect  ( newpath ) fullpath = os . path . join ( document root , newpath ) if os . path . isdir ( fullpath ) : if show indexes : return directory index ( newpath , fullpath ) raise  Http 404 (   ( u' Directory   indexes  are  not  allowed  here.' ) ) if ( not os . path . exists ( fullpath ) ) : raise  Http 404 ( (   ( u'"%(path)s"  does  not  exist' ) % { u'path' : fullpath } ) ) statobj = os . stat ( fullpath ) if ( not was modified since ( request . META . get ( u'HTTP IF MODIFIED SINCE' ) , statobj . st mtime , statobj . st size ) ) : return  Http  Response  Not  Modified  ( ) ( content type , encoding ) = mimetypes . guess type ( fullpath ) content type = ( content type or u'application/octet-stream' ) f = open ( fullpath , u'rb' ) response =  Compatible  Streaming  Http  Response  ( iter ( ( lambda : f . read ( STREAM CHUNK SIZE ) ) , '' ) , content type = content type ) response [ u' Last - Modified ' ] = http date ( statobj . st mtime ) if stat . S ISREG ( statobj . st mode ) : response [ u' Content - Length ' ] = statobj . st size if encoding : response [ u' Content - Encoding ' ] = encoding return response
def voronoi cells ( G , center nodes , weight = 'weight' ) : paths = nx . multi source dijkstra path ( G , center nodes , weight = weight ) nearest = { v : p [ 0 ] for ( v , p ) in paths . items ( ) } cells = groups ( nearest ) unreachable = ( set ( G ) - set ( nearest ) ) if unreachable : cells [ 'unreachable' ] = unreachable return cells
def get test data ( nb train = 1000 , nb test = 500 , input shape = ( 10 , ) , output shape = ( 2 , ) , classification =  True  , nb class = 2 ) : nb sample = ( nb train + nb test ) if classification : y = np . random . randint ( 0 , nb class , size = ( nb sample , ) ) X = np . zeros ( ( ( nb sample , ) + input shape ) ) for i in range ( nb sample ) : X [ i ] = np . random . normal ( loc = y [ i ] , scale = 0.7 , size = input shape ) else : y loc = np . random . random ( ( nb sample , ) ) X = np . zeros ( ( ( nb sample , ) + input shape ) ) y = np . zeros ( ( ( nb sample , ) + output shape ) ) for i in range ( nb sample ) : X [ i ] = np . random . normal ( loc = y loc [ i ] , scale = 0.7 , size = input shape ) y [ i ] = np . random . normal ( loc = y loc [ i ] , scale = 0.7 , size = output shape ) return ( ( X [ : nb train ] , y [ : nb train ] ) , ( X [ nb train : ] , y [ nb train : ] ) )
def font priority ( font ) : style normal = ( font [ u'font-style' ] == u'normal' ) width normal = ( font [ u'font-stretch' ] == u'normal' ) weight normal = ( font [ u'font-weight' ] == u'normal' ) num normal = sum ( filter (  None  , ( style normal , width normal , weight normal ) ) ) subfamily name = ( font [ u'wws subfamily name' ] or font [ u'preferred subfamily name' ] or font [ u'subfamily name' ] ) if ( ( num normal == 3 ) and ( subfamily name == u' Regular ' ) ) : return 0 if ( num normal == 3 ) : return 1 if ( subfamily name == u' Regular ' ) : return 2 return ( 3 + ( 3 - num normal ) )
def load Icon  ( stock item id ) : stock item = getattr ( gtk , stock item id ) local icon = os . path . join ( GUI DATA PATH , 'icons' , '16' , ( '%s.png' % stock item ) ) if os . path . exists ( local icon ) : im = gtk .  Image  ( ) im . set from file ( local icon ) im . show ( ) return im . get pixbuf ( ) else : icon theme = gtk .  Icon  Theme  ( ) try : icon = icon theme . load icon ( stock item , 16 , ( ) ) except : icon = load Image  ( 'missing-image.png' ) . get pixbuf ( ) return icon
def iter Services  ( xrd tree ) : xrd = get Yadis XRD ( xrd tree ) return prio Sort  ( xrd . findall ( service tag ) )
def test nearmiss wrong version ( ) : version = 1000 nm3 =  Near  Miss  ( version = version , random state = RND SEED ) assert raises (  Value  Error  , nm3 . fit sample , X , Y )
@ require global staff @ require POST def enable certificate generation ( request , course id =  None  ) : course key =  Course  Key  . from string ( course id ) is enabled = ( request . POST . get ( 'certificates-enabled' , 'false' ) == 'true' ) certs api . set cert generation enabled ( course key , is enabled ) return redirect (  instructor dash url ( course key , section = 'certificates' ) )
def decompose matrix ( matrix ) : M = numpy . array ( matrix , dtype = numpy . float64 , copy =  True  ) . T if ( abs ( M [ ( 3 , 3 ) ] ) <  EPS ) : raise  Value  Error  ( 'M[3,  3]  is  zero' ) M /= M [ ( 3 , 3 ) ] P = M . copy ( ) P [ : , 3 ] = ( 0.0 , 0.0 , 0.0 , 1.0 ) if ( not numpy . linalg . det ( P ) ) : raise  Value  Error  ( 'matrix  is  singular' ) scale = numpy . zeros ( ( 3 , ) ) shear = [ 0.0 , 0.0 , 0.0 ] angles = [ 0.0 , 0.0 , 0.0 ] if any ( ( abs ( M [ : 3 , 3 ] ) >  EPS ) ) : perspective = numpy . dot ( M [ : , 3 ] , numpy . linalg . inv ( P . T ) ) M [ : , 3 ] = ( 0.0 , 0.0 , 0.0 , 1.0 ) else : perspective = numpy . array ( [ 0.0 , 0.0 , 0.0 , 1.0 ] ) translate = M [ 3 , : 3 ] . copy ( ) M [ 3 , : 3 ] = 0.0 row = M [ : 3 , : 3 ] . copy ( ) scale [ 0 ] = vector norm ( row [ 0 ] ) row [ 0 ] /= scale [ 0 ] shear [ 0 ] = numpy . dot ( row [ 0 ] , row [ 1 ] ) row [ 1 ] -= ( row [ 0 ] * shear [ 0 ] ) scale [ 1 ] = vector norm ( row [ 1 ] ) row [ 1 ] /= scale [ 1 ] shear [ 0 ] /= scale [ 1 ] shear [ 1 ] = numpy . dot ( row [ 0 ] , row [ 2 ] ) row [ 2 ] -= ( row [ 0 ] * shear [ 1 ] ) shear [ 2 ] = numpy . dot ( row [ 1 ] , row [ 2 ] ) row [ 2 ] -= ( row [ 1 ] * shear [ 2 ] ) scale [ 2 ] = vector norm ( row [ 2 ] ) row [ 2 ] /= scale [ 2 ] shear [ 1 : ] /= scale [ 2 ] if ( numpy . dot ( row [ 0 ] , numpy . cross ( row [ 1 ] , row [ 2 ] ) ) < 0 ) : numpy . negative ( scale , scale ) numpy . negative ( row , row ) angles [ 1 ] = math . asin ( ( - row [ ( 0 , 2 ) ] ) ) if math . cos ( angles [ 1 ] ) : angles [ 0 ] = math . atan2 ( row [ ( 1 , 2 ) ] , row [ ( 2 , 2 ) ] ) angles [ 2 ] = math . atan2 ( row [ ( 0 , 1 ) ] , row [ ( 0 , 0 ) ] ) else : angles [ 0 ] = math . atan2 ( ( - row [ ( 2 , 1 ) ] ) , row [ ( 1 , 1 ) ] ) angles [ 2 ] = 0.0 return ( scale , shear , angles , translate , perspective )
def patch os ( ) : patch module ( 'os' )
@ task @ cmdopts ( [ ( 'src=' , 's' , ' Url   to  source' ) , ( 'rev=' , 'r' , 'HG  revision' ) , ( 'clean' , 'c' , ' Delete   old  source  folder' ) ] ) def get source ( options ) : if options . rev : options . url = ( 'https://bitbucket.org/spoob/pyload/get/%s.zip' % options . rev ) pyload = path ( 'pyload' ) if ( len ( pyload . listdir ( ) ) and ( not options . clean ) ) : return elif pyload . exists ( ) : pyload . rmtree ( ) urlretrieve ( options . src , 'pyload src.zip' ) zip =  Zip  File  ( 'pyload src.zip' ) zip . extractall ( ) path ( 'pyload src.zip' ) . remove ( ) folder = [ x for x in path ( '.' ) . dirs ( ) if x . name . startswith ( 'spoob-pyload-' ) ] [ 0 ] folder . move ( pyload ) change mode ( pyload , 420 ) change mode ( pyload , 493 , folder =  True  ) for file in pyload . files ( ) : if file . name . endswith ( '.py' ) : file . chmod ( 493 ) ( pyload / '.hgtags' ) . remove ( ) ( pyload / '.hgignore' ) . remove ( ) f = open ( ( pyload / '  init  .py' ) , 'wb' ) f . close ( )
def set special ( user , special , cmd ) : lst = list tab ( user ) for cron in lst [ 'special' ] : if ( ( special == cron [ 'spec' ] ) and ( cmd == cron [ 'cmd' ] ) ) : return 'present' spec = { 'spec' : special , 'cmd' : cmd } lst [ 'special' ] . append ( spec ) comdat =  write cron lines ( user ,  render tab ( lst ) ) if comdat [ 'retcode' ] : return comdat [ 'stderr' ] return 'new'
def file hash ( load , fnd ) : gitfs = salt . utils . gitfs .  Git FS (   opts   ) gitfs . init remotes (   opts   [ 'gitfs remotes' ] , PER REMOTE OVERRIDES , PER REMOTE ONLY ) return gitfs . file hash ( load , fnd )
def escape ( s ) : if ( s is  None  ) : return '' assert isinstance ( s , basestring ) , ( 'expected  %s  but  got  %s;  value=%s' % ( basestring , type ( s ) , s ) ) s = s . replace ( '\\' , '\\\\' ) s = s . replace ( '\n' , '\\n' ) s = s . replace ( ' DCTB ' , '\\t' ) s = s . replace ( ',' , ' DCTB ' ) return s
@ requires application ( ) def test context properties ( ) : a = use app ( ) if ( a . backend name . lower ( ) == 'pyglet' ) : return if ( a . backend name . lower ( ) == 'osmesa' ) : return configs = [ dict ( samples = 4 ) , dict ( stencil size = 8 ) , dict ( samples = 4 , stencil size = 8 ) ] if ( a . backend name . lower ( ) != 'glfw' ) : configs . append ( dict ( double buffer =  False  , samples = 4 ) ) configs . append ( dict ( double buffer =  False  ) ) else : assert raises (  Runtime  Error  ,  Canvas  , app = a , config = dict ( double buffer =  False  ) ) if ( ( a . backend name . lower ( ) == 'sdl2' ) and ( os . getenv ( 'TRAVIS' ) == 'true' ) ) : raise  Skip  Test  ( ' Travis   SDL  cannot  set  context' ) for config in configs : n items = len ( config ) with  Canvas  ( config = config ) : if ( 'true' in ( os . getenv ( 'TRAVIS' , '' ) , os . getenv ( 'APPVEYOR' , '' ) . lower ( ) ) ) : props = config else : props = get gl configuration ( ) assert equal ( len ( config ) , n items ) for ( key , val ) in config . items ( ) : if ( key == 'samples' ) : iswx = ( a . backend name . lower ( ) == 'wx' ) if ( not ( sys . platform . startswith ( 'win' ) or iswx ) ) : assert equal ( val , props [ key ] , key ) assert raises (  Type  Error  ,  Canvas  , config = 'foo' ) assert raises (  Key  Error  ,  Canvas  , config = dict ( foo =  True  ) ) assert raises (  Type  Error  ,  Canvas  , config = dict ( double buffer = 'foo' ) )
def es delete cmd ( index , noinput =  False  , log = log ) : try : indexes = [ name for ( name , count ) in get indexes ( ) ] except ES EXCEPTIONS : log . error ( ' Your   elasticsearch  process  is  not  running  or  ES URLS  is  set  wrong  in  your  settings local.py  file.' ) return if ( index not in indexes ) : log . error ( ' Index   "%s"  is  not  a  valid  index.' , index ) return if ( ( index in all read indexes ( ) ) and ( not noinput ) ) : ret = raw input ( ( '"%s"  is  a  read  index.   Are   you  sure  you  want  to  delete  it?  (yes/no)  ' % index ) ) if ( ret != 'yes' ) : log . info ( ' Not   deleting  the  index.' ) return log . info ( ' Deleting   index  "%s"...' , index ) delete index ( index ) log . info ( ' Done !' )
def get pending computer name ( ) : current = get computer name ( ) pending =   salt   [ 'reg.read value' ] ( 'HKLM' , 'SYSTEM\\ Current  Control  Set \\ Services \\ Tcpip \\ Parameters ' , 'NV   Hostname ' ) [ 'vdata' ] if pending : return ( pending if ( pending != current ) else  None  ) return  False
def is empty ( G ) : return ( not any ( G . adj . values ( ) ) )
def p statement assign ( t ) : names [ t [ 1 ] ] = t [ 3 ]
def  is task visible ( context , task ) : if context . is admin : return  True  if ( task [ 'owner' ] is  None  ) : return  True  if ( context . owner is not  None  ) : if ( context . owner == task [ 'owner' ] ) : return  True  return  False
def group Flip  Vert  ( flip List  , y Reflect  = 0 ) : if ( type ( flip List  ) != list ) : flip List  = [ flip List  ] for item in flip List  : if ( type ( item ) in ( list , numpy . ndarray ) ) : if ( ( type ( item [ 0 ] ) in ( list , numpy . ndarray ) ) and ( len ( item [ 0 ] ) == 2 ) ) : for i in range ( len ( item ) ) : item [ i ] [ 1 ] = ( ( 2 * y Reflect  ) - item [ i ] [ 1 ] ) else : msg = ' Cannot   vert-flip  elements  in  "%s",  type=%s' raise  Value  Error  ( ( msg % ( str ( item ) , type ( item [ 0 ] ) ) ) ) elif ( type ( item ) in immutables ) : raise  Value  Error  ( ( ' Cannot   change  immutable  item  "%s"' % str ( item ) ) ) if hasattr ( item , 'set Pos ' ) : item . set Pos  ( [ 1 , ( - 1 ) ] , '*' ) item . set Pos  ( [ 0 , ( 2 * y Reflect  ) ] , '+' ) elif hasattr ( item , 'pos' ) : item . pos [ 1 ] *= ( - 1 ) item . pos [ 1 ] += ( 2 * y Reflect  ) if hasattr ( item , 'set Flip  Vert ' ) : item . set Flip  Vert  ( ( not item . flip Vert  ) ) elif hasattr ( item , 'vertices' ) : try : v = ( item . vertices * [ 1 , ( - 1 ) ] ) except  Exception  : v = [ [ item . vertices [ i ] [ 0 ] , ( ( - 1 ) * item . vertices [ i ] [ 1 ] ) ] for i in range ( len ( item . vertices ) ) ] item . set Vertices  ( v ) if ( hasattr ( item , 'set Ori ' ) and item . ori ) : item . set Ori  ( ( - 1 ) , '*' ) item .  need Vertex  Update  =  True
def merge with ( func , * dicts , ** kwargs ) : if ( ( len ( dicts ) == 1 ) and ( not isinstance ( dicts [ 0 ] , dict ) ) ) : dicts = dicts [ 0 ] factory =  get factory ( merge with , kwargs ) result = factory ( ) for d in dicts : for ( k , v ) in iteritems ( d ) : if ( k not in result ) : result [ k ] = [ v ] else : result [ k ] . append ( v ) return valmap ( func , result , factory )
def remove Endpoints  ( pixel Table  , layer Extrusion  Width  , paths , removed Endpoints  , around Width  ) : for removed Endpoint  Index  in xrange ( ( len ( removed Endpoints  ) - 1 ) , ( - 1 ) , ( - 1 ) ) : removed Endpoint  = removed Endpoints  [ removed Endpoint  Index  ] removed Endpoint  Point  = removed Endpoint  . point if is Point  Added  Around  Closest  ( pixel Table  , layer Extrusion  Width  , paths , removed Endpoint  Point  , around Width  ) : removed Endpoints  . remove ( removed Endpoint  )
def text List  To  Colors  Simple  ( names ) : u Names  = list ( set ( names ) ) u Names  . sort ( ) text To  Color  = [ u Names  . index ( n ) for n in names ] text To  Color  = np . array ( text To  Color  ) text To  Color  = ( ( 255 * ( text To  Color  - text To  Color  . min ( ) ) ) / ( text To  Color  . max ( ) - text To  Color  . min ( ) ) ) textmaps = generate Color  Map  ( ) colors = [ textmaps [ int ( c ) ] for c in text To  Color  ] return colors
def get unique variable ( name ) : candidates = tf . get collection ( tf .  Graph  Keys  . GLOBAL VARIABLES , name ) if ( not candidates ) : raise  Value  Error  ( ( ' Couldnt   find  variable  %s' % name ) ) for candidate in candidates : if ( candidate . op . name == name ) : return candidate raise  Value  Error  ( ' Variable   %s  does  not  uniquely  identify  a  variable' , name )
def multidict to dict ( multidict ) : if config . AUTO COLLAPSE MULTI KEYS : d = dict ( multidict . lists ( ) ) for ( key , value ) in d . items ( ) : if ( len ( value ) == 1 ) : d [ key ] = value [ 0 ] return d else : return multidict . to dict ( )
@ register . filter def display url ( url ) : url = force bytes ( url , errors = 'replace' ) return urllib . unquote ( url ) . decode ( 'utf-8' , errors = 'replace' )
def  Command  Line  ( args =  None  , arglist =  None  ) : help text = '\n Commands :\n\ntrain  -  give  size  of  training  set  to  use,  as  argument\npredict  -  give  input  sequence  as  argument  (or  specify  inputs  via  --from-file  <filename>)\n\n' parser = argparse .  Argument  Parser  ( description = help text , formatter class = argparse .  Raw  Text  Help  Formatter  ) parser . add argument ( 'cmd' , help = 'command' ) parser . add argument ( 'cmd input' , nargs = '*' , help = 'input  to  command' ) parser . add argument ( '-v' , '--verbose' , nargs = 0 , help = 'increase  output  verbosity  (add  more  -v  to  increase  versbosity)' , action = V Action  , dest = 'verbose' ) parser . add argument ( '-m' , '--model' , help = 'seq2seq  model  name:  either  embedding rnn  (default)  or  embedding attention' , default =  None  ) parser . add argument ( '-r' , '--learning-rate' , type = float , help = 'learning  rate  (default  0.0001)' , default = 0.0001 ) parser . add argument ( '-e' , '--epochs' , type = int , help = 'number  of  trainig  epochs' , default = 10 ) parser . add argument ( '-i' , '--input-weights' , type = str , help = 'tflearn  file  with  network  weights  to  load' , default =  None  ) parser . add argument ( '-o' , '--output-weights' , type = str , help = 'new  tflearn  file  where  network  weights  are  to  be  saved' , default =  None  ) parser . add argument ( '-p' , '--pattern-name' , type = str , help = 'name  of  pattern  to  use  for  sequence' , default =  None  ) parser . add argument ( '-n' , '--name' , type = str , help = 'name  of  model,  used  when  generating  default  weights  filenames' , default =  None  ) parser . add argument ( '--in-len' , type = int , help = 'input  sequence  length  (default  10)' , default =  None  ) parser . add argument ( '--out-len' , type = int , help = 'output  sequence  length  (default  10)' , default =  None  ) parser . add argument ( '--from-file' , type = str , help = 'name  of  file  to  take  input  data  sequences  from  (json  format)' , default =  None  ) parser . add argument ( '--iter-num' , type = int , help = 'training  iteration  number;  specify  instead  of  input-  or  output-weights  to  use  generated  filenames' , default =  None  ) parser . add argument ( '--data-dir' , help = 'directory  to  use  for  storing  checkpoints  (also  used  when  generating  default  weights  filenames)' , default =  None  ) parser . add argument ( '-L' , '--num-layers' , type = int , help = 'number  of  RNN  layers  to  use  in  the  model  (default  1)' , default = 1 ) parser . add argument ( '--cell-size' , type = int , help = 'size  of  RNN  cell  to  use  (default  32)' , default = 32 ) parser . add argument ( '--cell-type' , type = str , help = 'type  of  RNN  cell  to  use  (default   Basic LSTM Cell )' , default = ' Basic LSTM Cell ' ) parser . add argument ( '--embedding-size' , type = int , help = 'size  of  embedding  to  use  (default  20)' , default = 20 ) parser . add argument ( '--tensorboard-verbose' , type = int , help = 'tensorboard  verbosity  level  (default  0)' , default = 0 ) if ( not args ) : args = parser . parse args ( arglist ) if ( args . iter num is not  None  ) : args . input weights = args . iter num args . output weights = ( args . iter num + 1 ) model params = dict ( num layers = args . num layers , cell size = args . cell size , cell type = args . cell type , embedding size = args . embedding size , learning rate = args . learning rate , tensorboard verbose = args . tensorboard verbose ) if ( args . cmd == 'train' ) : try : num points = int ( args . cmd input [ 0 ] ) except : raise  Exception  ( ' Please   specify  the  number  of  datapoints  to  use  for  training,  as  the  first  argument' ) sp =  Sequence  Pattern  ( args . pattern name , in seq len = args . in len , out seq len = args . out len ) ts2s = TF Learn  Seq 2 Seq  ( sp , seq2seq model = args . model , data dir = args . data dir , name = args . name , verbose = args . verbose ) ts2s . train ( num epochs = args . epochs , num points = num points , weights output fn = args . output weights , weights input fn = args . input weights , model params = model params ) return ts2s elif ( args . cmd == 'predict' ) : if args . from file : inputs = json . loads ( args . from file ) try : input x = map ( int , args . cmd input ) inputs = [ input x ] except : raise  Exception  ( ' Please   provide  a  space-delimited  input  sequence  as  the  argument' ) sp =  Sequence  Pattern  ( args . pattern name , in seq len = args . in len , out seq len = args . out len ) ts2s = TF Learn  Seq 2 Seq  ( sp , seq2seq model = args . model , data dir = args . data dir , name = args . name , verbose = args . verbose ) results = [ ] for x in inputs : ( prediction , y ) = ts2s . predict ( x , weights input fn = args . input weights , model params = model params ) print ( ( '==>   For   input  %s,  prediction=%s  (expected=%s)' % ( x , prediction , sp . generate output sequence ( x ) ) ) ) results . append ( [ prediction , y ] ) ts2s . prediction results = results return ts2s else : print ( ( ' Unknown   command  %s' % args . cmd ) )
def recreate field ( unbound ) : if ( not isinstance ( unbound ,  Unbound  Field  ) ) : raise  Value  Error  ( ( 'recreate field  expects   Unbound  Field   instance,  %s  was  passed.' % type ( unbound ) ) ) return unbound . field class ( * unbound . args , ** unbound . kwargs )
def send returns ( ) : try : send id = request . args [ 0 ] except : redirect ( f = 'send' ) stable = s3db . inv send if ( not auth . s3 has permission ( 'update' , stable , record id = send id ) ) : session . error = T ( ' You   do  not  have  permission  to  return  this  sent  shipment.' ) send record = db ( ( stable . id == send id ) ) . select ( stable . status , limitby = ( 0 , 1 ) ) . first ( ) inv ship status = s3db . inv ship status if ( send record . status == inv ship status [ 'IN PROCESS' ] ) : session . error = T ( ' This   shipment  has  not  been  sent  -  it  cannot  be  returned  because  it  can  still  be  edited.' ) if session . error : redirect ( URL ( c = 'inv' , f = 'send' , args = [ send id ] ) ) rtable = s3db . inv recv tracktable = s3db . inv track item stable [ send id ] = dict ( status = inv ship status [ 'RETURNING' ] , owned by user =  None  , owned by group = ADMIN ) recv row = db ( ( tracktable . send id == send id ) ) . select ( tracktable . recv id , limitby = ( 0 , 1 ) ) . first ( ) if recv row : recv id = recv row . recv id rtable [ recv id ] = dict ( date = request . utcnow , status = inv ship status [ 'RETURNING' ] , owned by user =  None  , owned by group = ADMIN ) db ( ( tracktable . send id == send id ) ) . update ( status = s3db . inv tracking status [ 'RETURNING' ] ) session . confirmation = T ( ' Sent    Shipment   has  returned,  indicate  how  many  items  will  be  returned  to   Warehouse .' ) redirect ( URL ( c = 'inv' , f = 'send' , args = [ send id , 'track item' ] ) )
def get validation errors ( outfile , app =  None  ) : from django . conf import settings from django . db import models , connection from django . db . models . loading import get app errors from django . db . models . fields . related import  Related  Object  from django . db . models . deletion import SET NULL , SET DEFAULT e =  Model  Error  Collection  ( outfile ) for ( app name , error ) in get app errors ( ) . items ( ) : e . add ( app name , error ) for cls in models . get models ( app ) : opts = cls .  meta for f in opts . local fields : if ( ( f . name == 'id' ) and ( not f . primary key ) and ( opts . pk . name == 'id' ) ) : e . add ( opts , ( '"%s":   You   can\'t  use  "id"  as  a  field  name,  because  each  model  automatically  gets  an  "id"  field  if  none  of  the  fields  have  primary key= True .   You   need  to  either  remove/rename  your  "id"  field  or  add  primary key= True   to  a  field.' % f . name ) ) if f . name . endswith ( ' ' ) : e . add ( opts , ( '"%s":   Field   names  cannot  end  with  underscores,  because  this  would  lead  to  ambiguous  queryset  filters.' % f . name ) ) if ( f . primary key and f . null and ( not connection . features . interprets empty strings as nulls ) ) : e . add ( opts , ( '"%s":   Primary   key  fields  cannot  have  null= True .' % f . name ) ) if isinstance ( f , models .  Char  Field  ) : try : max length = int ( f . max length ) if ( max length <= 0 ) : e . add ( opts , ( '"%s":   Char  Fields   require  a  "max length"  attribute  that  is  a  positive  integer.' % f . name ) ) except (  Value  Error  ,  Type  Error  ) : e . add ( opts , ( '"%s":   Char  Fields   require  a  "max length"  attribute  that  is  a  positive  integer.' % f . name ) ) if isinstance ( f , models .  Decimal  Field  ) : ( decimalp ok , mdigits ok ) = (  False  ,  False  ) decimalp msg = '"%s":   Decimal  Fields   require  a  "decimal places"  attribute  that  is  a  non-negative  integer.' try : decimal places = int ( f . decimal places ) if ( decimal places < 0 ) : e . add ( opts , ( decimalp msg % f . name ) ) else : decimalp ok =  True  except (  Value  Error  ,  Type  Error  ) : e . add ( opts , ( decimalp msg % f . name ) ) mdigits msg = '"%s":   Decimal  Fields   require  a  "max digits"  attribute  that  is  a  positive  integer.' try : max digits = int ( f . max digits ) if ( max digits <= 0 ) : e . add ( opts , ( mdigits msg % f . name ) ) else : mdigits ok =  True  except (  Value  Error  ,  Type  Error  ) : e . add ( opts , ( mdigits msg % f . name ) ) invalid values msg = '"%s":   Decimal  Fields   require  a  "max digits"  attribute  value  that  is  greater  than  or  equal  to  the  value  of  the  "decimal places"  attribute.' if ( decimalp ok and mdigits ok ) : if ( decimal places > max digits ) : e . add ( opts , ( invalid values msg % f . name ) ) if ( isinstance ( f , models .  File  Field  ) and ( not f . upload to ) ) : e . add ( opts , ( '"%s":   File  Fields   require  an  "upload to"  attribute.' % f . name ) ) if isinstance ( f , models .  Image  Field  ) : try : from PIL import  Image  except  Import  Error  : try : import  Image  except  Import  Error  : e . add ( opts , ( '"%s":   To   use   Image  Fields ,  you  need  to  install  the   Python    Imaging    Library .   Get   it  at  http://www.pythonware.com/products/pil/  .' % f . name ) ) if ( isinstance ( f , models .  Boolean  Field  ) and getattr ( f , 'null' ,  False  ) ) : e . add ( opts , ( '"%s":   Boolean  Fields   do  not  accept  null  values.   Use   a   Null  Boolean  Field   instead.' % f . name ) ) if f . choices : if ( isinstance ( f . choices , basestring ) or ( not is iterable ( f . choices ) ) ) : e . add ( opts , ( '"%s":  "choices"  should  be  iterable  (e.g.,  a  tuple  or  list).' % f . name ) ) else : for c in f . choices : if ( ( not isinstance ( c , ( list , tuple ) ) ) or ( len ( c ) != 2 ) ) : e . add ( opts , ( '"%s":  "choices"  should  be  a  sequence  of  two-tuples.' % f . name ) ) if ( f . db index not in (  None  ,  True  ,  False  ) ) : e . add ( opts , ( '"%s":  "db index"  should  be  either   None ,   True   or   False .' % f . name ) ) connection . validation . validate field ( e , opts , f ) if ( f . rel and hasattr ( f . rel , 'on delete' ) ) : if ( ( f . rel . on delete == SET NULL ) and ( not f . null ) ) : e . add ( opts , ( "'%s'  specifies  on delete=SET NULL,  but  cannot  be  null." % f . name ) ) elif ( ( f . rel . on delete == SET DEFAULT ) and ( not f . has default ( ) ) ) : e . add ( opts , ( "'%s'  specifies  on delete=SET DEFAULT,  but  has  no  default  value." % f . name ) ) if f . rel : if ( f . rel . to not in models . get models ( ) ) : e . add ( opts , ( "'%s'  has  a  relation  with  model  %s,  which  has  either  not  been  installed  or  is  abstract." % ( f . name , f . rel . to ) ) ) if isinstance ( f . rel . to , ( str , unicode ) ) : continue if ( not f . rel . to .  meta . get field ( f . rel . field name ) . unique ) : e . add ( opts , ( " Field   '%s'  under  model  '%s'  must  have  a  unique= True   constraint." % ( f . rel . field name , f . rel . to .   name   ) ) ) rel opts = f . rel . to .  meta rel name =  Related  Object  ( f . rel . to , cls , f ) . get accessor name ( ) rel query name = f . related query name ( ) if ( not f . rel . is hidden ( ) ) : for r in rel opts . fields : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . local many to many : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . get all related many to many objects ( ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) for r in rel opts . get all related objects ( ) : if ( r . field is not f ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) seen intermediary signatures = [ ] for ( i , f ) in enumerate ( opts . local many to many ) : if ( f . rel . to not in models . get models ( ) ) : e . add ( opts , ( "'%s'  has  an  m2m  relation  with  model  %s,  which  has  either  not  been  installed  or  is  abstract." % ( f . name , f . rel . to ) ) ) if isinstance ( f . rel . to , ( str , unicode ) ) : continue if f . unique : e . add ( opts , ( " Many  To  Many  Fields   cannot  be  unique.     Remove   the  unique  argument  on  '%s'." % f . name ) ) if ( ( f . rel . through is not  None  ) and ( not isinstance ( f . rel . through , basestring ) ) ) : ( from model , to model ) = ( cls , f . rel . to ) if ( ( from model == to model ) and f . rel . symmetrical and ( not f . rel . through .  meta . auto created ) ) : e . add ( opts , ' Many -to-many  fields  with  intermediate  tables  cannot  be  symmetrical.' ) ( seen from , seen to , seen self ) = (  False  ,  False  , 0 ) for inter field in f . rel . through .  meta . fields : rel to = getattr ( inter field . rel , 'to' ,  None  ) if ( from model == to model ) : if ( rel to == from model ) : seen self += 1 if ( seen self > 2 ) : e . add ( opts , ( ' Intermediary   model  %s  has  more  than  two  foreign  keys  to  %s,  which  is  ambiguous  and  is  not  permitted.' % ( f . rel . through .  meta . object name , from model .  meta . object name ) ) ) elif ( rel to == from model ) : if seen from : e . add ( opts , ( ' Intermediary   model  %s  has  more  than  one  foreign  key  to  %s,  which  is  ambiguous  and  is  not  permitted.' % ( f . rel . through .  meta . object name , from model .  meta . object name ) ) ) else : seen from =  True  elif ( rel to == to model ) : if seen to : e . add ( opts , ( ' Intermediary   model  %s  has  more  than  one  foreign  key  to  %s,  which  is  ambiguous  and  is  not  permitted.' % ( f . rel . through .  meta . object name , rel to .  meta . object name ) ) ) else : seen to =  True  if ( f . rel . through not in models . get models ( include auto created =  True  ) ) : e . add ( opts , ( "'%s'  specifies  an  m2m  relation  through  model  %s,  which  has  not  been  installed." % ( f . name , f . rel . through ) ) ) signature = ( f . rel . to , cls , f . rel . through ) if ( signature in seen intermediary signatures ) : e . add ( opts , ( ' The   model  %s  has  two  manually-defined  m2m  relations  through  the  model  %s,  which  is  not  permitted.   Please   consider  using  an  extra  field  on  your  intermediary  model  instead.' % ( cls .  meta . object name , f . rel . through .  meta . object name ) ) ) else : seen intermediary signatures . append ( signature ) if ( not f . rel . through .  meta . auto created ) : ( seen related fk , seen this fk ) = (  False  ,  False  ) for field in f . rel . through .  meta . fields : if field . rel : if ( ( not seen related fk ) and ( field . rel . to == f . rel . to ) ) : seen related fk =  True  elif ( field . rel . to == cls ) : seen this fk =  True  if ( ( not seen related fk ) or ( not seen this fk ) ) : e . add ( opts , ( "'%s'  is  a  manually-defined  m2m  relation  through  model  %s,  which  does  not  have  foreign  keys  to  %s  and  %s" % ( f . name , f . rel . through .  meta . object name , f . rel . to .  meta . object name , cls .  meta . object name ) ) ) elif isinstance ( f . rel . through , basestring ) : e . add ( opts , ( "'%s'  specifies  an  m2m  relation  through  model  %s,  which  has  not  been  installed" % ( f . name , f . rel . through ) ) ) rel opts = f . rel . to .  meta rel name =  Related  Object  ( f . rel . to , cls , f ) . get accessor name ( ) rel query name = f . related query name ( ) if ( rel name is not  None  ) : for r in rel opts . fields : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . local many to many : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . get all related many to many objects ( ) : if ( r . field is not f ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) for r in rel opts . get all related objects ( ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if opts . ordering : for field name in opts . ordering : if ( field name == '?' ) : continue if field name . startswith ( '-' ) : field name = field name [ 1 : ] if ( opts . order with respect to and ( field name == ' order' ) ) : continue if ( '  ' in field name ) : continue if ( field name == 'pk' ) : continue try : opts . get field ( field name , many to many =  False  ) except models .  Field  Does  Not  Exist  : e . add ( opts , ( '"ordering"  refers  to  "%s",  a  field  that  doesn\'t  exist.' % field name ) ) for ut in opts . unique together : for field name in ut : try : f = opts . get field ( field name , many to many =  True  ) except models .  Field  Does  Not  Exist  : e . add ( opts , ( '"unique together"  refers  to  %s,  a  field  that  doesn\'t  exist.   Check   your  syntax.' % field name ) ) else : if isinstance ( f . rel , models .  Many  To  Many  Rel  ) : e . add ( opts , ( '"unique together"  refers  to  %s.   Many  To  Many  Fields   are  not  supported  in  unique together.' % f . name ) ) if ( f not in opts . local fields ) : e . add ( opts , ( '"unique together"  refers  to  %s.   This   is  not  in  the  same  model  as  the  unique together  statement.' % f . name ) ) return len ( e . errors )
def get single ( name , url , module , required , getter = u'  version  ' ) : mod = get version module ( module , name , url ) version getter = getattr ( mod , getter ) if hasattr ( version getter , u'  call  ' ) : current = version getter ( ) else : current = version getter return ( name , url , current , required )
def get health ( ** kwargs ) : with   Ipmi  Command  ( ** kwargs ) as s : return s . get health ( )
def index bar ( ax , vals , facecolor = u'b' , edgecolor = u'l' , width = 4 , alpha = 1.0 ) : facecolors = ( mcolors . to rgba ( facecolor , alpha ) , ) edgecolors = ( mcolors . to rgba ( edgecolor , alpha ) , ) right = ( width / 2.0 ) left = ( ( - width ) / 2.0 ) bars = [ ( ( left , 0 ) , ( left , v ) , ( right , v ) , ( right , 0 ) ) for v in vals if ( v != ( - 1 ) ) ] sx = ( ax . figure . dpi * ( 1.0 / 72.0 ) ) sy = ( ax . bbox . height / ax . view Lim  . height ) bar Transform  =  Affine 2D ( ) . scale ( sx , sy ) offsets Bars  = [ ( i , 0 ) for ( i , v ) in enumerate ( vals ) if ( v != ( - 1 ) ) ] bar Collection  =  Poly  Collection  ( bars , facecolors = facecolors , edgecolors = edgecolors , antialiaseds = ( 0 , ) , linewidths = ( 0.5 , ) , offsets = offsets Bars  , trans Offset  = ax . trans Data  ) bar Collection  . set transform ( bar Transform  ) ( minpy , maxx ) = ( 0 , len ( offsets Bars  ) ) miny = 0 maxy = max ( [ v for v in vals if ( v != ( - 1 ) ) ] ) corners = ( ( minpy , miny ) , ( maxx , maxy ) ) ax . update datalim ( corners ) ax . autoscale view ( ) ax . add collection ( bar Collection  ) return bar Collection
def zk service name ( ip address , keyname ) : key file = '{}/{}.key' . format ( KEY DIRECTORY , keyname ) ssh cmd = [ 'ssh' , '-i' , key file , ip address , 'ls  {}' . format ( SERVICES DIR ) ] response = subprocess . check output ( ssh cmd ) init files = response . split ( ) for init file in init files : if ( 'zookeeper' in init file ) : return init file raise OS Error  ( ' Unable   to  find   Zoo  Keeper   on  {}' . format ( ip address ) )
def erfinv ( x , a = 0.147 ) : lnx = log ( ( 1 - ( x * x ) ) ) part1 = ( ( 2 / ( a * pi ) ) + ( lnx / 2 ) ) part2 = ( lnx / a ) sgn = ( 1 if ( x > 0 ) else ( - 1 ) ) return ( sgn * sqrt ( ( sqrt ( ( ( part1 * part1 ) - part2 ) ) - part1 ) ) )
@ app . route ( '/digest-auth/<qop>/<user>/<passwd>/<algorithm>' ) def digest auth ( qop =  None  , user = 'user' , passwd = 'passwd' , algorithm = 'MD5' ) : if ( algorithm not in ( 'MD5' , 'SHA-256' ) ) : algorithm = 'MD5' if ( qop not in ( 'auth' , 'auth-int' ) ) : qop =  None  if ( ( ' Authorization ' not in request . headers ) or ( not check digest auth ( user , passwd ) ) or ( ' Cookie ' not in request . headers ) ) : response = app . make response ( '' ) response . status code = 401 nonce = H ( '' . join ( [ getattr ( request , 'remote addr' , u'' ) . encode ( 'ascii' ) , ':' , str ( time . time ( ) ) . encode ( 'ascii' ) , ':' , os . urandom ( 10 ) ] ) , 'MD5' ) opaque = H ( os . urandom ( 10 ) , 'MD5' ) auth = WWW Authenticate  ( 'digest' ) auth . set digest ( 'me@kennethreitz.com' , nonce , opaque = opaque , qop = ( ( 'auth' , 'auth-int' ) if ( qop is  None  ) else ( qop , ) ) , algorithm = algorithm ) response . headers [ 'WWW- Authenticate ' ] = auth . to header ( ) response . headers [ ' Set - Cookie ' ] = 'fake=fake value' return response return jsonify ( authenticated =  True  , user = user )
def lookup ( path , parent =  None  , user =  None  , exists =  None  ) : url = build url ( RESOURCE , route = 'lookup' ) params = make params ( path = path , parent = parent , user = user , exists = exists ) return request ( 'get' , url , params = params )
def extra context request processor ( page , request ) : request .  feincms extra context . update ( { u'in appcontent subpage' :  False  , u'extra path' : u'/' } ) url = page . get absolute url ( ) if ( request . path != url ) : request .  feincms extra context . update ( { u'in appcontent subpage' :  True  , u'extra path' : re . sub ( ( u'^' + re . escape ( url . rstrip ( u'/' ) ) ) , u'' , request . path ) } )
def url filter ( http url req ) : if http url req : d = [ '.jpg' , '.jpeg' , '.gif' , '.png' , '.css' , '.ico' , '.js' , '.svg' , '.woff' ] if any ( ( http url req . endswith ( i ) for i in d ) ) : return return http url req
def inference network ( x ) : with pt . defaults scope ( activation fn = tf . nn . elu , batch normalize =  True  , scale after normalization =  True  ) : params = pt . wrap ( x ) . reshape ( [ M , 28 , 28 , 1 ] ) . conv2d ( 5 , 32 , stride = 2 ) . conv2d ( 5 , 64 , stride = 2 ) . conv2d ( 5 , 128 , edges = 'VALID' ) . dropout ( 0.9 ) . flatten ( ) . fully connected ( ( d * 2 ) , activation fn =  None  ) . tensor mu = tf . reshape ( params [ : , : d ] , [ ( - 1 ) ] ) sigma = tf . reshape ( tf . nn . softplus ( params [ : , d : ] ) , [ ( - 1 ) ] ) return ( mu , sigma )
def install editor ( template , wait =  False  ) : def call editor ( self , filename , line = 0 ) : if ( line is  None  ) : line = 0 cmd = template . format ( filename = pipes . quote ( filename ) , line = line ) print ( '>' , cmd ) if sys . platform . startswith ( 'win' ) : cmd = shlex . split ( cmd ) proc = subprocess .  Popen  ( cmd , shell =  True  ) if ( wait and ( proc . wait ( ) != 0 ) ) : raise  Try  Next  ( ) if wait : py3compat . input ( ' Press    Enter   when  done  editing:' ) get ipython ( ) . set hook ( 'editor' , call editor ) get ipython ( ) . editor = template
def get ( request ) : return request . session . get ( 'partial pipeline' )
def test base of reduce preferred ( ) : dsk = dict ( ( ( ( 'a' , i ) , ( f , ( 'a' , ( i - 1 ) ) , ( 'b' , i ) ) ) for i in [ 1 , 2 , 3 ] ) ) dsk [ ( 'a' , 0 ) ] = ( f , ( 'b' , 0 ) ) dsk . update ( dict ( ( ( ( 'b' , i ) , ( f , 'c' , 1 ) ) for i in [ 0 , 1 , 2 , 3 ] ) ) ) dsk [ 'c' ] = 1 o = order ( dsk ) assert ( o == { ( 'a' , 3 ) : 0 , ( 'a' , 2 ) : 1 , ( 'a' , 1 ) : 2 , ( 'a' , 0 ) : 3 , ( 'b' , 0 ) : 4 , 'c' : 5 , ( 'b' , 1 ) : 6 , ( 'b' , 2 ) : 7 , ( 'b' , 3 ) : 8 } ) assert ( min ( [ ( 'b' , i ) for i in [ 0 , 1 , 2 , 3 ] ] , key = o . get ) == ( 'b' , 0 ) )
def get names ( contents ) : names = [ ] in entry =  False  pos = 0 contents length = len ( contents ) while  True  : if ( not in entry ) : matcher = re . search ( NAME FIELD REGEX , contents [ pos : ] ) if ( not matcher ) : break pos += matcher . end ( ) in entry =  True  else : chars = [ ] bracket depth = 1 for c in contents [ pos : ] : if ( c == '}' ) : bracket depth -= 1 if ( bracket depth == 0 ) : break if ( c == '{' ) : bracket depth += 1 chars . append ( c ) names . extend ( [ unicode (  Name  ( s ) ) for s in tokenize list ( u'' . join ( chars ) ) ] ) pos += len ( chars ) if ( pos >= contents length ) : break in entry =  False  return sorted ( set ( names ) )
def  zero fill array ( context , builder , ary ) : cgutils . memset ( builder , ary . data , builder . mul ( ary . itemsize , ary . nitems ) , 0 )
def test classification report imbalanced multiclass with string label ( ) : ( y true , y pred ,   ) = make prediction ( binary =  False  ) y true = np . array ( [ 'blue' , 'green' , 'red' ] ) [ y true ] y pred = np . array ( [ 'blue' , 'green' , 'red' ] ) [ y pred ] expected report = 'pre  rec  spe  f1  geo  iba  sup  blue  0.83  0.79  0.92  0.81  0.86  0.74  24  green  0.33  0.10  0.86  0.15  0.44  0.19  31  red  0.42  0.90  0.55  0.57  0.63  0.37  20  avg  /  total  0.51  0.53  0.80  0.47  0.62  0.41  75' report = classification report imbalanced ( y true , y pred ) assert equal (  format report ( report ) , expected report ) expected report = 'pre  rec  spe  f1  geo  iba  sup  a  0.83  0.79  0.92  0.81  0.86  0.74  24  b  0.33  0.10  0.86  0.15  0.44  0.19  31  c  0.42  0.90  0.55  0.57  0.63  0.37  20  avg  /  total  0.51  0.53  0.80  0.47  0.62  0.41  75' report = classification report imbalanced ( y true , y pred , target names = [ 'a' , 'b' , 'c' ] ) assert equal (  format report ( report ) , expected report )
def   Handle  Wildcard  Cases  ( first handler , second handler ) : merged handlers = set ( ) if ( ( len ( first handler . pattern ) <= 1 ) or ( len ( second handler . pattern ) <= 1 ) ) : return merged handlers if ( ( first handler . pattern [ ( - 1 ) ] , second handler . pattern [ 0 ] ) != ( '*' , '*' ) ) : return merged handlers first no star = first handler . pattern [ : ( - 1 ) ] merged handlers . add (  Simple  Handler  ( ( first no star + second handler . pattern ) ) ) if second handler .  Matches  String  ( first no star ) : merged handlers . add (  Simple  Handler  ( first no star ) ) return merged handlers
def intcurve diffequ ( vector field , param , start point , coord sys =  None  ) : if ( ( contravariant order ( vector field ) != 1 ) or covariant order ( vector field ) ) : raise  Value  Error  ( ' The   supplied  field  was  not  a  vector  field.' ) coord sys = ( coord sys if coord sys else start point .  coord sys ) gammas = [  Function  ( ( 'f %d' % i ) ) ( param ) for i in range ( start point .  coord sys . dim ) ] arbitrary p =  Point  ( coord sys , gammas ) coord functions = coord sys . coord functions ( ) equations = [ simplify ( ( diff ( cf . rcall ( arbitrary p ) , param ) - vector field . rcall ( cf ) . rcall ( arbitrary p ) ) ) for cf in coord functions ] init cond = [ simplify ( ( cf . rcall ( arbitrary p ) . subs ( param , 0 ) - cf . rcall ( start point ) ) ) for cf in coord functions ] return ( equations , init cond )
def cleanup instance ( xenapi , instance , vm ref , vm rec ) : xenapi .  vmops .  destroy ( instance , vm ref )
def  Render  Token  Approval  Template  ( oauth callback ) : template dict = { 'oauth callback' : cgi . escape ( oauth callback , quote =  True  ) } return ( TOKEN APPROVAL TEMPLATE % template dict )
@ hsa . jit ( device =  True  ) def shuf device inclusive scan ( data , temp ) : tid = hsa . get local id ( 0 ) lane = ( tid & (  WARPSIZE - 1 ) ) warpid = ( tid >> 6 ) warp scan res = shuf wave inclusive scan ( data ) hsa . barrier ( ) if ( lane == (  WARPSIZE - 1 ) ) : temp [ warpid ] = warp scan res hsa . barrier ( ) if ( warpid == 0 ) : shuf wave inclusive scan ( temp [ lane ] ) hsa . barrier ( ) blocksum = 0 if ( warpid > 0 ) : blocksum = temp [ ( warpid - 1 ) ] return ( warp scan res + blocksum )
def send Stay  Awake  ( ) : cocoa .  Update  System  Activity  ( 0 )
def  load plugins ( plugins , debug =  True  ) : plugs = [ ] for plugin in plugins : setup class = plugin . get ( 'setup class' ) plugin name = plugin . get ( '  name  ' ) . split ( ) [ ( - 1 ) ] mod name = '.' . join ( setup class . split ( '.' ) [ : ( - 1 ) ] ) class name = setup class . split ( '.' ) [ ( - 1 ) ] try : mod =   import   ( mod name , globals ( ) , locals ( ) , [ class name ] ) except  Syntax  Error  as e : raise exception .  Plugin  Syntax  Error  ( ( ' Plugin   %s  (%s)  contains  a  syntax  error  at  line  %s' % ( plugin name , e . filename , e . lineno ) ) ) except  Import  Error  as e : raise exception .  Plugin  Load  Error  ( ( ' Failed   to  import  plugin  %s:  %s' % ( plugin name , e [ 0 ] ) ) ) klass = getattr ( mod , class name ,  None  ) if ( not klass ) : raise exception .  Plugin  Error  ( ( ' Plugin   class  %s  does  not  exist' % setup class ) ) if ( not issubclass ( klass , clustersetup .  Cluster  Setup  ) ) : raise exception .  Plugin  Error  ( ( ' Plugin   %s  must  be  a  subclass  of  starcluster.clustersetup. Cluster  Setup ' % setup class ) ) ( args , kwargs ) = utils . get arg spec ( klass .   init   , debug = debug ) config args = [ ] missing args = [ ] for arg in args : if ( arg in plugin ) : config args . append ( plugin . get ( arg ) ) else : missing args . append ( arg ) if debug : log . debug ( ( 'config args  =  %s' % config args ) ) if missing args : raise exception .  Plugin  Error  ( ( ' Not   enough  settings  provided  for  plugin  %s  (missing:  %s)' % ( plugin name , ',  ' . join ( missing args ) ) ) ) config kwargs = { } for arg in kwargs : if ( arg in plugin ) : config kwargs [ arg ] = plugin . get ( arg ) if debug : log . debug ( ( 'config kwargs  =  %s' % config kwargs ) ) try : plug obj = klass ( * config args , ** config kwargs ) except  Exception  as exc : log . error ( ' Error   occured:' , exc info =  True  ) raise exception .  Plugin  Load  Error  ( ( ' Failed   to  load  plugin  %s  with  the  following  error:  %s  -  %s' % ( setup class , exc .   class   .   name   , exc . message ) ) ) if ( not hasattr ( plug obj , '  name  ' ) ) : setattr ( plug obj , '  name  ' , plugin name ) plugs . append ( plug obj ) return plugs
def average pooling nd ( x , ksize , stride =  None  , pad = 0 , use cudnn =  True  ) : ndim = len ( x . shape [ 2 : ] ) return  Average  Pooling ND ( ndim , ksize , stride = stride , pad = pad , use cudnn = use cudnn ) ( x )
def sh3 ( cmd ) : p =  Popen  ( cmd , stdout = PIPE , stderr = PIPE , shell =  True  , env = sub environment ( ) ) ( out , err ) = p . communicate ( ) retcode = p . returncode if retcode : raise  Called  Process  Error  ( retcode , cmd ) else : return ( out . rstrip ( ) , err . rstrip ( ) )
def check c overviewer ( ) : root dir = util . get program path ( ) try : import c overviewer except  Import  Error  : if ( hasattr ( sys , 'frozen' ) and ( platform . system ( ) == ' Windows ' ) ) : print ' Something   has  gone  wrong  importing  the  c overviewer  extension.     Please ' print 'make  sure  the  2008  and  2010  redistributable  packages  from   Microsoft ' print 'are  installed.' return 1 ext = os . path . join ( root dir , 'overviewer core' , ( 'c overviewer.%s' % ( 'pyd' if ( platform . system ( ) == ' Windows ' ) else 'so' ) ) ) if os . path . exists ( ext ) : traceback . print exc ( ) print '' print ' Something   has  gone  wrong  importing  the  c overviewer  extension.     Please ' print 'make  sure  it  is  up-to-date  (clean  and  rebuild)' return 1 print ' You   need  to  compile  the  c overviewer  module  to  run   Minecraft    Overviewer .' print ' Run   `python  setup.py  build`,  or  see  the  README  for  details.' return 1 if hasattr ( sys , 'frozen' ) : pass elif ( 'extension version' in dir ( c overviewer ) ) : if os . path . exists ( os . path . join ( root dir , 'overviewer core' , 'src' , 'overviewer.h' ) ) : with open ( os . path . join ( root dir , 'overviewer core' , 'src' , 'overviewer.h' ) ) as f : lines = f . readlines ( ) lines = filter ( ( lambda x : x . startswith ( '#define  OVERVIEWER EXTENSION VERSION' ) ) , lines ) if lines : l = lines [ 0 ] if ( int ( l . split ( ) [ 2 ] . strip ( ) ) != c overviewer . extension version ( ) ) : print ' Please   rebuild  your  c overviewer  module.     It   is  out  of  date!' return 1 else : print ' Please   rebuild  your  c overviewer  module.     It   is  out  of  date!' return 1 return 0
def read file ( path , fileobj =  None  , yields lines =  True  , cleanup =  None  ) : f =  None  try : if ( fileobj is  None  ) : f = open ( path , 'rb' ) else : f = fileobj decompressed f = decompress ( f , path ) if ( ( decompressed f is f ) and yields lines ) : lines = f else : lines = to lines ( decompressed f ) for line in lines : ( yield line ) finally : try : if ( f and ( f is not fileobj ) ) : f . close ( ) finally : if cleanup : cleanup ( )
def get translation dict from file ( path , lang , app ) : cleaned = { } if os . path . exists ( path ) : csv content = read csv file ( path ) for item in csv content : if ( len ( item ) == 3 ) : cleaned [ item [ 1 ] ] = strip ( item [ 2 ] ) elif ( len ( item ) == 2 ) : cleaned [ item [ 0 ] ] = strip ( item [ 1 ] ) else : raise  Exception  ( u" Bad   translation  in  '{app}'  for  language  '{lang}':  {values}" . format ( app = app , lang = lang , values = repr ( item ) . encode ( u'utf-8' ) ) ) return cleaned
def nottest ( func ) : func .   test   =  False  return func
def header ( hdr , value , unique =  False  ) : if ( unique is  True  ) : for ( h , v ) in ctx . headers : if ( h == hdr ) : return elif ( unique is  None  ) : ctx . headers = [ h for h in ctx . headers if ( h [ 0 ] != hdr ) ] ctx . headers . append ( ( hdr , value ) )
def  windows long path name ( short path ) : if ( not isinstance ( short path , six . text type ) ) : short path = short path . decode (  fsencoding ( ) ) import ctypes buf = ctypes . create unicode buffer ( 260 ) get long path name w = ctypes . windll . kernel32 .  Get  Long  Path  Name W return value = get long path name w ( short path , buf , 260 ) if ( ( return value == 0 ) or ( return value > 260 ) ) : return short path else : long path = buf . value if ( ( len ( long path ) > 1 ) and ( long path [ 1 ] == ':' ) ) : long path = ( long path [ 0 ] . upper ( ) + long path [ 1 : ] ) return long path
def MultiplyThreeNums(a, b, c): return a * b * c
def revoke ( name =  None  , source group name =  None  , source group owner id =  None  , ip protocol =  None  , from port =  None  , to port =  None  , cidr ip =  None  , group id =  None  , source group group id =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  , vpc id =  None  , vpc name =  None  , egress =  False  ) : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) group =  get group ( conn , name = name , vpc id = vpc id , vpc name = vpc name , group id = group id , region = region , key = key , keyid = keyid , profile = profile ) if group : try : revoked =  None  if ( not egress ) : revoked = conn . revoke security group ( src security group name = source group name , src security group owner id = source group owner id , ip protocol = ip protocol , from port = from port , to port = to port , cidr ip = cidr ip , group id = group . id , src security group group id = source group group id ) else : revoked = conn . revoke security group egress ( ip protocol = ip protocol , from port = from port , to port = to port , cidr ip = cidr ip , group id = group . id , src group id = source group group id ) if revoked : log . info ( ' Removed   rule  from  security  group  {0}  with  id  {1}.' . format ( group . name , group . id ) ) return  True  else : msg = ' Failed   to  remove  rule  from  security  group  {0}  with  id  {1}.' . format ( group . name , group . id ) log . error ( msg ) return  False  except boto . exception . EC2 Response  Error  as e : msg = ' Failed   to  remove  rule  from  security  group  {0}  with  id  {1}.' . format ( group . name , group . id ) log . error ( msg ) log . error ( e ) return  False  else : log . error ( ' Failed   to  remove  rule  from  security  group.' ) return  False
def build request ( url , data =  None  , headers =  None  , bump = '' ) : if ( not USER AGENT ) : build user agent ( ) if ( not headers ) : headers = { } if ( url [ 0 ] == ':' ) : schemed url = ( '%s%s' % ( SCHEME , url ) ) else : schemed url = url if ( '?' in url ) : delim = '&' else : delim = '?' final url = ( '%s%sx=%s.%s' % ( schemed url , delim , int ( ( timeit . time . time ( ) * 1000 ) ) , bump ) ) headers . update ( { ' User - Agent ' : USER AGENT , ' Cache - Control ' : 'no-cache' } ) printer ( ( '%s  %s' % ( ( 'GET' , 'POST' ) [ bool ( data ) ] , final url ) ) , debug =  True  ) return  Request  ( final url , data = data , headers = headers )
def AddThreeNums(a, b, c): return a + b + c
def on log ( request , page name ) : page =  Page  . query . filter by ( name = page name ) . first ( ) if ( page is  None  ) : return page missing ( request , page name ,  False  ) return  Response  ( generate template ( 'action log.html' , page = page ) )
def paths equal ( paths 1 , paths 2 ) : if ( len ( paths 1 ) != len ( paths 2 ) ) : return  False  for ( path 1 , path 2 ) in zip ( paths 1 , paths 2 ) : if ( len ( path 1 ) != len ( path 2 ) ) : return  False  for ( path item 1 , path item 2 ) in zip ( path 1 , path 2 ) : if ( path item 1 . display name != path item 2 . display name ) : return  False  usage key 1 = path item 1 . usage key . replace ( course key = modulestore ( ) . fill in run ( path item 1 . usage key . course key ) ) usage key 2 = path item 1 . usage key . replace ( course key = modulestore ( ) . fill in run ( path item 2 . usage key . course key ) ) if ( usage key 1 != usage key 2 ) : return  False  return  True
def SubtractFromTheFirstPascal(a, b): return a - b
@ log call def metadef property get all ( context , namespace name ) : namespace = metadef namespace get ( context , namespace name ) properties = [ ]  check namespace visibility ( context , namespace , namespace name ) for property in DATA [ 'metadef properties' ] : if ( property [ 'namespace id' ] == namespace [ 'id' ] ) : properties . append ( property ) return properties
def desaturate ( color , percent ) : return adjust ( color , 1 , ( - percent ) )
def  check all skipped ( test ) : import doctest all skipped = all ( ( x . options . get ( doctest . SKIP ,  False  ) for x in test . examples ) ) if all skipped : pytest . skip ( 'all  tests  skipped  by  +SKIP  option' )
def isEven(num): return num % 2 == 0
def to size in mb ( data size , unit ) : if ( unit == 'G' ) : return ( data size * 1024 ) elif ( unit == 'B' ) : return ( data size / ( 1024 * 1024.0 ) ) else : return data size
def shortest path ( files path list ) : if ( len ( files path list ) > 0 ) : shortest path = files path list [ 0 ] shortest path length = len ( files path list [ 0 ] ) for path elmts in files path list : if ( len ( path elmts ) < shortest path length ) : shortest path length = len ( path elmts ) shortest path = path elmts return os . path . join ( * shortest path )
def greet(name): return f"Hello, {name}!"
def nat gateway absent ( name =  None  , subnet name =  None  , subnet id =  None  , region =  None  , key =  None  , keyid =  None  , profile =  None  , wait for delete retries = 0 ) : ret = { 'name' : name , 'result' :  True  , 'comment' : '' , 'changes' : { } } r =   salt   [ 'boto vpc.describe nat gateways' ] ( subnet name = subnet name , subnet id = subnet id , region = region , key = key , keyid = keyid , profile = profile ) if ( not r ) : ret [ 'comment' ] = ' Nat   gateway  does  not  exist.' return ret if   opts   [ 'test' ] : ret [ 'comment' ] = ' Nat   gateway  is  set  to  be  removed.' ret [ 'result' ] =  None  return ret for gw in r : rtbl id = gw . get ( ' Nat  Gateway  Id ' ) r =   salt   [ 'boto vpc.delete nat gateway' ] ( nat gateway id = rtbl id , release eips =  True  , region = region , key = key , keyid = keyid , profile = profile , wait for delete =  True  , wait for delete retries = wait for delete retries ) if ( 'error' in r ) : ret [ 'result' ] =  False  ret [ 'comment' ] = ' Failed   to  delete  nat  gateway:  {0}' . format ( r [ 'error' ] [ 'message' ] ) return ret ret [ 'comment' ] = ',  ' . join ( ( ret [ 'comment' ] , ' Nat   gateway  {0}  deleted.' . format ( rtbl id ) ) ) ret [ 'changes' ] [ 'old' ] = { 'nat gateway' : rtbl id } ret [ 'changes' ] [ 'new' ] = { 'nat gateway' :  None  } return ret
def setup platform ( hass , config , add devices , discovery info =  None  ) : if ( not int ( hub . config . get ( CONF SMARTPLUGS , 1 ) ) ) : return  False  hub . update smartplugs ( ) switches = [ ] switches . extend ( [  Verisure  Smartplug  ( value . device Label  ) for value in hub . smartplug status . values ( ) ] ) add devices ( switches )
def addThreeNums(a, b, c): return a + b + c
@ verbose def data path ( path =  None  , force update =  False  , update path =  True  , download =  True  , verbose =  None  ) : return  data path ( path = path , force update = force update , update path = update path , name = 'visual 92 categories' , download = download )
def py library ( name , srcs = [ ] , deps = [ ] , base =  None  , visibility =  None  , ** kwargs ) : target =  Python  Library  ( name , srcs , deps , base , visibility , kwargs ) blade . blade . register target ( target )
def multiplyThreeNums(a, b, c): return a * b * c
def dssp dict from pdb file ( in file , DSSP = 'dssp' ) : p = subprocess .  Popen  ( [ DSSP , in file ] , universal newlines =  True  , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) ( out , err ) = p . communicate ( ) if err . strip ( ) : warnings . warn ( err ) if ( not out . strip ( ) ) : raise  Exception  ( 'DSSP  failed  to  produce  an  output' ) ( out dict , keys ) =  make dssp dict (  String IO ( out ) ) return ( out dict , keys )
def changes from tree ( names , lookup entry , object store , tree , want unchanged =  False  ) : other names = set ( names ) if ( tree is not  None  ) : for ( name , mode , sha ) in object store . iter tree contents ( tree ) : try : ( other sha , other mode ) = lookup entry ( name ) except  Key  Error  : ( yield ( ( name ,  None  ) , ( mode ,  None  ) , ( sha ,  None  ) ) ) else : other names . remove ( name ) if ( want unchanged or ( other sha != sha ) or ( other mode != mode ) ) : ( yield ( ( name , name ) , ( mode , other mode ) , ( sha , other sha ) ) ) for name in other names : try : ( other sha , other mode ) = lookup entry ( name ) except  Key  Error  : pass else : ( yield ( (  None  , name ) , (  None  , other mode ) , (  None  , other sha ) ) )
def subtractFromTheFirst(a, b): return a - b
def confirmation view ( template , doc = ' Display   a  confirmation  view.' ) : def confirmed ( request ) : comment =  None  if ( 'c' in request . GET ) : try : comment = comments . get model ( ) . objects . get ( pk = request . GET [ 'c' ] ) except (  Object  Does  Not  Exist  ,  Value  Error  ) : pass return render to response ( template , { 'comment' : comment } , context instance =  Request  Context  ( request ) ) confirmed .   doc   = textwrap . dedent ( ( '                %s\n\n                 Templates :  :template:`%s``\n                 Context :\n                        comment\n                                 The   posted  comment\n                ' % ( doc , template ) ) ) return confirmed
def analyze python ( source ) : lines = source . splitlines (  True  ) lines . append ( '' ) readline = functools . partial ( next , iter ( lines ) , '' ) kind = tok str = '' tok type = tokenize . COMMENT written = ( 1 , 0 ) for tok in tokenize . generate tokens ( readline ) : ( prev tok type , prev tok str ) = ( tok type , tok str ) ( tok type , tok str , ( srow , scol ) , ( erow , ecol ) , logical lineno ) = tok kind = '' if ( tok type == tokenize . COMMENT ) : kind = 'comment' elif ( ( tok type == tokenize . OP ) and ( tok str [ : 1 ] not in '{}[](),.:;@' ) ) : kind = 'operator' elif ( tok type == tokenize . STRING ) : kind = 'string' if ( ( prev tok type == tokenize . INDENT ) or ( scol == 0 ) ) : kind = 'docstring' elif ( tok type == tokenize . NAME ) : if ( tok str in ( 'def' , 'class' , 'import' , 'from' ) ) : kind = 'definition' elif ( prev tok str in ( 'def' , 'class' ) ) : kind = 'defname' elif keyword . iskeyword ( tok str ) : kind = 'keyword' elif ( is builtin ( tok str ) and ( prev tok str != '.' ) ) : kind = 'builtin' if kind : ( text , written ) = combine range ( lines , written , ( srow , scol ) ) ( yield ( '' , text ) ) ( text , written ) = ( tok str , ( erow , ecol ) ) ( yield ( kind , text ) ) ( line upto token , written ) = combine range ( lines , written , ( erow , ecol ) ) ( yield ( '' , line upto token ) )
def vtk output ( obj ) : if vtk old ( ) : return obj . output return obj . get output ( )
def FindMaxOfThreeNums(a, b, c): return max(a, b, c)
def  logm ( A ) : A = np . asarray ( A ) if ( ( len ( A . shape ) != 2 ) or ( A . shape [ 0 ] != A . shape [ 1 ] ) ) : raise  Value  Error  ( 'expected  a  square  matrix' ) if issubclass ( A . dtype . type , np . integer ) : A = np . asarray ( A , dtype = float ) keep it real = np . isrealobj ( A ) try : if np . array equal ( A , np . triu ( A ) ) : A =  logm force nonsingular triangular matrix ( A ) if ( np . min ( np . diag ( A ) ) < 0 ) : A = A . astype ( complex ) return  logm triu ( A ) else : if keep it real : ( T , Z ) = schur ( A ) if ( not np . array equal ( T , np . triu ( T ) ) ) : ( T , Z ) = rsf2csf ( T , Z ) else : ( T , Z ) = schur ( A , output = 'complex' ) T =  logm force nonsingular triangular matrix ( T , inplace =  True  ) U =  logm triu ( T ) ZH = np . conjugate ( Z ) . T return Z . dot ( U ) . dot ( ZH ) except (  Sqrtm  Error  ,  Logm  Error  ) : X = np . empty like ( A ) X . fill ( np . nan ) return X
def render ( cheetah data , saltenv = 'base' , sls = '' , method = 'xml' , ** kws ) : if ( not HAS LIBS ) : return { } if ( not isinstance ( cheetah data , string types ) ) : cheetah data = cheetah data . read ( ) if cheetah data . startswith ( '#!' ) : cheetah data = cheetah data [ ( cheetah data . find ( '\n' ) + 1 ) : ] if ( not cheetah data . strip ( ) ) : return { } return str (  Template  ( cheetah data , search List  = [ kws ] ) )
def PowerOfNum(base, exponent): return base ** exponent
def compile rules ( environment ) : e = re . escape rules = [ ( len ( environment . comment start string ) , 'comment' , e ( environment . comment start string ) ) , ( len ( environment . block start string ) , 'block' , e ( environment . block start string ) ) , ( len ( environment . variable start string ) , 'variable' , e ( environment . variable start string ) ) ] if ( environment . line statement prefix is not  None  ) : rules . append ( ( len ( environment . line statement prefix ) , 'linestatement' , ( '^\\s*' + e ( environment . line statement prefix ) ) ) ) if ( environment . line comment prefix is not  None  ) : rules . append ( ( len ( environment . line comment prefix ) , 'linecomment' , ( '(?:^|(?<=\\S))[^\\S\\r\\n]*' + e ( environment . line comment prefix ) ) ) ) return [ x [ 1 : ] for x in sorted ( rules , reverse =  True  ) ]
def check anyuri ( uri ) : if ( re . match ( ( u'(([a-zA-Z][0-9a-zA-Z+\\-\\.]*:)?/{0,2}[0-9a-zA-Z;' + u"/?:@&=+$\\.\\- !~*'()%]+)?(#[0-9a-zA-Z;/?:@&=+$\\.\\- !~*'()%]+)?" ) , uri ) is  None  ) : return  False  try : urllib . parse . urlparse ( uri ) except  Exception  : return  False  return  True
def DivideTwoNums(a, b): return a / b
def AbsoluteDifference(a, b): return abs(a - b)
def SquareRoot(num): return num ** 0.5def find module ( module , path =  None  , imp =  None  ) : if ( imp is  None  ) : imp = importlib . import module with cwd in path ( ) : if ( '.' in module ) : last =  None  parts = module . split ( '.' ) for ( i , part ) in enumerate ( parts [ : ( - 1 ) ] ) : path = imp ( '.' . join ( parts [ : ( i + 1 ) ] ) ) .   path   last =  imp . find module ( parts [ ( i + 1 ) ] , path ) return last return  imp . find module ( module )
def create l2 gw service ( cluster , tenant id , display name , devices ) : tags = [ { 'tag' : tenant id , 'scope' : 'os tid' } ] gateways = [ { 'transport node uuid' : device [ 'id' ] , 'device id' : device [ 'interface name' ] , 'type' : 'L2 Gateway ' } for device in devices ] gwservice obj = { 'display name' :  check and truncate name ( display name ) , 'tags' : tags , 'gateways' : gateways , 'type' : 'L2 Gateway  Service  Config ' } try : return json . loads ( do single request ( 'POST' ,  build uri path ( GWSERVICE RESOURCE ) , json . dumps ( gwservice obj ) , cluster = cluster ) ) except  Nvp  Api  Client  .  Nvp  Api  Exception  : LOG . exception (   ( ' An   exception  occured  while  communicating  with  the  NVP  controller  for  cluster:%s' ) , cluster . name ) raise
def IsEven(num): return num % 2 == 0
def Greet(name): return f"Hello, {name}!"
def splithost ( url ) : global  hostprog if (  hostprog is  None  ) :  hostprog = re . compile ( '//([^/?]*)(.*)' , re . DOTALL ) match =  hostprog . match ( url ) if match : ( host port , path ) = match . groups ( ) if ( path and ( path [ 0 ] != '/' ) ) : path = ( '/' + path ) return ( host port , path ) return (  None  , url )
def run wsgi ( conf file , app section , * args , ** kwargs ) : try : ( app , conf , logger , log name ) = init request processor ( conf file , app section , * args , ** kwargs ) except  Config  File  Error  as e : print e return sock = get socket ( conf , default port = kwargs . get ( 'default port' , 8080 ) ) drop privileges ( conf . get ( 'user' , 'swift' ) ) reserve = int ( conf . get ( 'fallocate reserve' , 0 ) ) if ( reserve > 0 ) : utils . FALLOCATE RESERVE = reserve capture stdio ( logger ) def run server ( ) : wsgi .  Http  Protocol  . default request version = 'HTTP/1.0' wsgi .  Http  Protocol  . log request = ( lambda * a :  None  ) wsgi .  Http  Protocol  . log message = ( lambda s , f , * a : logger . error ( ( 'ERROR  WSGI:  ' + ( f % a ) ) ) ) wsgi . WRITE TIMEOUT = int ( ( conf . get ( 'client timeout' ) or 60 ) ) eventlet . hubs . use hub ( get hub ( ) ) eventlet . patcher . monkey patch ( all =  False  , socket =  True  ) eventlet debug = config true value ( conf . get ( 'eventlet debug' , 'no' ) ) eventlet . debug . hub exceptions ( eventlet debug ) app = loadapp ( ( 'config:%s' % conf file ) , global conf = { 'log name' : log name } ) pool =  Green  Pool  ( size = 1024 ) try : wsgi . server ( sock , app ,  Null  Logger  ( ) , custom pool = pool ) except socket . error as err : if ( err [ 0 ] != errno . EINVAL ) : raise pool . waitall ( ) worker count = int ( conf . get ( 'workers' , '1' ) ) if ( worker count == 0 ) : run server ( ) return def kill children ( * args ) : ' Kills   the  entire  process  group.' logger . error ( 'SIGTERM  received' ) signal . signal ( signal . SIGTERM , signal . SIG IGN ) running [ 0 ] =  False  os . killpg ( 0 , signal . SIGTERM ) def hup ( * args ) : ' Shuts   down  the  server,  but  allows  running  requests  to  complete' logger . error ( 'SIGHUP  received' ) signal . signal ( signal . SIGHUP , signal . SIG IGN ) running [ 0 ] =  False  running = [  True  ] signal . signal ( signal . SIGTERM , kill children ) signal . signal ( signal . SIGHUP , hup ) children = [ ] while running [ 0 ] : while ( len ( children ) < worker count ) : pid = os . fork ( ) if ( pid == 0 ) : signal . signal ( signal . SIGHUP , signal . SIG DFL ) signal . signal ( signal . SIGTERM , signal . SIG DFL ) run server ( ) logger . notice ( ( ' Child   %d  exiting  normally' % os . getpid ( ) ) ) return else : logger . notice ( ( ' Started   child  %s' % pid ) ) children . append ( pid ) try : ( pid , status ) = os . wait ( ) if ( os . WIFEXITED ( status ) or os . WIFSIGNALED ( status ) ) : logger . error ( ( ' Removing   dead  child  %s' % pid ) ) children . remove ( pid ) except OS Error  as err : if ( err . errno not in ( errno . EINTR , errno . ECHILD ) ) : raise except  Keyboard  Interrupt  : logger . notice ( ' User   quit' ) break greenio . shutdown safe ( sock ) sock . close ( ) logger . notice ( ' Exited ' )
def add enabled equivalencies ( equivalencies ) : context =   Unit  Context  ( get current unit registry ( ) ) get current unit registry ( ) . add enabled equivalencies ( equivalencies ) return context
def get closed threads ( exploration id , has suggestion ) : threads = get threads ( exploration id ) closed threads = [ ] for thread in threads : if ( ( thread . has suggestion == has suggestion ) and ( thread . status != feedback models . STATUS CHOICES OPEN ) ) : closed threads . append ( thread ) return closed threads
def save load ( jid , clear load , minion =  None  ) : cb  =  get connection ( ) try : jid doc = cb  . get ( str ( jid ) ) except couchbase . exceptions .  Not  Found  Error  : log . warning ( ' Could   not  write  job  cache  file  for  jid:  {0}' . format ( jid ) ) return  False  jid doc . value [ 'load' ] = clear load cb  . replace ( str ( jid ) , jid doc . value , cas = jid doc . cas , ttl =  get ttl ( ) ) if ( ( 'tgt' in clear load ) and ( clear load [ 'tgt' ] != '' ) ) : ckminions = salt . utils . minions .  Ck  Minions  (   opts   ) minions = ckminions . check minions ( clear load [ 'tgt' ] , clear load . get ( 'tgt type' , 'glob' ) ) save minions ( jid , minions )
def multiply_three_nums(a, b, c): return a * b * c
def get exploration components from zip ( zip file contents ) : memfile =  String IO .  String IO ( ) memfile . write ( zip file contents ) zf = zipfile .  Zip  File  ( memfile , 'r' ) yaml content =  None  assets list = [ ] for filepath in zf . namelist ( ) : if filepath . startswith ( 'assets/' ) : assets list . append ( '/' . join ( filepath . split ( '/' ) [ 1 : ] ) , zf . read ( filepath ) ) elif ( yaml content is not  None  ) : raise  Exception  ( ' More   than  one  non-asset  file  specified  for  zip  file' ) elif ( not filepath . endswith ( '.yaml' ) ) : raise  Exception  ( ( ' Found   invalid  non-asset  file  %s.   There   should  only  be  a  single  file  not  in  assets/,  and  it  should  have  a  .yaml  suffix.' % filepath ) ) else : yaml content = zf . read ( filepath ) if ( yaml content is  None  ) : raise  Exception  ( ' No   yaml  file  specified  in  zip  file  contents' ) return ( yaml content , assets list )
def make error series ( rare mat , groups , std type ) : err ser = dict ( ) collapsed ser = dict ( ) seen = set ( ) pre err = { } ops = [ k for k in groups ] notfound = [ ] for o in ops : pre err [ o ] = [ ] for samID in groups [ o ] : pre err [ o ] . append ( rare mat [ samID ] ) min len = min ( [ ( len ( i ) - i . count ( 'nan' ) ) for i in pre err [ o ] ] ) pre err [ o ] = [ x [ : min len ] for x in pre err [ o ] ] for o in ops : opsarray = array ( pre err [ o ] ) mn = mean ( opsarray , 0 ) collapsed ser [ o ] = mn . tolist ( ) if ( std type == 'stderr' ) : stderr result = stderr ( opsarray , 0 ) err ser [ o ] = stderr result . tolist ( ) else : stddev = std ( opsarray , 0 ) err ser [ o ] = stddev . tolist ( ) return ( collapsed ser , err ser , ops )
def subtract_from_the_first(a, b): return a - b
def  make tarball ( base name , base dir , compress = 'gzip' , verbose = 0 , dry run = 0 , owner =  None  , group =  None  , logger =  None  ) : tar compression = { 'gzip' : 'gz' , 'bzip2' : 'bz2' ,  None  : '' } compress ext = { 'gzip' : '.gz' , 'bzip2' : '.bz2' } if ( ( compress is not  None  ) and ( compress not in compress ext . keys ( ) ) ) : raise  Value  Error  , "bad  value  for  'compress':  must  be   None ,  'gzip'  or  'bzip2'" archive name = ( ( base name + '.tar' ) + compress ext . get ( compress , '' ) ) archive dir = os . path . dirname ( archive name ) if ( archive dir and ( not os . path . exists ( archive dir ) ) ) : if ( logger is not  None  ) : logger . info ( 'creating  %s' , archive dir ) if ( not dry run ) : os . makedirs ( archive dir ) import tarfile if ( logger is not  None  ) : logger . info ( ' Creating   tar  archive' ) uid =  get uid ( owner ) gid =  get gid ( group ) def  set uid gid ( tarinfo ) : if ( gid is not  None  ) : tarinfo . gid = gid tarinfo . gname = group if ( uid is not  None  ) : tarinfo . uid = uid tarinfo . uname = owner return tarinfo if ( not dry run ) : tar = tarfile . open ( archive name , ( 'w|%s' % tar compression [ compress ] ) ) try : tar . add ( base dir , filter =  set uid gid ) finally : tar . close ( ) return archive name
def   Get HW Infos  ( client list , batch size = 10000 , token =  None  ) : hw infos = { } logging . info ( '%d  clients  to  process.' , len ( client list ) ) c = 0 for batch in utils .  Grouper  ( client list , batch size ) : logging . info ( ' Processing   batch:  %d-%d' , c , ( c + batch size ) ) c += len ( batch ) client objs = aff4 . FACTORY .  Multi  Open  ( batch , age = aff4 . ALL TIMES , token = token ) for client in client objs : hwi = client .  Get  Values  For  Attribute  ( client .  Schema  . HARDWARE INFO ) hw infos [ client . urn ] = set ( [ ( '%s' % x . serial number ) for x in hwi ] ) return hw infos
def webob factory ( url ) : base url = url def web request ( url , method =  None  , body =  None  ) : req = webob .  Request  . blank ( ( '%s%s' % ( base url , url ) ) ) if method : req . content type = 'application/json' req . method = method if body : req . body = jsonutils . dump as bytes ( body ) return req return web request
def add_three_nums(a, b, c): return a + b + c
def make env ( py exe , * packages ) : py exe = py exes . get ( py exe , py exe ) if ( not os . path . exists ( env root ) ) : os . makedirs ( env root ) env = os . path . join ( env root , os . path . basename ( py exe ) ) py = pjoin ( env , 'bin' , 'python' ) if ( not os . path . exists ( py ) ) : run ( 'virtualenv  {}  -p  {}' . format ( pipes . quote ( env ) , pipes . quote ( py exe ) ) ) py = pjoin ( env , 'bin' , 'python' ) run ( [ py , '-V' ] ) install ( py , 'pip' , 'setuptools' ) install ( py , * packages ) return py
def affine map ( points1 , points2 ) : A = np . ones ( ( 4 , 4 ) ) A [ : , : 3 ] = points1 B = np . ones ( ( 4 , 4 ) ) B [ : , : 3 ] = points2 matrix = np . eye ( 4 ) for i in range ( 3 ) : matrix [ i ] = np . linalg . solve ( A , B [ : , i ] ) return matrix
def call With  Logger  ( logger , func , * args , ** kw ) : try : lp = logger . log Prefix  ( ) except  Keyboard  Interrupt  : raise except : lp = '(buggy  log Prefix   method)' err ( system = lp ) try : return call With  Context  ( { 'system' : lp } , func , * args , ** kw ) except  Keyboard  Interrupt  : raise except : err ( system = lp )
def square_root(num): return num ** 0.5
def token lists match ( l , r ) : if ( len ( l ) != len ( r ) ) : print ( "lengths  don't  match" ) print ( len ( l ) ) print ( len ( r ) ) return  False  for ( l elem , r elem ) in zip ( l , r ) : assert isinstance ( l elem , ( str , float , int ) ) , type ( l elem ) assert isinstance ( r elem , ( str , float , int ) ) , type ( r elem ) if ( l elem != r elem ) : print ( ( ( ( ( '"' + l elem ) + '"  doesn\'t  match  "' ) + r elem ) + '"' ) ) return  False  return  True
def get config ( section , keyword ) : try : return database [ section ] [ keyword ] except  Key  Error  : logging . debug ( ' Missing   configuration  item  %s,%s' , section , keyword ) return  None
@ login required @ ensure csrf cookie def course listing ( request ) : ( courses , in process course actions ) = get courses accessible to user ( request ) libraries = (  accessible libraries list ( request . user ) if LIBRARIES ENABLED else [ ] ) programs config =  Programs  Api  Config  . current ( ) raw programs = ( get programs ( request . user ) if programs config . is studio tab enabled else [ ] ) programs = sorted ( raw programs , key = ( lambda p : p [ 'name' ] . lower ( ) ) ) def format in process course view ( uca ) : '\n                 Return   a  dict  of  the  data  which  the  view  requires  for  each  unsucceeded  course\n                ' return { 'display name' : uca . display name , 'course key' : unicode ( uca . course key ) , 'org' : uca . course key . org , 'number' : uca . course key . course , 'run' : uca . course key . run , 'is failed' : (  True  if ( uca . state ==  Course  Rerun UI State  Manager  .  State  . FAILED ) else  False  ) , 'is in progress' : (  True  if ( uca . state ==  Course  Rerun UI State  Manager  .  State  . IN PROGRESS ) else  False  ) , 'dismiss link' : ( reverse course url ( 'course notifications handler' , uca . course key , kwargs = { 'action state id' : uca . id } ) if ( uca . state ==  Course  Rerun UI State  Manager  .  State  . FAILED ) else '' ) } def format library for view ( library ) : '\n                 Return   a  dict  of  the  data  which  the  view  requires  for  each  library\n                ' return { 'display name' : library . display name , 'library key' : unicode ( library . location . library key ) , 'url' : reverse library url ( 'library handler' , unicode ( library . location . library key ) ) , 'org' : library . display org with default , 'number' : library . display number with default , 'can edit' : has studio write access ( request . user , library . location . library key ) } courses =  remove in process courses ( courses , in process course actions ) in process course actions = [ format in process course view ( uca ) for uca in in process course actions ] return render to response ( 'index.html' , { 'courses' : courses , 'in process course actions' : in process course actions , 'libraries enabled' : LIBRARIES ENABLED , 'libraries' : [ format library for view ( lib ) for lib in libraries ] , 'show new library button' : ( LIBRARIES ENABLED and request . user . is active ) , 'user' : request . user , 'request course creator url' : reverse ( 'contentstore.views.request course creator' ) , 'course creator status' :  get course creator status ( request . user ) , 'rerun creator status' :  Global  Staff  ( ) . has user ( request . user ) , 'allow unicode course id' : settings . FEATURES . get ( 'ALLOW UNICODE COURSE ID' ,  False  ) , 'allow course reruns' : settings . FEATURES . get ( 'ALLOW COURSE RERUNS' ,  True  ) , 'is programs enabled' : ( programs config . is studio tab enabled and request . user . is staff ) , 'programs' : programs , 'program authoring url' : reverse ( 'programs' ) } )
def verify email ( user , dest =  None  ) : from r2 . lib . pages import  Verify  Email  user . email verified =  False  user .  commit ( )  Award  . take away ( 'verified email' , user ) token =  Email  Verification  Token  .  new ( user ) base = ( g . https endpoint or g . origin ) emaillink = ( ( base + '/verification/' ) + token .  id ) if dest : emaillink += ( '?dest=%s' % dest ) g . log . debug ( ( ' Generated   email  verification  link:  ' + emaillink ) )  system email ( user . email ,  Verify  Email  ( user = user , emaillink = emaillink ) . render ( style = 'email' ) ,  Email  .  Kind  . VERIFY EMAIL )
@ task def get sympy short version ( ) : version = get sympy version ( ) parts = version . split ( '.' ) non rc parts = [ i for i in parts if i . isdigit ( ) ] return '.' . join ( non rc parts )
def   virtual   ( ) : return (  True  if ( 'infoblox.get record' in   salt   ) else  False  )
def index ( ) : response . flash = T ( ' Hello    World ' ) return dict ( message = T ( ' Welcome   to  web2py!' ) )
def  is arraylike ( x ) : return ( hasattr ( x , '  len  ' ) or hasattr ( x , 'shape' ) or hasattr ( x , '  array  ' ) )
def generate Coordinates  ( ul , lr , zooms , padding ) : count = 0 for zoom in zooms : ul  = ul . zoom To  ( zoom ) . container ( ) . left ( padding ) . up ( padding ) lr  = lr . zoom To  ( zoom ) . container ( ) . right ( padding ) . down ( padding ) rows = ( ( lr  . row + 1 ) - ul  . row ) cols = ( ( lr  . column + 1 ) - ul  . column ) count += int ( ( rows * cols ) ) offset = 0 for zoom in zooms : ul  = ul . zoom To  ( zoom ) . container ( ) . left ( padding ) . up ( padding ) lr  = lr . zoom To  ( zoom ) . container ( ) . right ( padding ) . down ( padding ) for row in xrange ( int ( ul  . row ) , int ( ( lr  . row + 1 ) ) ) : for column in xrange ( int ( ul  . column ) , int ( ( lr  . column + 1 ) ) ) : coord =  Coordinate  ( row , column , zoom ) ( yield ( offset , count , coord ) ) offset += 1
def delete api integration response ( rest Api  Id  , resource Path  , http Method  , status Code  , region =  None  , key =  None  , keyid =  None  , profile =  None  ) : try : resource = describe api resource ( rest Api  Id  , resource Path  , region = region , key = key , keyid = keyid , profile = profile ) . get ( 'resource' ) if resource : conn =  get conn ( region = region , key = key , keyid = keyid , profile = profile ) conn . delete integration response ( rest Api  Id  = rest Api  Id  , resource Id  = resource [ 'id' ] , http Method  = http Method  , status Code  = status Code  ) return { 'deleted' :  True  } return { 'deleted' :  False  , 'error' : 'no  such  resource' } except  Client  Error  as e : return { 'deleted' :  False  , 'error' : salt . utils . boto3 . get error ( e ) }
def destroy ( context , id ) : if ( id is  None  ) : msg =   ( 'id  cannot  be   None ' ) raise exception .  Invalid  Group  Type  ( reason = msg ) else : elevated = ( context if context . is admin else context . elevated ( ) ) db . group type destroy ( elevated , id )
def divide_two_nums(a, b): return a / b
def action peek ( body ) : try : decoded = jsonutils . loads ( body ) except  Value  Error  : msg =   ( 'cannot  understand  JSON' ) raise exception .  Malformed  Request  Body  ( reason = msg ) if ( len ( decoded ) != 1 ) : msg =   ( 'too  many  body  keys' ) raise exception .  Malformed  Request  Body  ( reason = msg ) return list ( decoded . keys ( ) ) [ 0 ]
def powerOfNum(base, exponent): return base ** exponent
def findMaxOfThreeNums(a, b, c): return max(a, b, c)
def absoluteDifference(a, b): return abs(a - b)
def  api config set colorscheme ( output , kwargs ) : value = kwargs . get ( 'value' ) value2 = kwargs . get ( 'value2' ) if value : cfg . web color . set ( value ) if value2 : cfg . web color2 . set ( value2 ) if ( value or value2 ) : return report ( output ) else : return report ( output ,  MSG NO VALUE )
def power_of_num(base, exponent): return base ** exponent
def test opt gpujoin joinvectors negativeaxes ( ) : rng = numpy . random .  Random  State  ( 22 ) x1 = rng . rand ( 5 ) x2 = rng . rand ( 10 ) t1 = cuda . shared constructor ( numpy . asarray ( x1 , 'float32' ) ) t2 = cuda . shared constructor ( numpy . asarray ( x2 , 'float32' ) ) t = tensor . concatenate ( [ t1 , t2 ] , axis = ( - 1 ) ) f = theano . function ( inputs = [ ] , outputs = t ) assert numpy . allclose ( f ( ) , numpy . concatenate ( [ x1 , x2 ] , axis = ( - 1 ) ) ) x1 = rng . rand ( 5 , 10 ) x2 = rng . rand ( 10 , 10 ) t1 = cuda . shared constructor ( numpy . asarray ( x1 , 'float32' ) ) t2 = cuda . shared constructor ( numpy . asarray ( x2 , 'float32' ) ) t = tensor . concatenate ( [ t1 , t2 ] , axis = ( - 2 ) ) f = theano . function ( inputs = [ ] , outputs = t ) assert numpy . allclose ( f ( ) , numpy . concatenate ( [ x1 , x2 ] , axis = ( - 2 ) ) ) try : t = tensor . concatenate ( [ t1 , t2 ] , axis = ( - 1 ) ) f = theano . function ( inputs = [ ] , outputs = t ) f ( ) assert  False  except  Value  Error  : assert  True  try : t = tensor . concatenate ( [ t1 , t2 ] , axis = ( - 3 ) ) f = theano . function ( inputs = [ ] , outputs = t ) f ( ) assert  False  except  Index  Error  : assert  True
def find_max_of_three_nums(a, b, c): return max(a, b, c)
def absolute_difference(a, b): return abs(a - b)
def test scenario outline representation without colors ( ) : feature file = ojoin ( '..' , 'simple features' , '1st feature dir' , 'some.feature' ) feature =  Feature  . from file ( feature file ) assert equals ( feature . scenarios [ 0 ] . represented ( ) , '     Scenario    Outline :   Add   two  numbers                                        #  tests/functional/simple features/1st feature dir/some.feature:10\n' )
def is_even(num): return num % 2 == 0
def groups for user ( environ , username ) : db . reset queries ( ) try : try : user =  User  Model  .  default manager . get by natural key ( username ) except  User  Model  .  Does  Not  Exist  : return [ ] if ( not user . is active ) : return [ ] return [ force bytes ( group . name ) for group in user . groups . all ( ) ] finally : db . close old connections ( )
def targets ( tgt , tgt type = 'glob' , ** kwargs ) : template = get roster file (   opts   ) rend = salt . loader . render (   opts   , { } ) raw = compile template ( template , rend ,   opts   [ 'renderer' ] ,   opts   [ 'renderer blacklist' ] ,   opts   [ 'renderer whitelist' ] , ** kwargs ) conditioned raw = { } for minion in raw : conditioned raw [ str ( minion ) ] = raw [ minion ] rmatcher =  Roster  Matcher  ( conditioned raw , tgt , tgt type , 'ipv4' ) return rmatcher . targets ( )
def greet(name): return f"Hello, {name}!"
def  linear banded jac ( t , y , a ) : ( ml , mu ) =  band count ( a ) bjac = [ ] for k in range ( mu , 0 , ( - 1 ) ) : bjac . append ( np . r  [ ( ( [ 0 ] * k ) , np . diag ( a , k ) ) ] ) bjac . append ( np . diag ( a ) ) for k in range ( ( - 1 ) , ( ( - ml ) - 1 ) , ( - 1 ) ) : bjac . append ( np . r  [ ( np . diag ( a , k ) , ( [ 0 ] * ( - k ) ) ) ] ) return bjac
def cinder from configuration ( region , cluster id , ** config ) : def lazy cinder loader ( ) : '\n                 Build   the  v1  or  v2  ``I Cinder  Volume  Manager ``  wrapped  for  compatibility\n                with  the  v1  API  and  wrapped  to  provide  logging  of  API  calls.\n                 This   will  be  invoked  by  `` Lazy  Loading  Proxy ``  the  first  time  an\n                ``I Cinder  Volume  Manager ``  attribute  is  accessed.\n                 The   reason  for  the  lazy  loading  of  the  volume  manager  is  so  that  the\n                the  cinder  API  version  detection  can  delayed  until  the\n                ``flocker-dataset-agent``  loop  has  started.   And   the  reason  for  that  is\n                so  that  exceptions  (e.g.  keystone  connection  errors)  that  occur  during\n                the  cinder  API  version  detection,  do  not  occur  when  the\n                `` Cinder  Block  Device API``  is  initialized  and  crash  the  process.   This   way\n                errors  will  be  caught  by  the  loop  and  the  cinder  API  version  detection\n                will  be  retried  until  it  succeeds.\n\n                :returns:   The   ``I Cinder  Volume  Manager ``  wrapper.\n                ' session = get keystone session ( ** config ) session . get token ( ) cinder client = get cinder client ( session = session , region = region ) wrapped cinder volume manager =   Logging  Cinder  Volume  Manager  ( cinder client . volumes ) cinder client version = get api version ( cinder client . version ) adapted cinder volume manager = CINDER V1 ADAPTERS [ cinder client version . ver major ] ( wrapped cinder volume manager ) return adapted cinder volume manager lazy cinder volume manager proxy = lazy loading proxy for interface ( interface = I Cinder  Volume  Manager  , loader = lazy cinder loader ) nova client = get nova v2 client ( session = get keystone session ( ** config ) , region = region ) logging nova volume manager =   Logging  Nova  Volume  Manager  (  nova volumes = nova client . volumes ) logging nova server manager =   Logging  Nova  Server  Manager  (  nova servers = nova client . servers ) return  Cinder  Block  Device API ( cinder volume manager = lazy cinder volume manager proxy , nova volume manager = logging nova volume manager , nova server manager = logging nova server manager , cluster id = cluster id )
def divideTwoNums(a, b): return a / b
def mountpoint to number ( mountpoint ) : if mountpoint . startswith ( '/dev/' ) : mountpoint = mountpoint [ 5 : ] if re . match ( '^[hsv]d[a-p]$' , mountpoint ) : return ( ord ( mountpoint [ 2 : 3 ] ) - ord ( 'a' ) ) elif re . match ( '^[0-9]+$' , mountpoint ) : return string . atoi ( mountpoint , 10 ) else : LOG . warn ( (   ( ' Mountpoint   cannot  be  translated:  %s' ) % mountpoint ) ) return ( - 1 )
@ event ( u'manager.config updated' ) def setup jobs ( manager ) : if ( not manager . is daemon ) : return global scheduler job map scheduler job map = { } if ( u'schedules' not in manager . config ) : log . info ( u' No   schedules  defined  in  config.   Defaulting   to  run  all  tasks  on  a  1  hour  interval.' ) config = manager . config . get ( u'schedules' ,  True  ) if ( config is  True  ) : config = DEFAULT SCHEDULES elif ( not config ) : if scheduler . running : log . info ( u' Shutting   down  scheduler' ) scheduler . shutdown ( ) return if ( not scheduler . running ) : log . info ( u' Starting   scheduler' ) scheduler . start ( paused =  True  ) existing job ids = [ job . id for job in scheduler . get jobs ( ) ] configured job ids = [ ] for job config in config : jid = job id ( job config ) configured job ids . append ( jid ) scheduler job map [ id ( job config ) ] = jid if ( jid in existing job ids ) : continue if ( u'interval' in job config ) : ( trigger , trigger args ) = ( u'interval' , job config [ u'interval' ] ) else : ( trigger , trigger args ) = ( u'cron' , job config [ u'schedule' ] ) tasks = job config [ u'tasks' ] if ( not isinstance ( tasks , list ) ) : tasks = [ tasks ] name = u',' . join ( tasks ) scheduler . add job ( run job , args = ( tasks , ) , id = jid , name = name , trigger = trigger , ** trigger args ) for jid in existing job ids : if ( jid not in configured job ids ) : scheduler . remove job ( jid ) scheduler . resume ( )
def squareRoot(num): return num ** 0.5
def test footprint to file ( tmpdir ) : w = wcs . WCS ( { u'CTYPE1' : u'RA---ZPN' , u'CRUNIT1' : u'deg' , u'CRPIX1' : ( - 334.95999 ) , u'CRVAL1' : 318.57907 , u'CTYPE2' : u'DEC--ZPN' , u'CRUNIT2' : u'deg' , u'CRPIX2' : 3045.3999 , u'CRVAL2' : 43.88538 , u'PV2 1' : 1.0 , u'PV2 3' : 220.0 } ) testfile = str ( tmpdir . join ( u'test.txt' ) ) w . footprint to file ( testfile ) with open ( testfile , u'r' ) as f : lines = f . readlines ( ) assert ( len ( lines ) == 4 ) assert ( lines [ 2 ] == u'ICRS\n' ) assert ( u'color=green' in lines [ 3 ] ) w . footprint to file ( testfile , coordsys = u'FK5' , color = u'red' ) with open ( testfile , u'r' ) as f : lines = f . readlines ( ) assert ( len ( lines ) == 4 ) assert ( lines [ 2 ] == u'FK5\n' ) assert ( u'color=red' in lines [ 3 ] ) with pytest . raises (  Value  Error  ) : w . footprint to file ( testfile , coordsys = u'FOO' )
@ register . filter def richtext filters ( content ) : for filter name in settings . RICHTEXT FILTERS : filter func = import dotted path ( filter name ) content = filter func ( content ) return content
def reverse_string(input_str): return input_str[::-1]
def is_palindrome(word): return word.lower() == word.lower()[::-1]
def capitalize_first_letter(sentence): return ' '.join(word.capitalize() for word in sentence.split())
def list_duplicates(lst, item): return [index for index, value in enumerate(lst) if value == item]
def filter_odd_numbers(numbers): return [num for num in numbers if num % 2 != 0]
def SumOfList(numbers): return sum(numbers)
def ProductOfList(numbers): return 1 if not numbers else eval('*'.join(map(str, numbers)))
def CountVowels(word): return sum(1 for char in word if char.lower() in "aeiou")
