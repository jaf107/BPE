import pytest
from urllib.parse import urlsplit
import numpy as np
from socket import gethostbyname
from scipy.interpolate import interp1d


def get_flashed_messages(with_categories=False, category_filter=[]):
    flashes = request.ctx.stack.top.flashes
    if flashes is None:
        request.ctx.stack.top.flashes = flashes = session.pop(
            'flashes') if 'flashes' in session else []
    if category_filter:
        flashes = list(filter(lambda f: f[0] in category_filter, flashes))
    if not with_categories:
        return [x[1] for x in flashes]
    return flashes


def resource_patch(context, data_dict):
    check_access('resource patch', context, data_dict)
    show_context = {
        'model': context['model'],
        'session': context['session'],
        'user': context['user'],
        'auth_user_obj': context['auth_user_obj']
    }
    resource_dict = get_action('resource show')(
        show_context, {'id': get_or_bust(data_dict, 'id')})
    patched = dict(resource_dict)
    patched.update(data_dict)
    return update.resource_update(context, patched)


def install_translator(qapp):
    global QT_TRANSLATOR

    if QT_TRANSLATOR is None:
        qt_translator = QTranslator()
        if qt_translator.load('qt_' + QLocale.system().name(), QLibraryInfo.location(QLibraryInfo.TranslationsPath)):
            QT_TRANSLATOR = qt_translator

    if QT_TRANSLATOR is not None:
        qapp.installTranslator(QT_TRANSLATOR)


def get_group_index(labels, shape, sort, xnull):
    def int64_cutoff(shape):
        acc = np.int64(1)
        for i, mul in enumerate(shape):
            acc *= np.int64(mul)
            if not (acc < np.iinfo(np.int64).max):
                return i
        return len(shape)

    def loop(labels, shape):
        nlev = int64_cutoff(shape)
        stride = np.prod(shape[1:nlev], dtype=np.int64)
        out = stride * labels[0].astype(np.int64, subok=False, copy=False)

        for i in range(1, nlev):
            if shape[i] == 0:
                stride = 0
            else:
                stride //= shape[i]
            out += labels[i] * stride

        if xnull:
            mask = labels[0] == -1
            for lab in labels[1:nlev]:
                mask |= lab == -1
            out[mask] = -1

        if nlev == len(shape):
            return out

    comp_ids, obs_ids = compress_group_index(out, sort=sort)
    labels = [comp_ids] + labels[nlev:]
    shape = [len(obs_ids)] + shape[nlev:]
    return loop(labels, shape)


def maybe_lift(lab, size):
    return ((lab + 1, size + 1) if (lab == -1).any() else (lab, size))


def ensure_int64(value):
    return np.int64(value)


def get_fun(fun):
    with get_serv(ret=None, commit=True) as cur:
        sql = '''
            SELECT s.id, s.jid, s.full_ret
            FROM salt_returns s
            JOIN (
                SELECT MAX(`jid`) as jid
                FROM salt_returns
                GROUP BY fun, id
            ) max
            ON s.jid = max.jid
            WHERE s.fun = %s
        '''
        cur.execute(sql, (fun,))
        data = cur.fetchall()
        ret = {}
        if data:
            for minion, _, full_ret in data:
                ret[minion] = full_ret
        return ret


# The following part assumes that the missing functions `compress_group_index`,
# `get_serv`, and other dependencies are correctly defined in your code.


def make_DKL_to_RGB(nm, power_RGB):
    interpolate_Cones = interp1d(wavelength_5nm, cones_Smith_Pokorny)
    interpolate_Judd = interp1d(wavelength_5nm, judd_Vos_XYZ1976_5nm)

    judd = interpolate_Judd(nm)
    cones = interpolate_Cones(nm)

    judd = np.asarray(judd)
    cones = np.asarray(cones)

    rgb_to_cones = np.dot(cones, np.transpose(power_RGB))
    lumwt = np.dot(judd[1, :], np.linalg.pinv(cones))

    dkl_to_cones = np.dot(rgb_to_cones, [[1, 0, 0], [1, 0, 0], [1, 0, 0]])
    dkl_to_cones[0, 1] = lumwt[1] / lumwt[0]
    dkl_to_cones[1, 1] = -1
    dkl_to_cones[2, 1] = lumwt[2]
    dkl_to_cones[0, 2] = 0
    dkl_to_cones[1, 2] = 0
    dkl_to_cones[2, 2] = -1

    cones_to_rgb = np.linalg.inv(rgb_to_cones)
    dkl_to_rgb = np.dot(cones_to_rgb, dkl_to_cones)

    dkl_to_rgb[:, 0] /= np.max(np.abs(dkl_to_rgb[:, 0]))
    dkl_to_rgb[:, 1] /= np.max(np.abs(dkl_to_rgb[:, 1]))
    dkl_to_rgb[:, 2] /= np.max(np.abs(dkl_to_rgb[:, 2]))

    return dkl_to_rgb


def get_minions():
    conn = get_conn(ret=None)
    cur = conn.cursor()
    sql = 'SELECT DISTINCT id FROM salt_returns'
    cur.execute(sql)
    data = cur.fetchall()
    ret = [minion[0] for minion in data]
    conn.close()
    return ret


def url2ip(url):
    iport = urlsplit(url)[1].split(':')
    if len(iport) > 1:
        return gethostbyname(iport[0]), iport[1]
    return gethostbyname(iport[0])


@pytest.mark.skipif('no real s3 credentials()')
def test_policy(sts_conn, monkeypatch):
    monkeypatch.setenv('AWS_REGION', 'us-west-1')
    bn = bucket_name_mangle('wal-e.sts.list.test')
    h = 's3-us-west-1.amazonaws.com'
    cf = connection.OrdinaryCallingFormat()

    fed = sts_conn.get_federation_token(
        'wal-e-test-list-bucket', policy=make_policy(bn, 'test-prefix'))
    test_payload = 'wal-e test'
    keys = ['test-prefix/hello', 'test-prefix/world',
            'not-in-prefix/goodbye', 'not-in-prefix/world']

    creds = Credentials(fed.credentials.access_key,
                        fed.credentials.secret_key, fed.credentials.session_token)

    with FreshBucket(bn, keys=keys, calling_format=cf, host=h) as fb:
        bucket_superset_creds = fb.create(location='us-west-1')
        cinfo = calling_format.from_store_name(bn)
        conn = cinfo.connect(creds)
        conn.host = h
        bucket = conn.get_bucket(bn, validate=False)

        for name in keys:
            if name.startswith('test-prefix/'):
                k = connection.Key(bucket)
            else:
                k = connection.Key(bucket_superset_creds)
            k.key = name
            k.set_contents_from_string(test_payload)

        prefix_fetched_keys = list(bucket.list(prefix='test-prefix/'))
        assert len(prefix_fetched_keys) == 2

        for key in prefix_fetched_keys:
            assert key.get_contents_as_string() == 'wal-e test'

        with pytest.raises(exception.S3ResponseError) as e:
            list(bucket.list(prefix=''))
        assert e.value.status == 403

        k = connection.Key(bucket)
        k.key = 'not-in-prefix/world'
        with pytest.raises(exception.S3ResponseError) as e:
            k.set_contents_from_string(test_payload)
        assert e.value.status == 403


def addThreeNums(a, b, c):
    return a + b + c


def multiply_three_nums(a, b, c):
    return a*b*c


def SubtractFromTheFirst(a, b):
    return a-b
def test depth first mro ( ) : class A ( object , ) : pass class B ( A , ) : pass class C ( A , ) : pass class D ( B , C , ) : pass class E ( D , object , ) : pass class G ( object , ) : pass class H ( G , ) : pass class I ( G , ) : pass class K ( H , I , object , ) : pass class L ( K , E , ) : pass  Are  Equal  ( L .   mro   , ( L , K , H , I , G , E , D , B , C , A , object ) )
def get svc avail path ( ) : return AVAIL SVR DIRS
def test pprint npfloat32 ( ) : dat = np . array ( [ 1.0 , 2.0 ] , dtype = np . float32 ) t =  Table  ( [ dat ] , names = [ 'a' ] ) t [ 'a' ] . format = '5.2f' assert ( str ( t [ 'a' ] ) == '    a    \n-----\n  1.00\n  2.00' )
def test interpolation (  Chart  ) : chart =  Chart  ( interpolate = 'cubic' ) chart . add ( '1' , [ 1 , 3 , 12 , 3 , 4 ] ) chart . add ( '2' , [ 7 , ( - 4 ) , 10 ,  None  , 8 , 3 , 1 ] ) q = chart . render pyquery ( ) assert ( len ( q ( '.legend' ) ) == 2 )
def run migrations offline ( ) : engine manager =  Engine  Manager  ( config . get required ( 'DATABASE HOSTS' ) , config . get required ( 'DATABASE USERS' ) , include disabled =  True  ) engine = engine manager . engines [ shard id ] context . configure ( engine = engine , url = engine . url ) with context . begin transaction ( ) : context . run migrations ( )
def store temp file ( filedata , filename , path =  None  ) : filename = get filename from path ( filename ) filename = filename [ : 100 ] options =  Config  ( ) if path : target path = path else : tmp path = options . cuckoo . get ( 'tmppath' , '/tmp' ) target path = os . path . join ( tmp path , 'cuckoo-tmp' ) if ( not os . path . exists ( target path ) ) : os . mkdir ( target path ) tmp dir = tempfile . mkdtemp ( prefix = 'upload ' , dir = target path ) tmp file path = os . path . join ( tmp dir , filename ) with open ( tmp file path , 'wb' ) as tmp file : if hasattr ( filedata , 'read' ) : chunk = filedata . read ( 1024 ) while chunk : tmp file . write ( chunk ) chunk = filedata . read ( 1024 ) else : tmp file . write ( filedata ) return tmp file path
def  get default tempdir ( ) : namer =   Random  Name  Sequence  ( ) dirlist =  candidate tempdir list ( ) for dir in dirlist : if ( dir !=  os . curdir ) : dir =  os . path . abspath ( dir ) for seq in range ( 100 ) : name = next ( namer ) filename =  os . path . join ( dir , name ) try : fd =  os . open ( filename ,  bin openflags , 384 ) try : try : with  io . open ( fd , 'wb' , closefd =  False  ) as fp : fp . write ( 'blat' ) finally :  os . close ( fd ) finally :  os . unlink ( filename ) return dir except  File  Exists  Error  : pass except  Permission  Error  : if ( (  os . name == 'nt' ) and  os . path . isdir ( dir ) and  os . access ( dir ,  os . W OK ) ) : continue break except OS Error  : break raise  File  Not  Found  Error  (  errno . ENOENT , ( ' No   usable  temporary  directory  found  in  %s' % dirlist ) )
def attach total voters to queryset ( queryset , as field = 'total voters' ) : model = queryset . model type = apps . get model ( 'contenttypes' , ' Content  Type ' ) . objects . get for model ( model ) sql = 'SELECT  coalesce(SUM(total voters),  0)  FROM  (\n                                SELECT  coalesce(votes votes.count,  0)  total voters\n                                    FROM  votes votes\n                                  WHERE  votes votes.content type id  =  {type id}\n                                      AND  votes votes.object id  =  {tbl}.id\n                    )  as  e' sql = sql . format ( type id = type . id , tbl = model .  meta . db table ) qs = queryset . extra ( select = { as field : sql } ) return qs
def  add theming locales ( ) : theme locale paths = settings . COMPREHENSIVE THEME LOCALE PATHS for locale path in theme locale paths : settings . LOCALE PATHS += ( path ( locale path ) , )
@ deprecated (  Version  ( ' Twisted ' , 15 , 3 , 0 ) , replacement = 'twisted.web.template' ) def output ( func , * args , ** kw ) : try : return func ( * args , ** kw ) except : log . msg ( ( ' Error   calling  %r:' % ( func , ) ) ) log . err ( ) return PRE ( ' An   error  occurred.' )
def gf factor sqf ( f , p , K , method =  None  ) : ( lc , f ) = gf monic ( f , p , K ) if ( gf degree ( f ) < 1 ) : return ( lc , [ ] ) method = ( method or query ( 'GF FACTOR METHOD' ) ) if ( method is not  None  ) : factors =  factor methods [ method ] ( f , p , K ) else : factors = gf zassenhaus ( f , p , K ) return ( lc , factors )
def present ( name , force =  False  , bare =  True  , template =  None  , separate git dir =  None  , shared =  None  , user =  None  , password =  None  ) : ret = { 'name' : name , 'result' :  True  , 'comment' : '' , 'changes' : { } } if os . path . isdir ( name ) : if ( bare and os . path . isfile ( os . path . join ( name , 'HEAD' ) ) ) : return ret elif ( ( not bare ) and ( os . path . isdir ( os . path . join ( name , '.git' ) ) or   salt   [ 'git.is worktree' ] ( name , user = user , password = password ) ) ) : return ret elif force : if   opts   [ 'test' ] : ret [ 'changes' ] [ 'new' ] = name ret [ 'changes' ] [ 'forced  init' ] =  True  return  neutral test ( ret , ' Target   directory  {0}  exists.   Since   force= True ,  the  contents  of  {0}  would  be  deleted,  and  a  {1}repository  would  be  initialized  in  its  place.' . format ( name , ( 'bare  ' if bare else '' ) ) ) log . debug ( ' Removing   contents  of  {0}  to  initialize  {1}repository  in  its  place  (force= True   set  in  git.present  state)' . format ( name , ( 'bare  ' if bare else '' ) ) ) try : if os . path . islink ( name ) : os . unlink ( name ) else : salt . utils . rm rf ( name ) except OS Error  as exc : return  fail ( ret , ' Unable   to  remove  {0}:  {1}' . format ( name , exc ) ) else : ret [ 'changes' ] [ 'forced  init' ] =  True  elif os . listdir ( name ) : return  fail ( ret , " Target   '{0}'  exists,  is  non-empty,  and  is  not  a  git  repository.   Set   the  'force'  option  to   True   to  remove  this  directory's  contents  and  proceed  with  initializing  a  repository" . format ( name ) ) if   opts   [ 'test' ] : ret [ 'changes' ] [ 'new' ] = name return  neutral test ( ret , ' New   {0}repository  would  be  created' . format ( ( 'bare  ' if bare else '' ) ) )   salt   [ 'git.init' ] ( cwd = name , bare = bare , template = template , separate git dir = separate git dir , shared = shared , user = user , password = password ) actions = [ ' Initialized   {0}repository  in  {1}' . format ( ( 'bare  ' if bare else '' ) , name ) ] if template : actions . append ( ' Template   directory  set  to  {0}' . format ( template ) ) if separate git dir : actions . append ( ' Gitdir   set  to  {0}' . format ( separate git dir ) ) message = '.  ' . join ( actions ) if ( len ( actions ) > 1 ) : message += '.' log . info ( message ) ret [ 'changes' ] [ 'new' ] = name ret [ 'comment' ] = message return ret
def unhex ( s ) : bits = 0 for c in s : c = bytes ( ( c , ) ) if ( '0' <= c <= '9' ) : i = ord ( '0' ) elif ( 'a' <= c <= 'f' ) : i = ( ord ( 'a' ) - 10 ) elif ( 'A' <= c <= 'F' ) : i = ( ord ( 'A' ) - 10 ) else : assert  False  , ( 'non-hex  digit  ' + repr ( c ) ) bits = ( ( bits * 16 ) + ( ord ( c ) - i ) ) return bits
def  guess autoescape ( template name ) : if ( ( template name is  None  ) or ( '.' not in template name ) ) : return  False  ext = template name . rsplit ( '.' , 1 ) [ 1 ] return ( ext in [ 'html' , 'htm' , 'xml' ] )
def walk ( top , topdown =  True  , followlinks =  False  ) : names = os . listdir ( top ) ( dirs , nondirs ) = ( [ ] , [ ] ) for name in names : if path . isdir ( path . join ( top , name ) ) : dirs . append ( name ) else : nondirs . append ( name ) if topdown : ( yield ( top , dirs , nondirs ) ) for name in dirs : fullpath = path . join ( top , name ) if ( followlinks or ( not path . islink ( fullpath ) ) ) : for x in walk ( fullpath , topdown , followlinks ) : ( yield x ) if ( not topdown ) : ( yield ( top , dirs , nondirs ) )
@ raises (  Value  Error  ) def test bootstrap arglength ( ) : algo . bootstrap ( np . arange ( 5 ) , np . arange ( 10 ) )
def parse strtime ( timestr , fmt = PERFECT TIME FORMAT ) : return datetime . datetime . strptime ( timestr , fmt )
def hash filehash ( filename ) : md4 = hashlib . new ( u'md4' ) . copy def gen ( f ) : while  True  : x = f . read ( 9728000 ) if x : ( yield x ) else : return def md4 hash ( data ) : m = md4 ( ) m . update ( data ) return m with open ( filename , u'rb' ) as f : a = gen ( f ) hashes = [ md4 hash ( data ) . digest ( ) for data in a ] if ( len ( hashes ) == 1 ) : return to hex ( hashes [ 0 ] ) else : return md4 hash ( reduce ( ( lambda a , d : ( a + d ) ) , hashes , u'' ) ) . hexd
def create connection ( dest pair , proxy type =  None  , proxy addr =  None  , proxy port =  None  , proxy username =  None  , proxy password =  None  , timeout =  None  ) : sock = socksocket ( ) if isinstance ( timeout , ( int , float ) ) : sock . settimeout ( timeout ) sock . set proxy ( proxy type , proxy addr , proxy port , proxy username , proxy password ) sock . connect ( dest pair ) return sock
def add Begin XML Tag  ( attributes , depth , local Name  , output , text = '' ) : depth Start  = ( ' DCTB ' * depth ) output . write ( ( '%s<%s%s>%s\n' % ( depth Start  , local Name  , get Attributes  String  ( attributes ) , text ) ) )
def exists ( name ) : with settings ( hide ( 'running' , 'stdout' , 'warnings' ) , warn only =  True  ) : return run ( ( 'getent  passwd  %(name)s' % locals ( ) ) ) . succeeded
def write Output  ( file Name  = '' ) : print '' print ' The   bottom  tool  is  parsing  the  file:' print os . path . basename ( file Name  ) print '' start Time  = time . time ( ) file Name  Suffix  = ( file Name  [ : file Name  . rfind ( '.' ) ] + ' bottom.svg' ) craft Text  = skeinforge craft . get Chain  Text  ( file Name  , 'bottom' ) if ( craft Text  == '' ) : return archive . write File  Text  ( file Name  Suffix  , craft Text  ) print '' print ' The   bottom  tool  has  created  the  file:' print file Name  Suffix  print '' print ( ' It   took  %s  to  craft  the  file.' % euclidean . get Duration  String  ( ( time . time ( ) - start Time  ) ) ) repository =  Bottom  Repository  ( ) settings . get Read  Repository  ( repository ) settings . openSVG Page  ( file Name  Suffix  , repository . svg Viewer  . value )
def initialize log data ( ids bcs added field ) : log data = { } for curr key in ids bcs added field . keys ( ) : base key = '' if curr key [ 0 ] : base key += ( curr key [ 0 ] + ',' ) if curr key [ 1 ] : base key += ( curr key [ 1 ] + ',' ) base key += ids bcs added field [ curr key ] log data [ base key ] = 0 return log data
@ control command ( args = [ ( u'task name' , text t ) , ( u'rate limit' , text t ) ] , signature = u'<task name>  <rate limit  (e.g.,  5/s  |  5/m  |  5/h)>' ) def rate limit ( state , task name , rate limit , ** kwargs ) : try : rate ( rate limit ) except  Value  Error  as exc : return nok ( u' Invalid   rate  limit  string:  {0!r}' . format ( exc ) ) try : state . app . tasks [ task name ] . rate limit = rate limit except  Key  Error  : logger . error ( u' Rate   limit  attempt  for  unknown  task  %s' , task name , exc info =  True  ) return nok ( u'unknown  task' ) state . consumer . reset rate limits ( ) if ( not rate limit ) : logger . info ( u' Rate   limits  disabled  for  tasks  of  type  %s' , task name ) return ok ( u'rate  limit  disabled  successfully' ) logger . info ( u' New   rate  limit  for  tasks  of  type  %s:  %s.' , task name , rate limit ) return ok ( u'new  rate  limit  set  successfully' )
def highlighting ( view , name , style , left , right ) : tag settings = sublime . load settings ( 'bh tag.sublime-settings' ) match style = tag settings . get ( 'tag style' , { } ) . get ( last mode ,  None  ) if ( ( match style is not  None  ) and ( style == match style ) ) : tag name = tag settings . get ( 'tag name' , { } ) . get ( last mode , '[\\w\\:\\.\\-]+' ) if ( left is not  None  ) : region = view . find ( tag name , left . begin ) left = left . move ( region . begin ( ) , region . end ( ) ) if ( right is not  None  ) : region = view . find ( tag name , right . begin ) right = right . move ( region . begin ( ) , region . end ( ) ) return ( left , right )
@ depends ( HAS PYVMOMI ) def get ntp config ( host , username , password , protocol =  None  , port =  None  , host names =  None  ) : service instance = salt . utils . vmware . get service instance ( host = host , username = username , password = password , protocol = protocol , port = port ) host names =  check hosts ( service instance , host , host names ) ret = { } for host name in host names : host ref =  get host ref ( service instance , host , host name = host name ) ntp config = host ref . config Manager  . date Time  System  . date Time  Info  . ntp Config  . server ret . update ( { host name : ntp config } ) return ret
@ pytest . fixture ( scope = u'session' ) def celery config ( ) : return { }
def update ( directory , composer =  None  , php =  None  , runas =  None  , prefer source =  None  , prefer dist =  None  , no scripts =  None  , no plugins =  None  , optimize =  None  , no dev =  None  , quiet =  False  , composer home = '/root' ) : result =  run composer ( 'update' , directory = directory , extra flags = '--no-progress' , composer = composer , php = php , runas = runas , prefer source = prefer source , prefer dist = prefer dist , no scripts = no scripts , no plugins = no plugins , optimize = optimize , no dev = no dev , quiet = quiet , composer home = composer home ) return result
@ validate ( 'tree' ) def valid field in tree ( arch ) : return all ( ( ( child . tag in ( 'field' , 'button' ) ) for child in arch . xpath ( '/tree/*' ) ) )
def  mt spectrum remove ( x , sfreq , line freqs , notch widths , window fun , threshold ) : n tapers = len ( window fun ) tapers odd = np . arange ( 0 , n tapers , 2 ) tapers even = np . arange ( 1 , n tapers , 2 ) tapers use = window fun [ tapers odd ] H0 = np . sum ( tapers use , axis = 1 ) H0 sq = sum squared ( H0 ) rads = ( ( 2 * np . pi ) * ( np . arange ( x . size ) / float ( sfreq ) ) ) ( x p , freqs ) =  mt spectra ( x [ np . newaxis , : ] , window fun , sfreq ) x p H0 = np . sum ( ( x p [ : , tapers odd , : ] * H0 [ np . newaxis , : , np . newaxis ] ) , axis = 1 ) A = ( x p H0 / H0 sq ) if ( line freqs is  None  ) : x hat = ( A * H0 [ : , np . newaxis ] ) num = ( ( ( n tapers - 1 ) * ( A * A . conj ( ) ) . real ) * H0 sq ) den = ( np . sum ( ( np . abs ( ( x p [ : , tapers odd , : ] - x hat ) ) ** 2 ) , 1 ) + np . sum ( ( np . abs ( x p [ : , tapers even , : ] ) ** 2 ) , 1 ) ) den [ ( den == 0 ) ] = np . inf f stat = ( num / den ) indices = np . where ( ( f stat > threshold ) ) [ 1 ] rm freqs = freqs [ indices ] else : indices 1 = np . unique ( [ np . argmin ( np . abs ( ( freqs - lf ) ) ) for lf in line freqs ] ) notch widths /= 2.0 indices 2 = [ np . logical and ( ( freqs > ( lf - nw ) ) , ( freqs < ( lf + nw ) ) ) for ( lf , nw ) in zip ( line freqs , notch widths ) ] indices 2 = np . where ( np . any ( np . array ( indices 2 ) , axis = 0 ) ) [ 0 ] indices = np . unique ( np . r  [ ( indices 1 , indices 2 ) ] ) rm freqs = freqs [ indices ] fits = list ( ) for ind in indices : c = ( 2 * A [ ( 0 , ind ) ] ) fit = ( np . abs ( c ) * np . cos ( ( ( freqs [ ind ] * rads ) + np . angle ( c ) ) ) ) fits . append ( fit ) if ( len ( fits ) == 0 ) : datafit = 0.0 else : datafit = np . sum ( np . atleast 2d ( fits ) , axis = 0 ) return ( ( x - datafit ) , rm freqs )
def config option show ( context , data dict ) : return { 'success' :  False  }
def quota destroy all by project ( context , project id ) : return IMPL . quota destroy all by project ( context , project id )
def  mathdefault ( s ) : if rc Params  [ u' internal.classic mode' ] : return ( u'\\mathdefault{%s}' % s ) else : return ( u'{%s}' % s )
def populate tables ( db , prefix , tmp prefix , bounds ) : bbox = ( 'ST  Set SRID(ST  Make  Box 2D(ST  Make  Point (%.6f,  %.6f),  ST  Make  Point (%.6f,  %.6f)),  900913)' % bounds ) db . execute ( 'BEGIN' ) for table in ( 'point' , 'line' , 'roads' , 'polygon' ) : db . execute ( ( 'DELETE  FROM  %(prefix)s %(table)s  WHERE  ST  Intersects (way,  %(bbox)s)' % locals ( ) ) ) db . execute ( ( 'INSERT  INTO  %(prefix)s %(table)s\n                                            SELECT  *  FROM  %(tmp prefix)s %(table)s\n                                            WHERE  ST  Intersects (way,  %(bbox)s)' % locals ( ) ) ) db . execute ( 'COMMIT' )
@ requires sklearn def test gat plot nonsquared ( ) : gat =  get data ( test times = dict ( start = 0.0 ) ) gat . plot ( ) ax = gat . plot diagonal ( ) scores = ax . get children ( ) [ 1 ] . get lines ( ) [ 2 ] . get ydata ( ) assert equals ( len ( scores ) , len ( gat . estimators  ) )
def strategy saturation largest first ( G , colors ) : distinct colors = { v : set ( ) for v in G } for i in range ( len ( G ) ) : if ( i == 0 ) : node = max ( G , key = G . degree ) ( yield node ) for v in G [ node ] : distinct colors [ v ] . add ( 0 ) else : saturation = { v : len ( c ) for ( v , c ) in distinct colors . items ( ) if ( v not in colors ) } node = max ( saturation , key = ( lambda v : ( saturation [ v ] , G . degree ( v ) ) ) ) ( yield node ) color = colors [ node ] for v in G [ node ] : distinct colors [ v ] . add ( color )
def get linode id from name ( name ) : nodes =  query ( 'linode' , 'list' ) [ 'DATA' ] linode id = '' for node in nodes : if ( name == node [ 'LABEL' ] ) : linode id = node [ 'LINODEID' ] return linode id if ( not linode id ) : raise  Salt  Cloud  Not  Found  ( ' The   specified  name,  {0},  could  not  be  found.' . format ( name ) )
def S white simple ( x ) : if ( x . ndim == 1 ) : x = x [ : ,  None  ] return np . dot ( x . T , x )
def did you mean units ( s , all units , deprecated units , format decomposed ) : def fix deprecated ( x ) : if ( x in deprecated units ) : results = [ ( x + u'  (deprecated)' ) ] decomposed =  try decomposed ( all units [ x ] , format decomposed ) if ( decomposed is not  None  ) : results . append ( decomposed ) return results return ( x , ) return did you mean ( s , all units , fix = fix deprecated )
def  find matching indices ( tree , bin X , left mask , right mask ) : left index = np . searchsorted ( tree , ( bin X & left mask ) ) right index = np . searchsorted ( tree , ( bin X | right mask ) , side = 'right' ) return ( left index , right index )
def test cons list ( ) : entry = tokenize ( '(a  .  [])' ) [ 0 ] assert ( entry ==  Hy  List  ( [  Hy  Symbol  ( 'a' ) ] ) ) assert ( type ( entry ) ==  Hy  List  ) entry = tokenize ( '(a  .  ())' ) [ 0 ] assert ( entry ==  Hy  Expression  ( [  Hy  Symbol  ( 'a' ) ] ) ) assert ( type ( entry ) ==  Hy  Expression  ) entry = tokenize ( '(a  b  .  {})' ) [ 0 ] assert ( entry ==  Hy  Dict  ( [  Hy  Symbol  ( 'a' ) ,  Hy  Symbol  ( 'b' ) ] ) ) assert ( type ( entry ) ==  Hy  Dict  )
def  disconnect session ( session ) : session [ 'client' ] . auth . logout ( session [ 'key' ] )
@ pytest . mark . django db def test verify user empty email ( trans member ) : with pytest . raises (  Email  Address  .  Does  Not  Exist  ) :  Email  Address  . objects . get ( user = trans member ) assert ( trans member . email == '' ) with pytest . raises (  Validation  Error  ) : accounts . utils . verify user ( trans member ) with pytest . raises (  Email  Address  .  Does  Not  Exist  ) :  Email  Address  . objects . get ( user = trans member )
def get occupied streams ( realm ) : subs filter =  Subscription  . objects . filter ( active =  True  , user profile  realm = realm , user profile  is active =  True  ) . values ( 'recipient id' ) stream ids =  Recipient  . objects . filter ( type =  Recipient  . STREAM , id  in = subs filter ) . values ( 'type id' ) return  Stream  . objects . filter ( id  in = stream ids , realm = realm , deactivated =  False  )
def get New  Repository  ( ) : return  Fill  Repository  ( )
def total seconds ( td ) : return ( ( td . microseconds + ( ( td . seconds + ( ( td . days * 24 ) * 3600 ) ) * ( 10 ** 6 ) ) ) / ( 10 ** 6 ) )
def object list ( request , queryset , paginate by =  None  , page =  None  , allow empty =  True  , template name =  None  , template loader = loader , extra context =  None  , context processors =  None  , template object name = 'object' , mimetype =  None  ) : if ( extra context is  None  ) : extra context = { } queryset = queryset .  clone ( ) if paginate by : paginator =  Paginator  ( queryset , paginate by , allow empty first page = allow empty ) if ( not page ) : page = request . GET . get ( 'page' , 1 ) try : page number = int ( page ) except  Value  Error  : if ( page == 'last' ) : page number = paginator . num pages else : raise  Http 404 try : page obj = paginator . page ( page number ) except  Invalid  Page  : raise  Http 404 c =  Request  Context  ( request , { ( '%s list' % template object name ) : page obj . object list , 'paginator' : paginator , 'page obj' : page obj , 'is paginated' : page obj . has other pages ( ) , 'results per page' : paginator . per page , 'has next' : page obj . has next ( ) , 'has previous' : page obj . has previous ( ) , 'page' : page obj . number , 'next' : page obj . next page number ( ) , 'previous' : page obj . previous page number ( ) , 'first on page' : page obj . start index ( ) , 'last on page' : page obj . end index ( ) , 'pages' : paginator . num pages , 'hits' : paginator . count , 'page range' : paginator . page range } , context processors ) else : c =  Request  Context  ( request , { ( '%s list' % template object name ) : queryset , 'paginator' :  None  , 'page obj' :  None  , 'is paginated' :  False  } , context processors ) if ( ( not allow empty ) and ( len ( queryset ) == 0 ) ) : raise  Http 404 for ( key , value ) in extra context . items ( ) : if callable ( value ) : c [ key ] = value ( ) else : c [ key ] = value if ( not template name ) : model = queryset . model template name = ( '%s/%s list.html' % ( model .  meta . app label , model .  meta . object name . lower ( ) ) ) t = template loader . get template ( template name ) return  Http  Response  ( t . render ( c ) , mimetype = mimetype )
def load passphrase from file ( ) : vf path = os . path . expanduser ( kVF Passphrase  File  ) assert ( os . access ( vf path , os . F OK ) and os . access ( vf path , os . R OK ) ) , ( '%s  must  exist  and  be  readable' % vf path ) with open ( vf path ) as f : user data = f . read ( ) return user data . strip ( '\n' )
@ register . simple tag ( takes context =  True  ) def zinnia loop template ( context , default template ) : ( matching , context object ) = get context first matching object ( context , [ 'category' , 'tag' , 'author' , 'pattern' , 'year' , 'month' , 'week' , 'day' ] ) context positions = get context loop positions ( context ) templates = loop template list ( context positions , context object , matching , default template , ENTRY LOOP TEMPLATES ) return select template ( templates )
def convert params ( exception =  Value  Error  , error = 400 ) : request = cherrypy . serving . request types = request . handler . callable .   annotations   with cherrypy . HTTP Error  . handle ( exception , error ) : for key in set ( types ) . intersection ( request . params ) : request . params [ key ] = types [ key ] ( request . params [ key ] )
def  tensordot as dot ( a , b , axes , dot , batched ) : ( a , b ) = ( as tensor variable ( a ) , as tensor variable ( b ) ) if ( ( not numpy . isscalar ( axes ) ) and ( len ( axes ) != 2 ) ) : raise  Value  Error  ( ( ' Axes   should  be  an  integer  or  a  list/tuple  of  len  2  (%s  was  provided)' % str ( axes ) ) ) elif numpy . isscalar ( axes ) : axes = int ( axes ) for ( operand name , operand ) in ( ( 'a' , a ) , ( 'b' , b ) ) : if ( axes > operand . ndim ) : raise  Value  Error  ( ( 'axes  can  not  be  larger  than  the  dimension  of  %s  (%s.ndim=%i,  axes=%i)' % ( operand name , operand name , operand . ndim , axes ) ) ) if ( batched and ( axes == operand . ndim ) ) : raise  Value  Error  ( ( 'axes  to  sum  over  must  not  include  the  batch  axis  of  %s  (%s.ndim=%i,  axes=%i)' % ( operand name , operand name , operand . ndim , axes ) ) ) batch axes = ( 1 if batched else 0 ) a outaxes = slice ( 0 , ( a . ndim - axes ) ) b outaxes = slice ( ( batch axes + axes ) , b . ndim ) outshape = concatenate ( [ a . shape [ a outaxes ] , b . shape [ b outaxes ] ] ) outbcast = ( a . broadcastable [ a outaxes ] + b . broadcastable [ b outaxes ] ) outndim = len ( outbcast ) a shape = ( [ 1 ] * 2 ) b shape = ( [ 1 ] * 2 ) for i in xrange ( 0 , axes ) : a shape [ 1 ] *= a . shape [ ( - ( i + 1 ) ) ] b shape [ 0 ] *= b . shape [ ( batch axes + i ) ] for i in xrange ( 0 , ( ( a . ndim - axes ) - batch axes ) ) : a shape [ 0 ] *= a . shape [ ( batch axes + i ) ] for i in xrange ( 0 , ( ( b . ndim - axes ) - batch axes ) ) : b shape [ 1 ] *= b . shape [ ( - ( i + 1 ) ) ] if batched : a shape . insert ( 0 , a . shape [ 0 ] ) b shape . insert ( 0 , b . shape [ 0 ] ) a reshaped = a . reshape ( a shape ) b reshaped = b . reshape ( b shape ) out reshaped = dot ( a reshaped , b reshaped ) out = out reshaped . reshape ( outshape , outndim ) return patternbroadcast ( out , outbcast ) else : axes = [  pack ( axes  ) for axes  in axes ] if ( len ( axes [ 0 ] ) != len ( axes [ 1 ] ) ) : raise  Value  Error  ( ' Axes   elements  must  have  the  same  length.' ) for ( i , ( operand name , operand ) ) in enumerate ( ( ( 'a' , a ) , ( 'b' , b ) ) ) : if ( len ( axes [ i ] ) > operand . ndim ) : raise  Value  Error  ( ( 'axes[%i]  should  be  array like  with  length  less  than  the  dimensions  of  %s  (%s.ndim=%i,  len(axes[0])=%i).' % ( i , operand name , operand name , operand . ndim , len ( axes [ i ] ) ) ) ) if ( ( len ( axes [ i ] ) > 0 ) and ( numpy . max ( axes [ i ] ) >= operand . ndim ) ) : raise  Value  Error  ( ( 'axes[%i]  contains  dimensions  greater  than  or  equal  to  %s.ndim  (%s.ndim=%i,  max(axes[0])=%i).' % ( i , operand name , operand name , operand . ndim , numpy . max ( numpy . array ( axes [ i ] ) ) ) ) ) if ( batched and ( 0 in axes [ i ] ) ) : raise  Value  Error  ( ( 'axes  to  sum  over  must  not  contain  the  batch  axis  (axes[%i]=%s)' % ( i , axes [ i ] ) ) ) batch axes = ( [ 0 ] if batched else [ ] ) other axes = [ [ x for x in xrange ( operand . ndim ) if ( ( x not in axes [ i ] ) and ( x not in batch axes ) ) ] for ( i , operand ) in enumerate ( ( a , b ) ) ] a shuffled = a . dimshuffle ( ( ( batch axes + other axes [ 0 ] ) + axes [ 0 ] ) ) b shuffled = b . dimshuffle ( ( ( batch axes + axes [ 1 ] ) + other axes [ 1 ] ) ) return  tensordot as dot ( a shuffled , b shuffled , len ( axes [ 0 ] ) , dot = dot , batched = batched )
def layer js ( ) : if ( settings . get security map ( ) and ( not s3 has role ( MAP ADMIN ) ) ) : auth . permission . fail ( ) tablename = ( '%s %s' % ( module , resourcename ) ) s3db . table ( tablename ) type = 'JS' LAYERS = T ( ( TYPE LAYERS FMT % type ) ) ADD NEW LAYER = T ( ( ADD NEW TYPE LAYER FMT % type ) ) EDIT LAYER = T ( ( EDIT TYPE LAYER FMT % type ) ) LIST LAYERS = T ( ( LIST TYPE LAYERS FMT % type ) ) NO LAYERS = T ( ( NO TYPE LAYERS FMT % type ) ) s3 . crud strings [ tablename ] =  Storage  ( label create = ADD LAYER , title display = LAYER DETAILS , title list = LAYERS , title update = EDIT LAYER , label list button = LIST LAYERS , label delete button = DELETE LAYER , msg record created = LAYER ADDED , msg record modified = LAYER UPDATED , msg record deleted = LAYER DELETED , msg list empty = NO LAYERS ) def prep ( r ) : if r . interactive : if ( r . component name == 'config' ) : ltable = s3db . gis layer config if ( r . method != 'update' ) : table = r . table query = ( ( ltable . layer id == table . layer id ) & ( table . id == r . id ) ) rows = db ( query ) . select ( ltable . config id ) ltable . config id . requires = IS ONE OF ( db , 'gis config.id' , '%(name)s' , not filterby = 'config id' , not filter opts = [ row . config id for row in rows ] ) return  True  s3 . prep = prep def postp ( r , output ) : if ( r . interactive and ( r . method != 'import' ) ) : if ( not r . component ) : inject enable ( output ) return output s3 . postp = postp output = s3 rest controller ( rheader = s3db . gis rheader ) return output
def  safe split ( estimator , X , y , indices , train indices =  None  ) : if ( hasattr ( estimator , 'kernel' ) and callable ( estimator . kernel ) and ( not isinstance ( estimator . kernel , GP Kernel  ) ) ) : raise  Value  Error  ( ' Cannot   use  a  custom  kernel  function.   Precompute   the  kernel  matrix  instead.' ) if ( not hasattr ( X , 'shape' ) ) : if getattr ( estimator , ' pairwise' ,  False  ) : raise  Value  Error  ( ' Precomputed   kernels  or  affinity  matrices  have  to  be  passed  as  arrays  or  sparse  matrices.' ) X subset = [ X [ idx ] for idx in indices ] elif getattr ( estimator , ' pairwise' ,  False  ) : if ( X . shape [ 0 ] != X . shape [ 1 ] ) : raise  Value  Error  ( 'X  should  be  a  square  kernel  matrix' ) if ( train indices is  None  ) : X subset = X [ np . ix  ( indices , indices ) ] else : X subset = X [ np . ix  ( indices , train indices ) ] else : X subset = safe indexing ( X , indices ) if ( y is not  None  ) : y subset = safe indexing ( y , indices ) else : y subset =  None  return ( X subset , y subset )
def add status query managers ( sender , ** kwargs ) : if ( not issubclass ( sender ,  Status  Model  ) ) : return for ( value , display ) in getattr ( sender , u'STATUS' , ( ) ) : try : sender .  meta . get field ( value ) raise  Improperly  Configured  ( ( u" Status  Model :   Model   '%s'  has  a  field  named  '%s'  which  conflicts  with  a  status  of  the  same  name." % ( sender .   name   , value ) ) ) except  Field  Does  Not  Exist  : pass sender . add to class ( value ,  Query  Manager  ( status = value ) )
def call url ( url , view kwargs =  None  ) : ( func name , func data ) = app . url map . bind ( '' ) . match ( url ) if ( view kwargs is not  None  ) : func data . update ( view kwargs ) view function = view functions [ func name ] rv = view function ( ** func data ) ( rv ,   ,   ,   ) = unpack ( rv ) if ( isinstance ( rv , werkzeug . wrappers .  Base  Response  ) and ( rv . status code in REDIRECT CODES ) ) : redirect url = rv . headers [ ' Location ' ] return call url ( redirect url ) return rv
def test consistency GPU parallel ( ) : if ( not cuda available ) : raise  Skip  Test  ( ' Optional   package  cuda  not  available' ) if ( config . mode == 'FAST COMPILE' ) : mode = 'FAST RUN' else : mode = config . mode seed = 12345 n samples = 5 n streams = 12 n substreams = 7 samples = [ ] curr rstate = numpy . array ( ( [ seed ] * 6 ) , dtype = 'int32' ) for i in range ( n streams ) : stream samples = [ ] rstate = [ curr rstate . copy ( ) ] for j in range ( 1 , n substreams ) : rstate . append ( rng mrg . ff 2p72 ( rstate [ ( - 1 ) ] ) ) rstate = numpy . asarray ( rstate ) . flatten ( ) tmp float buf = numpy . frombuffer ( rstate . data , dtype = 'float32' ) rstate = float32 shared constructor ( tmp float buf ) ( new rstate , sample ) = rng mrg . GPU mrg uniform . new ( rstate , ndim =  None  , dtype = 'float32' , size = ( n substreams , ) ) rstate . default update = new rstate sample . rstate = rstate sample . update = ( rstate , new rstate ) cpu sample = tensor . as tensor variable ( sample ) f = theano . function ( [ ] , cpu sample , mode = mode ) for k in range ( n samples ) : s = f ( ) stream samples . append ( s ) samples . append ( numpy . array ( stream samples ) . T . flatten ( ) ) curr rstate = rng mrg . ff 2p134 ( curr rstate ) samples = numpy . array ( samples ) . flatten ( ) assert numpy . allclose ( samples , java samples )
def get subclasses ( c ) : return ( c .   subclasses   ( ) + sum ( map ( get subclasses , c .   subclasses   ( ) ) , [ ] ) )
def function no Args  ( ) : return
def blacklist check ( path ) : ( head , tests dir ) = os . path . split ( path . dirname ) if ( tests dir != u'tests' ) : return  True  ( head , top module ) = os . path . split ( head ) return ( path . purebasename in IGNORED TESTS . get ( top module , [ ] ) )
def detect ( stream ) : try : openpyxl . reader . excel . load workbook ( stream ) return  True  except openpyxl . shared . exc .  Invalid  File  Exception  : pass
def pick plugin ( config , default , plugins , question , ifaces ) : if ( default is not  None  ) : filtered = plugins . filter ( ( lambda p ep : ( p ep . name == default ) ) ) else : if config . noninteractive mode : raise errors .  Missing  Commandline  Flag  ( " Missing   command  line  flags.   For   non-interactive  execution,  you  will  need  to  specify  a  plugin  on  the  command  line.     Run   with  '--help  plugins'  to  see  a  list  of  options,  and  see  https://eff.org/letsencrypt-plugins  for  more  detail  on  what  the  plugins  do  and  how  to  use  them." ) filtered = plugins . visible ( ) . ifaces ( ifaces ) filtered . init ( config ) verified = filtered . verify ( ifaces ) verified . prepare ( ) prepared = verified . available ( ) if ( len ( prepared ) > 1 ) : logger . debug ( ' Multiple   candidate  plugins:  %s' , prepared ) plugin ep = choose plugin ( list ( six . itervalues ( prepared ) ) , question ) if ( plugin ep is  None  ) : return  None  else : return plugin ep . init ( ) elif ( len ( prepared ) == 1 ) : plugin ep = list ( prepared . values ( ) ) [ 0 ] logger . debug ( ' Single   candidate  plugin:  %s' , plugin ep ) if plugin ep . misconfigured : return  None  return plugin ep . init ( ) else : logger . debug ( ' No   candidate  plugin' ) return  None
@ decorator . decorator def outplace ( f , clip , * a , ** k ) : newclip = clip . copy ( ) f ( newclip , * a , ** k ) return newclip
def escape ( inp ) : def conv ( obj ) : ' Convert   obj.' if isinstance ( obj , list ) : rv = as unicode ( ( ( '[' + ',' . join ( ( conv ( o ) for o in obj ) ) ) + ']' ) ) elif isinstance ( obj , dict ) : rv = as unicode ( ( ( '{' + ',' . join ( [ ( '%s:%s' % ( conv ( key ) , conv ( value ) ) ) for ( key , value ) in obj . iteritems ( ) ] ) ) + '}' ) ) else : rv = ( as unicode ( '"%s"' ) % as unicode ( obj ) . replace ( '"' , '\\"' ) ) return rv return conv ( inp )
@ register ( u'yank-last-arg' ) def yank last arg ( event ) : n = ( event . arg if event . arg present else  None  ) event . current buffer . yank last arg ( n )
def normalize ( pattern ) : result = [ ] non capturing groups = [ ] consume next =  True  pattern iter = next char ( iter ( pattern ) ) num args = 0 try : ( ch , escaped ) = next ( pattern iter ) except  Stop  Iteration  : return [ ( '' , [ ] ) ] try : while  True  : if escaped : result . append ( ch ) elif ( ch == '.' ) : result . append ( '.' ) elif ( ch == '|' ) : raise  Not  Implemented  Error  ( ' Awaiting    Implementation ' ) elif ( ch == '^' ) : pass elif ( ch == '$' ) : break elif ( ch == ')' ) : start = non capturing groups . pop ( ) inner =  Non  Capture  ( result [ start : ] ) result = ( result [ : start ] + [ inner ] ) elif ( ch == '[' ) : ( ch , escaped ) = next ( pattern iter ) result . append ( ch ) ( ch , escaped ) = next ( pattern iter ) while ( escaped or ( ch != ']' ) ) : ( ch , escaped ) = next ( pattern iter ) elif ( ch == '(' ) : ( ch , escaped ) = next ( pattern iter ) if ( ( ch != '?' ) or escaped ) : name = ( ' %d' % num args ) num args += 1 result . append (  Group  ( ( ( '%%(%s)s' % name ) , name ) ) ) walk to end ( ch , pattern iter ) else : ( ch , escaped ) = next ( pattern iter ) if ( ch in '!=<' ) : walk to end ( ch , pattern iter ) elif ( ch in 'i Lmsu #' ) : warnings . warn ( ( ' Using   (?%s)  in  url()  patterns  is  deprecated.' % ch ) ,  Removed  In  Django 21 Warning  ) walk to end ( ch , pattern iter ) elif ( ch == ':' ) : non capturing groups . append ( len ( result ) ) elif ( ch != 'P' ) : raise  Value  Error  ( ( " Non -reversible  reg-exp  portion:  '(?%s'" % ch ) ) else : ( ch , escaped ) = next ( pattern iter ) if ( ch not in ( '<' , '=' ) ) : raise  Value  Error  ( ( " Non -reversible  reg-exp  portion:  '(?P%s'" % ch ) ) if ( ch == '<' ) : terminal char = '>' else : terminal char = ')' name = [ ] ( ch , escaped ) = next ( pattern iter ) while ( ch != terminal char ) : name . append ( ch ) ( ch , escaped ) = next ( pattern iter ) param = '' . join ( name ) if ( terminal char != ')' ) : result . append (  Group  ( ( ( '%%(%s)s' % param ) , param ) ) ) walk to end ( ch , pattern iter ) else : result . append (  Group  ( ( ( '%%(%s)s' % param ) ,  None  ) ) ) elif ( ch in '*?+{' ) : ( count , ch ) = get quantifier ( ch , pattern iter ) if ch : consume next =  False  if ( count == 0 ) : if contains ( result [ ( - 1 ) ] ,  Group  ) : result [ ( - 1 ) ] =  Choice  ( [  None  , result [ ( - 1 ) ] ] ) else : result . pop ( ) elif ( count > 1 ) : result . extend ( ( [ result [ ( - 1 ) ] ] * ( count - 1 ) ) ) else : result . append ( ch ) if consume next : ( ch , escaped ) = next ( pattern iter ) else : consume next =  True  except  Stop  Iteration  : pass except  Not  Implemented  Error  : return [ ( '' , [ ] ) ] return list ( zip ( * flatten result ( result ) ) )
def  ssh args ( ssh bin , address , ec2 key pair file ) : if ( ec2 key pair file is  None  ) : raise  Value  Error  ( 'SSH  key  file  path  is   None ' ) return ( ssh bin + [ '-i' , ec2 key pair file , '-o' , ' Strict  Host  Key  Checking =no' , '-o' , ' User  Known  Hosts  File =/dev/null' , ( 'hadoop@%s' % ( address , ) ) ] )
def make twilio request ( method , uri , ** kwargs ) : headers = kwargs . get ( 'headers' , { } ) user agent = ( 'twilio-python/%s  ( Python   %s)' % (   version   , platform . python version ( ) ) ) headers [ ' User - Agent ' ] = user agent headers [ ' Accept - Charset ' ] = 'utf-8' if ( ( method == 'POST' ) and ( ' Content - Type ' not in headers ) ) : headers [ ' Content - Type ' ] = 'application/x-www-form-urlencoded' kwargs [ 'headers' ] = headers if ( ' Accept ' not in headers ) : headers [ ' Accept ' ] = 'application/json' if kwargs . pop ( 'use json extension' ,  False  ) : uri += '.json' resp = make request ( method , uri , ** kwargs ) if ( not resp . ok ) : try : error = json . loads ( resp . content ) code = error [ 'code' ] message = error [ 'message' ] except : code =  None  message = resp . content raise  Twilio  Rest  Exception  ( status = resp . status code , method = method , uri = resp . url , msg = message , code = code ) return resp
def serve ( request , path , document root =  None  , show indexes =  False  ) : path = posixpath . normpath ( unquote ( path ) ) path = path . lstrip ( u'/' ) newpath = u'' for part in path . split ( u'/' ) : if ( not part ) : continue ( drive , part ) = os . path . splitdrive ( part ) ( head , part ) = os . path . split ( part ) if ( part in ( os . curdir , os . pardir ) ) : continue newpath = os . path . join ( newpath , part ) . replace ( u'\\' , u'/' ) if ( newpath and ( path != newpath ) ) : return  Http  Response  Redirect  ( newpath ) fullpath = os . path . join ( document root , newpath ) if os . path . isdir ( fullpath ) : if show indexes : return directory index ( newpath , fullpath ) raise  Http 404 (   ( u' Directory   indexes  are  not  allowed  here.' ) ) if ( not os . path . exists ( fullpath ) ) : raise  Http 404 ( (   ( u'"%(path)s"  does  not  exist' ) % { u'path' : fullpath } ) ) statobj = os . stat ( fullpath ) if ( not was modified since ( request . META . get ( u'HTTP IF MODIFIED SINCE' ) , statobj . st mtime , statobj . st size ) ) : return  Http  Response  Not  Modified  ( ) ( content type , encoding ) = mimetypes . guess type ( fullpath ) content type = ( content type or u'application/octet-stream' ) f = open ( fullpath , u'rb' ) response =  Compatible  Streaming  Http  Response  ( iter ( ( lambda : f . read ( STREAM CHUNK SIZE ) ) , '' ) , content type = content type ) response [ u' Last - Modified ' ] = http date ( statobj . st mtime ) if stat . S ISREG ( statobj . st mode ) : response [ u' Content - Length ' ] = statobj . st size if encoding : response [ u' Content - Encoding ' ] = encoding return response
def voronoi cells ( G , center nodes , weight = 'weight' ) : paths = nx . multi source dijkstra path ( G , center nodes , weight = weight ) nearest = { v : p [ 0 ] for ( v , p ) in paths . items ( ) } cells = groups ( nearest ) unreachable = ( set ( G ) - set ( nearest ) ) if unreachable : cells [ 'unreachable' ] = unreachable return cells
def get test data ( nb train = 1000 , nb test = 500 , input shape = ( 10 , ) , output shape = ( 2 , ) , classification =  True  , nb class = 2 ) : nb sample = ( nb train + nb test ) if classification : y = np . random . randint ( 0 , nb class , size = ( nb sample , ) ) X = np . zeros ( ( ( nb sample , ) + input shape ) ) for i in range ( nb sample ) : X [ i ] = np . random . normal ( loc = y [ i ] , scale = 0.7 , size = input shape ) else : y loc = np . random . random ( ( nb sample , ) ) X = np . zeros ( ( ( nb sample , ) + input shape ) ) y = np . zeros ( ( ( nb sample , ) + output shape ) ) for i in range ( nb sample ) : X [ i ] = np . random . normal ( loc = y loc [ i ] , scale = 0.7 , size = input shape ) y [ i ] = np . random . normal ( loc = y loc [ i ] , scale = 0.7 , size = output shape ) return ( ( X [ : nb train ] , y [ : nb train ] ) , ( X [ nb train : ] , y [ nb train : ] ) )
def font priority ( font ) : style normal = ( font [ u'font-style' ] == u'normal' ) width normal = ( font [ u'font-stretch' ] == u'normal' ) weight normal = ( font [ u'font-weight' ] == u'normal' ) num normal = sum ( filter (  None  , ( style normal , width normal , weight normal ) ) ) subfamily name = ( font [ u'wws subfamily name' ] or font [ u'preferred subfamily name' ] or font [ u'subfamily name' ] ) if ( ( num normal == 3 ) and ( subfamily name == u' Regular ' ) ) : return 0 if ( num normal == 3 ) : return 1 if ( subfamily name == u' Regular ' ) : return 2 return ( 3 + ( 3 - num normal ) )
def load Icon  ( stock item id ) : stock item = getattr ( gtk , stock item id ) local icon = os . path . join ( GUI DATA PATH , 'icons' , '16' , ( '%s.png' % stock item ) ) if os . path . exists ( local icon ) : im = gtk .  Image  ( ) im . set from file ( local icon ) im . show ( ) return im . get pixbuf ( ) else : icon theme = gtk .  Icon  Theme  ( ) try : icon = icon theme . load icon ( stock item , 16 , ( ) ) except : icon = load Image  ( 'missing-image.png' ) . get pixbuf ( ) return icon
def iter Services  ( xrd tree ) : xrd = get Yadis XRD ( xrd tree ) return prio Sort  ( xrd . findall ( service tag ) )
def test nearmiss wrong version ( ) : version = 1000 nm3 =  Near  Miss  ( version = version , random state = RND SEED ) assert raises (  Value  Error  , nm3 . fit sample , X , Y )
@ require global staff @ require POST def enable certificate generation ( request , course id =  None  ) : course key =  Course  Key  . from string ( course id ) is enabled = ( request . POST . get ( 'certificates-enabled' , 'false' ) == 'true' ) certs api . set cert generation enabled ( course key , is enabled ) return redirect (  instructor dash url ( course key , section = 'certificates' ) )
def decompose matrix ( matrix ) : M = numpy . array ( matrix , dtype = numpy . float64 , copy =  True  ) . T if ( abs ( M [ ( 3 , 3 ) ] ) <  EPS ) : raise  Value  Error  ( 'M[3,  3]  is  zero' ) M /= M [ ( 3 , 3 ) ] P = M . copy ( ) P [ : , 3 ] = ( 0.0 , 0.0 , 0.0 , 1.0 ) if ( not numpy . linalg . det ( P ) ) : raise  Value  Error  ( 'matrix  is  singular' ) scale = numpy . zeros ( ( 3 , ) ) shear = [ 0.0 , 0.0 , 0.0 ] angles = [ 0.0 , 0.0 , 0.0 ] if any ( ( abs ( M [ : 3 , 3 ] ) >  EPS ) ) : perspective = numpy . dot ( M [ : , 3 ] , numpy . linalg . inv ( P . T ) ) M [ : , 3 ] = ( 0.0 , 0.0 , 0.0 , 1.0 ) else : perspective = numpy . array ( [ 0.0 , 0.0 , 0.0 , 1.0 ] ) translate = M [ 3 , : 3 ] . copy ( ) M [ 3 , : 3 ] = 0.0 row = M [ : 3 , : 3 ] . copy ( ) scale [ 0 ] = vector norm ( row [ 0 ] ) row [ 0 ] /= scale [ 0 ] shear [ 0 ] = numpy . dot ( row [ 0 ] , row [ 1 ] ) row [ 1 ] -= ( row [ 0 ] * shear [ 0 ] ) scale [ 1 ] = vector norm ( row [ 1 ] ) row [ 1 ] /= scale [ 1 ] shear [ 0 ] /= scale [ 1 ] shear [ 1 ] = numpy . dot ( row [ 0 ] , row [ 2 ] ) row [ 2 ] -= ( row [ 0 ] * shear [ 1 ] ) shear [ 2 ] = numpy . dot ( row [ 1 ] , row [ 2 ] ) row [ 2 ] -= ( row [ 1 ] * shear [ 2 ] ) scale [ 2 ] = vector norm ( row [ 2 ] ) row [ 2 ] /= scale [ 2 ] shear [ 1 : ] /= scale [ 2 ] if ( numpy . dot ( row [ 0 ] , numpy . cross ( row [ 1 ] , row [ 2 ] ) ) < 0 ) : numpy . negative ( scale , scale ) numpy . negative ( row , row ) angles [ 1 ] = math . asin ( ( - row [ ( 0 , 2 ) ] ) ) if math . cos ( angles [ 1 ] ) : angles [ 0 ] = math . atan2 ( row [ ( 1 , 2 ) ] , row [ ( 2 , 2 ) ] ) angles [ 2 ] = math . atan2 ( row [ ( 0 , 1 ) ] , row [ ( 0 , 0 ) ] ) else : angles [ 0 ] = math . atan2 ( ( - row [ ( 2 , 1 ) ] ) , row [ ( 1 , 1 ) ] ) angles [ 2 ] = 0.0 return ( scale , shear , angles , translate , perspective )
def patch os ( ) : patch module ( 'os' )
@ task @ cmdopts ( [ ( 'src=' , 's' , ' Url   to  source' ) , ( 'rev=' , 'r' , 'HG  revision' ) , ( 'clean' , 'c' , ' Delete   old  source  folder' ) ] ) def get source ( options ) : if options . rev : options . url = ( 'https://bitbucket.org/spoob/pyload/get/%s.zip' % options . rev ) pyload = path ( 'pyload' ) if ( len ( pyload . listdir ( ) ) and ( not options . clean ) ) : return elif pyload . exists ( ) : pyload . rmtree ( ) urlretrieve ( options . src , 'pyload src.zip' ) zip =  Zip  File  ( 'pyload src.zip' ) zip . extractall ( ) path ( 'pyload src.zip' ) . remove ( ) folder = [ x for x in path ( '.' ) . dirs ( ) if x . name . startswith ( 'spoob-pyload-' ) ] [ 0 ] folder . move ( pyload ) change mode ( pyload , 420 ) change mode ( pyload , 493 , folder =  True  ) for file in pyload . files ( ) : if file . name . endswith ( '.py' ) : file . chmod ( 493 ) ( pyload / '.hgtags' ) . remove ( ) ( pyload / '.hgignore' ) . remove ( ) f = open ( ( pyload / '  init  .py' ) , 'wb' ) f . close ( )
def set special ( user , special , cmd ) : lst = list tab ( user ) for cron in lst [ 'special' ] : if ( ( special == cron [ 'spec' ] ) and ( cmd == cron [ 'cmd' ] ) ) : return 'present' spec = { 'spec' : special , 'cmd' : cmd } lst [ 'special' ] . append ( spec ) comdat =  write cron lines ( user ,  render tab ( lst ) ) if comdat [ 'retcode' ] : return comdat [ 'stderr' ] return 'new'
def file hash ( load , fnd ) : gitfs = salt . utils . gitfs .  Git FS (   opts   ) gitfs . init remotes (   opts   [ 'gitfs remotes' ] , PER REMOTE OVERRIDES , PER REMOTE ONLY ) return gitfs . file hash ( load , fnd )
def escape ( s ) : if ( s is  None  ) : return '' assert isinstance ( s , basestring ) , ( 'expected  %s  but  got  %s;  value=%s' % ( basestring , type ( s ) , s ) ) s = s . replace ( '\\' , '\\\\' ) s = s . replace ( '\n' , '\\n' ) s = s . replace ( ' DCTB ' , '\\t' ) s = s . replace ( ',' , ' DCTB ' ) return s
@ requires application ( ) def test context properties ( ) : a = use app ( ) if ( a . backend name . lower ( ) == 'pyglet' ) : return if ( a . backend name . lower ( ) == 'osmesa' ) : return configs = [ dict ( samples = 4 ) , dict ( stencil size = 8 ) , dict ( samples = 4 , stencil size = 8 ) ] if ( a . backend name . lower ( ) != 'glfw' ) : configs . append ( dict ( double buffer =  False  , samples = 4 ) ) configs . append ( dict ( double buffer =  False  ) ) else : assert raises (  Runtime  Error  ,  Canvas  , app = a , config = dict ( double buffer =  False  ) ) if ( ( a . backend name . lower ( ) == 'sdl2' ) and ( os . getenv ( 'TRAVIS' ) == 'true' ) ) : raise  Skip  Test  ( ' Travis   SDL  cannot  set  context' ) for config in configs : n items = len ( config ) with  Canvas  ( config = config ) : if ( 'true' in ( os . getenv ( 'TRAVIS' , '' ) , os . getenv ( 'APPVEYOR' , '' ) . lower ( ) ) ) : props = config else : props = get gl configuration ( ) assert equal ( len ( config ) , n items ) for ( key , val ) in config . items ( ) : if ( key == 'samples' ) : iswx = ( a . backend name . lower ( ) == 'wx' ) if ( not ( sys . platform . startswith ( 'win' ) or iswx ) ) : assert equal ( val , props [ key ] , key ) assert raises (  Type  Error  ,  Canvas  , config = 'foo' ) assert raises (  Key  Error  ,  Canvas  , config = dict ( foo =  True  ) ) assert raises (  Type  Error  ,  Canvas  , config = dict ( double buffer = 'foo' ) )
def es delete cmd ( index , noinput =  False  , log = log ) : try : indexes = [ name for ( name , count ) in get indexes ( ) ] except ES EXCEPTIONS : log . error ( ' Your   elasticsearch  process  is  not  running  or  ES URLS  is  set  wrong  in  your  settings local.py  file.' ) return if ( index not in indexes ) : log . error ( ' Index   "%s"  is  not  a  valid  index.' , index ) return if ( ( index in all read indexes ( ) ) and ( not noinput ) ) : ret = raw input ( ( '"%s"  is  a  read  index.   Are   you  sure  you  want  to  delete  it?  (yes/no)  ' % index ) ) if ( ret != 'yes' ) : log . info ( ' Not   deleting  the  index.' ) return log . info ( ' Deleting   index  "%s"...' , index ) delete index ( index ) log . info ( ' Done !' )
def get pending computer name ( ) : current = get computer name ( ) pending =   salt   [ 'reg.read value' ] ( 'HKLM' , 'SYSTEM\\ Current  Control  Set \\ Services \\ Tcpip \\ Parameters ' , 'NV   Hostname ' ) [ 'vdata' ] if pending : return ( pending if ( pending != current ) else  None  ) return  False
def is empty ( G ) : return ( not any ( G . adj . values ( ) ) )
def p statement assign ( t ) : names [ t [ 1 ] ] = t [ 3 ]
def  is task visible ( context , task ) : if context . is admin : return  True  if ( task [ 'owner' ] is  None  ) : return  True  if ( context . owner is not  None  ) : if ( context . owner == task [ 'owner' ] ) : return  True  return  False
def group Flip  Vert  ( flip List  , y Reflect  = 0 ) : if ( type ( flip List  ) != list ) : flip List  = [ flip List  ] for item in flip List  : if ( type ( item ) in ( list , numpy . ndarray ) ) : if ( ( type ( item [ 0 ] ) in ( list , numpy . ndarray ) ) and ( len ( item [ 0 ] ) == 2 ) ) : for i in range ( len ( item ) ) : item [ i ] [ 1 ] = ( ( 2 * y Reflect  ) - item [ i ] [ 1 ] ) else : msg = ' Cannot   vert-flip  elements  in  "%s",  type=%s' raise  Value  Error  ( ( msg % ( str ( item ) , type ( item [ 0 ] ) ) ) ) elif ( type ( item ) in immutables ) : raise  Value  Error  ( ( ' Cannot   change  immutable  item  "%s"' % str ( item ) ) ) if hasattr ( item , 'set Pos ' ) : item . set Pos  ( [ 1 , ( - 1 ) ] , '*' ) item . set Pos  ( [ 0 , ( 2 * y Reflect  ) ] , '+' ) elif hasattr ( item , 'pos' ) : item . pos [ 1 ] *= ( - 1 ) item . pos [ 1 ] += ( 2 * y Reflect  ) if hasattr ( item , 'set Flip  Vert ' ) : item . set Flip  Vert  ( ( not item . flip Vert  ) ) elif hasattr ( item , 'vertices' ) : try : v = ( item . vertices * [ 1 , ( - 1 ) ] ) except  Exception  : v = [ [ item . vertices [ i ] [ 0 ] , ( ( - 1 ) * item . vertices [ i ] [ 1 ] ) ] for i in range ( len ( item . vertices ) ) ] item . set Vertices  ( v ) if ( hasattr ( item , 'set Ori ' ) and item . ori ) : item . set Ori  ( ( - 1 ) , '*' ) item .  need Vertex  Update  =  True
def merge with ( func , * dicts , ** kwargs ) : if ( ( len ( dicts ) == 1 ) and ( not isinstance ( dicts [ 0 ] , dict ) ) ) : dicts = dicts [ 0 ] factory =  get factory ( merge with , kwargs ) result = factory ( ) for d in dicts : for ( k , v ) in iteritems ( d ) : if ( k not in result ) : result [ k ] = [ v ] else : result [ k ] . append ( v ) return valmap ( func , result , factory )
def remove Endpoints  ( pixel Table  , layer Extrusion  Width  , paths , removed Endpoints  , around Width  ) : for removed Endpoint  Index  in xrange ( ( len ( removed Endpoints  ) - 1 ) , ( - 1 ) , ( - 1 ) ) : removed Endpoint  = removed Endpoints  [ removed Endpoint  Index  ] removed Endpoint  Point  = removed Endpoint  . point if is Point  Added  Around  Closest  ( pixel Table  , layer Extrusion  Width  , paths , removed Endpoint  Point  , around Width  ) : removed Endpoints  . remove ( removed Endpoint  )
def text List  To  Colors  Simple  ( names ) : u Names  = list ( set ( names ) ) u Names  . sort ( ) text To  Color  = [ u Names  . index ( n ) for n in names ] text To  Color  = np . array ( text To  Color  ) text To  Color  = ( ( 255 * ( text To  Color  - text To  Color  . min ( ) ) ) / ( text To  Color  . max ( ) - text To  Color  . min ( ) ) ) textmaps = generate Color  Map  ( ) colors = [ textmaps [ int ( c ) ] for c in text To  Color  ] return colors
def get unique variable ( name ) : candidates = tf . get collection ( tf .  Graph  Keys  . GLOBAL VARIABLES , name ) if ( not candidates ) : raise  Value  Error  ( ( ' Couldnt   find  variable  %s' % name ) ) for candidate in candidates : if ( candidate . op . name == name ) : return candidate raise  Value  Error  ( ' Variable   %s  does  not  uniquely  identify  a  variable' , name )
def multidict to dict ( multidict ) : if config . AUTO COLLAPSE MULTI KEYS : d = dict ( multidict . lists ( ) ) for ( key , value ) in d . items ( ) : if ( len ( value ) == 1 ) : d [ key ] = value [ 0 ] return d else : return multidict . to dict ( )
@ register . filter def display url ( url ) : url = force bytes ( url , errors = 'replace' ) return urllib . unquote ( url ) . decode ( 'utf-8' , errors = 'replace' )
def  Command  Line  ( args =  None  , arglist =  None  ) : help text = '\n Commands :\n\ntrain  -  give  size  of  training  set  to  use,  as  argument\npredict  -  give  input  sequence  as  argument  (or  specify  inputs  via  --from-file  <filename>)\n\n' parser = argparse .  Argument  Parser  ( description = help text , formatter class = argparse .  Raw  Text  Help  Formatter  ) parser . add argument ( 'cmd' , help = 'command' ) parser . add argument ( 'cmd input' , nargs = '*' , help = 'input  to  command' ) parser . add argument ( '-v' , '--verbose' , nargs = 0 , help = 'increase  output  verbosity  (add  more  -v  to  increase  versbosity)' , action = V Action  , dest = 'verbose' ) parser . add argument ( '-m' , '--model' , help = 'seq2seq  model  name:  either  embedding rnn  (default)  or  embedding attention' , default =  None  ) parser . add argument ( '-r' , '--learning-rate' , type = float , help = 'learning  rate  (default  0.0001)' , default = 0.0001 ) parser . add argument ( '-e' , '--epochs' , type = int , help = 'number  of  trainig  epochs' , default = 10 ) parser . add argument ( '-i' , '--input-weights' , type = str , help = 'tflearn  file  with  network  weights  to  load' , default =  None  ) parser . add argument ( '-o' , '--output-weights' , type = str , help = 'new  tflearn  file  where  network  weights  are  to  be  saved' , default =  None  ) parser . add argument ( '-p' , '--pattern-name' , type = str , help = 'name  of  pattern  to  use  for  sequence' , default =  None  ) parser . add argument ( '-n' , '--name' , type = str , help = 'name  of  model,  used  when  generating  default  weights  filenames' , default =  None  ) parser . add argument ( '--in-len' , type = int , help = 'input  sequence  length  (default  10)' , default =  None  ) parser . add argument ( '--out-len' , type = int , help = 'output  sequence  length  (default  10)' , default =  None  ) parser . add argument ( '--from-file' , type = str , help = 'name  of  file  to  take  input  data  sequences  from  (json  format)' , default =  None  ) parser . add argument ( '--iter-num' , type = int , help = 'training  iteration  number;  specify  instead  of  input-  or  output-weights  to  use  generated  filenames' , default =  None  ) parser . add argument ( '--data-dir' , help = 'directory  to  use  for  storing  checkpoints  (also  used  when  generating  default  weights  filenames)' , default =  None  ) parser . add argument ( '-L' , '--num-layers' , type = int , help = 'number  of  RNN  layers  to  use  in  the  model  (default  1)' , default = 1 ) parser . add argument ( '--cell-size' , type = int , help = 'size  of  RNN  cell  to  use  (default  32)' , default = 32 ) parser . add argument ( '--cell-type' , type = str , help = 'type  of  RNN  cell  to  use  (default   Basic LSTM Cell )' , default = ' Basic LSTM Cell ' ) parser . add argument ( '--embedding-size' , type = int , help = 'size  of  embedding  to  use  (default  20)' , default = 20 ) parser . add argument ( '--tensorboard-verbose' , type = int , help = 'tensorboard  verbosity  level  (default  0)' , default = 0 ) if ( not args ) : args = parser . parse args ( arglist ) if ( args . iter num is not  None  ) : args . input weights = args . iter num args . output weights = ( args . iter num + 1 ) model params = dict ( num layers = args . num layers , cell size = args . cell size , cell type = args . cell type , embedding size = args . embedding size , learning rate = args . learning rate , tensorboard verbose = args . tensorboard verbose ) if ( args . cmd == 'train' ) : try : num points = int ( args . cmd input [ 0 ] ) except : raise  Exception  ( ' Please   specify  the  number  of  datapoints  to  use  for  training,  as  the  first  argument' ) sp =  Sequence  Pattern  ( args . pattern name , in seq len = args . in len , out seq len = args . out len ) ts2s = TF Learn  Seq 2 Seq  ( sp , seq2seq model = args . model , data dir = args . data dir , name = args . name , verbose = args . verbose ) ts2s . train ( num epochs = args . epochs , num points = num points , weights output fn = args . output weights , weights input fn = args . input weights , model params = model params ) return ts2s elif ( args . cmd == 'predict' ) : if args . from file : inputs = json . loads ( args . from file ) try : input x = map ( int , args . cmd input ) inputs = [ input x ] except : raise  Exception  ( ' Please   provide  a  space-delimited  input  sequence  as  the  argument' ) sp =  Sequence  Pattern  ( args . pattern name , in seq len = args . in len , out seq len = args . out len ) ts2s = TF Learn  Seq 2 Seq  ( sp , seq2seq model = args . model , data dir = args . data dir , name = args . name , verbose = args . verbose ) results = [ ] for x in inputs : ( prediction , y ) = ts2s . predict ( x , weights input fn = args . input weights , model params = model params ) print ( ( '==>   For   input  %s,  prediction=%s  (expected=%s)' % ( x , prediction , sp . generate output sequence ( x ) ) ) ) results . append ( [ prediction , y ] ) ts2s . prediction results = results return ts2s else : print ( ( ' Unknown   command  %s' % args . cmd ) )
def recreate field ( unbound ) : if ( not isinstance ( unbound ,  Unbound  Field  ) ) : raise  Value  Error  ( ( 'recreate field  expects   Unbound  Field   instance,  %s  was  passed.' % type ( unbound ) ) ) return unbound . field class ( * unbound . args , ** unbound . kwargs )
def send returns ( ) : try : send id = request . args [ 0 ] except : redirect ( f = 'send' ) stable = s3db . inv send if ( not auth . s3 has permission ( 'update' , stable , record id = send id ) ) : session . error = T ( ' You   do  not  have  permission  to  return  this  sent  shipment.' ) send record = db ( ( stable . id == send id ) ) . select ( stable . status , limitby = ( 0 , 1 ) ) . first ( ) inv ship status = s3db . inv ship status if ( send record . status == inv ship status [ 'IN PROCESS' ] ) : session . error = T ( ' This   shipment  has  not  been  sent  -  it  cannot  be  returned  because  it  can  still  be  edited.' ) if session . error : redirect ( URL ( c = 'inv' , f = 'send' , args = [ send id ] ) ) rtable = s3db . inv recv tracktable = s3db . inv track item stable [ send id ] = dict ( status = inv ship status [ 'RETURNING' ] , owned by user =  None  , owned by group = ADMIN ) recv row = db ( ( tracktable . send id == send id ) ) . select ( tracktable . recv id , limitby = ( 0 , 1 ) ) . first ( ) if recv row : recv id = recv row . recv id rtable [ recv id ] = dict ( date = request . utcnow , status = inv ship status [ 'RETURNING' ] , owned by user =  None  , owned by group = ADMIN ) db ( ( tracktable . send id == send id ) ) . update ( status = s3db . inv tracking status [ 'RETURNING' ] ) session . confirmation = T ( ' Sent    Shipment   has  returned,  indicate  how  many  items  will  be  returned  to   Warehouse .' ) redirect ( URL ( c = 'inv' , f = 'send' , args = [ send id , 'track item' ] ) )
def get validation errors ( outfile , app =  None  ) : from django . conf import settings from django . db import models , connection from django . db . models . loading import get app errors from django . db . models . fields . related import  Related  Object  from django . db . models . deletion import SET NULL , SET DEFAULT e =  Model  Error  Collection  ( outfile ) for ( app name , error ) in get app errors ( ) . items ( ) : e . add ( app name , error ) for cls in models . get models ( app ) : opts = cls .  meta for f in opts . local fields : if ( ( f . name == 'id' ) and ( not f . primary key ) and ( opts . pk . name == 'id' ) ) : e . add ( opts , ( '"%s":   You   can\'t  use  "id"  as  a  field  name,  because  each  model  automatically  gets  an  "id"  field  if  none  of  the  fields  have  primary key= True .   You   need  to  either  remove/rename  your  "id"  field  or  add  primary key= True   to  a  field.' % f . name ) ) if f . name . endswith ( ' ' ) : e . add ( opts , ( '"%s":   Field   names  cannot  end  with  underscores,  because  this  would  lead  to  ambiguous  queryset  filters.' % f . name ) ) if ( f . primary key and f . null and ( not connection . features . interprets empty strings as nulls ) ) : e . add ( opts , ( '"%s":   Primary   key  fields  cannot  have  null= True .' % f . name ) ) if isinstance ( f , models .  Char  Field  ) : try : max length = int ( f . max length ) if ( max length <= 0 ) : e . add ( opts , ( '"%s":   Char  Fields   require  a  "max length"  attribute  that  is  a  positive  integer.' % f . name ) ) except (  Value  Error  ,  Type  Error  ) : e . add ( opts , ( '"%s":   Char  Fields   require  a  "max length"  attribute  that  is  a  positive  integer.' % f . name ) ) if isinstance ( f , models .  Decimal  Field  ) : ( decimalp ok , mdigits ok ) = (  False  ,  False  ) decimalp msg = '"%s":   Decimal  Fields   require  a  "decimal places"  attribute  that  is  a  non-negative  integer.' try : decimal places = int ( f . decimal places ) if ( decimal places < 0 ) : e . add ( opts , ( decimalp msg % f . name ) ) else : decimalp ok =  True  except (  Value  Error  ,  Type  Error  ) : e . add ( opts , ( decimalp msg % f . name ) ) mdigits msg = '"%s":   Decimal  Fields   require  a  "max digits"  attribute  that  is  a  positive  integer.' try : max digits = int ( f . max digits ) if ( max digits <= 0 ) : e . add ( opts , ( mdigits msg % f . name ) ) else : mdigits ok =  True  except (  Value  Error  ,  Type  Error  ) : e . add ( opts , ( mdigits msg % f . name ) ) invalid values msg = '"%s":   Decimal  Fields   require  a  "max digits"  attribute  value  that  is  greater  than  or  equal  to  the  value  of  the  "decimal places"  attribute.' if ( decimalp ok and mdigits ok ) : if ( decimal places > max digits ) : e . add ( opts , ( invalid values msg % f . name ) ) if ( isinstance ( f , models .  File  Field  ) and ( not f . upload to ) ) : e . add ( opts , ( '"%s":   File  Fields   require  an  "upload to"  attribute.' % f . name ) ) if isinstance ( f , models .  Image  Field  ) : try : from PIL import  Image  except  Import  Error  : try : import  Image  except  Import  Error  : e . add ( opts , ( '"%s":   To   use   Image  Fields ,  you  need  to  install  the   Python    Imaging    Library .   Get   it  at  http://www.pythonware.com/products/pil/  .' % f . name ) ) if ( isinstance ( f , models .  Boolean  Field  ) and getattr ( f , 'null' ,  False  ) ) : e . add ( opts , ( '"%s":   Boolean  Fields   do  not  accept  null  values.   Use   a   Null  Boolean  Field   instead.' % f . name ) ) if f . choices : if ( isinstance ( f . choices , basestring ) or ( not is iterable ( f . choices ) ) ) : e . add ( opts , ( '"%s":  "choices"  should  be  iterable  (e.g.,  a  tuple  or  list).' % f . name ) ) else : for c in f . choices : if ( ( not isinstance ( c , ( list , tuple ) ) ) or ( len ( c ) != 2 ) ) : e . add ( opts , ( '"%s":  "choices"  should  be  a  sequence  of  two-tuples.' % f . name ) ) if ( f . db index not in (  None  ,  True  ,  False  ) ) : e . add ( opts , ( '"%s":  "db index"  should  be  either   None ,   True   or   False .' % f . name ) ) connection . validation . validate field ( e , opts , f ) if ( f . rel and hasattr ( f . rel , 'on delete' ) ) : if ( ( f . rel . on delete == SET NULL ) and ( not f . null ) ) : e . add ( opts , ( "'%s'  specifies  on delete=SET NULL,  but  cannot  be  null." % f . name ) ) elif ( ( f . rel . on delete == SET DEFAULT ) and ( not f . has default ( ) ) ) : e . add ( opts , ( "'%s'  specifies  on delete=SET DEFAULT,  but  has  no  default  value." % f . name ) ) if f . rel : if ( f . rel . to not in models . get models ( ) ) : e . add ( opts , ( "'%s'  has  a  relation  with  model  %s,  which  has  either  not  been  installed  or  is  abstract." % ( f . name , f . rel . to ) ) ) if isinstance ( f . rel . to , ( str , unicode ) ) : continue if ( not f . rel . to .  meta . get field ( f . rel . field name ) . unique ) : e . add ( opts , ( " Field   '%s'  under  model  '%s'  must  have  a  unique= True   constraint." % ( f . rel . field name , f . rel . to .   name   ) ) ) rel opts = f . rel . to .  meta rel name =  Related  Object  ( f . rel . to , cls , f ) . get accessor name ( ) rel query name = f . related query name ( ) if ( not f . rel . is hidden ( ) ) : for r in rel opts . fields : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . local many to many : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . get all related many to many objects ( ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) for r in rel opts . get all related objects ( ) : if ( r . field is not f ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) seen intermediary signatures = [ ] for ( i , f ) in enumerate ( opts . local many to many ) : if ( f . rel . to not in models . get models ( ) ) : e . add ( opts , ( "'%s'  has  an  m2m  relation  with  model  %s,  which  has  either  not  been  installed  or  is  abstract." % ( f . name , f . rel . to ) ) ) if isinstance ( f . rel . to , ( str , unicode ) ) : continue if f . unique : e . add ( opts , ( " Many  To  Many  Fields   cannot  be  unique.     Remove   the  unique  argument  on  '%s'." % f . name ) ) if ( ( f . rel . through is not  None  ) and ( not isinstance ( f . rel . through , basestring ) ) ) : ( from model , to model ) = ( cls , f . rel . to ) if ( ( from model == to model ) and f . rel . symmetrical and ( not f . rel . through .  meta . auto created ) ) : e . add ( opts , ' Many -to-many  fields  with  intermediate  tables  cannot  be  symmetrical.' ) ( seen from , seen to , seen self ) = (  False  ,  False  , 0 ) for inter field in f . rel . through .  meta . fields : rel to = getattr ( inter field . rel , 'to' ,  None  ) if ( from model == to model ) : if ( rel to == from model ) : seen self += 1 if ( seen self > 2 ) : e . add ( opts , ( ' Intermediary   model  %s  has  more  than  two  foreign  keys  to  %s,  which  is  ambiguous  and  is  not  permitted.' % ( f . rel . through .  meta . object name , from model .  meta . object name ) ) ) elif ( rel to == from model ) : if seen from : e . add ( opts , ( ' Intermediary   model  %s  has  more  than  one  foreign  key  to  %s,  which  is  ambiguous  and  is  not  permitted.' % ( f . rel . through .  meta . object name , from model .  meta . object name ) ) ) else : seen from =  True  elif ( rel to == to model ) : if seen to : e . add ( opts , ( ' Intermediary   model  %s  has  more  than  one  foreign  key  to  %s,  which  is  ambiguous  and  is  not  permitted.' % ( f . rel . through .  meta . object name , rel to .  meta . object name ) ) ) else : seen to =  True  if ( f . rel . through not in models . get models ( include auto created =  True  ) ) : e . add ( opts , ( "'%s'  specifies  an  m2m  relation  through  model  %s,  which  has  not  been  installed." % ( f . name , f . rel . through ) ) ) signature = ( f . rel . to , cls , f . rel . through ) if ( signature in seen intermediary signatures ) : e . add ( opts , ( ' The   model  %s  has  two  manually-defined  m2m  relations  through  the  model  %s,  which  is  not  permitted.   Please   consider  using  an  extra  field  on  your  intermediary  model  instead.' % ( cls .  meta . object name , f . rel . through .  meta . object name ) ) ) else : seen intermediary signatures . append ( signature ) if ( not f . rel . through .  meta . auto created ) : ( seen related fk , seen this fk ) = (  False  ,  False  ) for field in f . rel . through .  meta . fields : if field . rel : if ( ( not seen related fk ) and ( field . rel . to == f . rel . to ) ) : seen related fk =  True  elif ( field . rel . to == cls ) : seen this fk =  True  if ( ( not seen related fk ) or ( not seen this fk ) ) : e . add ( opts , ( "'%s'  is  a  manually-defined  m2m  relation  through  model  %s,  which  does  not  have  foreign  keys  to  %s  and  %s" % ( f . name , f . rel . through .  meta . object name , f . rel . to .  meta . object name , cls .  meta . object name ) ) ) elif isinstance ( f . rel . through , basestring ) : e . add ( opts , ( "'%s'  specifies  an  m2m  relation  through  model  %s,  which  has  not  been  installed" % ( f . name , f . rel . through ) ) ) rel opts = f . rel . to .  meta rel name =  Related  Object  ( f . rel . to , cls , f ) . get accessor name ( ) rel query name = f . related query name ( ) if ( rel name is not  None  ) : for r in rel opts . fields : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . local many to many : if ( r . name == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) if ( r . name == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . name , f . name ) ) ) for r in rel opts . get all related many to many objects ( ) : if ( r . field is not f ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  related  m2m  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) for r in rel opts . get all related objects ( ) : if ( r . get accessor name ( ) == rel name ) : e . add ( opts , ( " Accessor   for  m2m  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if ( r . get accessor name ( ) == rel query name ) : e . add ( opts , ( " Reverse   query  name  for  m2m  field  '%s'  clashes  with  related  field  '%s.%s'.   Add   a  related name  argument  to  the  definition  for  '%s'." % ( f . name , rel opts . object name , r . get accessor name ( ) , f . name ) ) ) if opts . ordering : for field name in opts . ordering : if ( field name == '?' ) : continue if field name . startswith ( '-' ) : field name = field name [ 1 : ] if ( opts . order with respect to and ( field name == ' order' ) ) : continue if ( '  ' in field name ) : continue if ( field name == 'pk' ) : continue try : opts . get field ( field name , many to many =  False  ) except models .  Field  Does  Not  Exist  : e . add ( opts , ( '"ordering"  refers  to  "%s",  a  field  that  doesn\'t  exist.' % field name ) ) for ut in opts . unique together : for field name in ut : try : f = opts . get field ( field name , many to many =  True  ) except models .  Field  Does  Not  Exist  : e . add ( opts , ( '"unique together"  refers  to  %s,  a  field  that  doesn\'t  exist.   Check   your  syntax.' % field name ) ) else : if isinstance ( f . rel , models .  Many  To  Many  Rel  ) : e . add ( opts , ( '"unique together"  refers  to  %s.   Many  To  Many  Fields   are  not  supported  in  unique together.' % f . name ) ) if ( f not in opts . local fields ) : e . add ( opts , ( '"unique together"  refers  to  %s.   This   is  not  in  the  same  model  as  the  unique together  statement.' % f . name ) ) return len ( e . errors )
def get single ( name , url , module , required , getter = u'  version  ' ) : mod = get version module ( module , name , url ) version getter = getattr ( mod , getter ) if hasattr ( version getter , u'  call  ' ) : current = version getter ( ) else : current = version getter return ( name , url , current , required )
def get health ( ** kwargs ) : with   Ipmi  Command  ( ** kwargs ) as s : return s . get health ( )
